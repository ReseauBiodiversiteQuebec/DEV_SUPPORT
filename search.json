[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Support technique au développement",
    "section": "",
    "text": "Ce site a pour objectif de lister et de centraliser la documentation sur les outils développés par ou pour Biodiversité Québec.",
    "crumbs": [
      "Accueil",
      "Support technique au développement"
    ]
  },
  {
    "objectID": "docs/catalogues/titiler.html",
    "href": "docs/catalogues/titiler.html",
    "title": "TiTiler",
    "section": "",
    "text": "TiTiler est un engin de traitement et une API qui intéragit de façon puissante et efficace avec les fichiers Cloud Optimized GeoTiffs.\nTiTiler permet de\n\nServir des tuiles “slippy” à partir d’un lien ver un COG pour alimenter des cartes Leaflet ou Maplibre.\nExtraire des informations (min, max, mean, histogram) à partir de COG, soit pour le fichier en entier, soit pour une zone définie avec un fichier GeoJSON envoyé via une requête POST.\n\nLien vers le serveur\nhttps://tiler.biodiversite-quebec.ca/\nCette ressource est hébergée sur une machine virtuelle sur Arbutus de façon à garder TiTiler à proximité de l’entreposage objet d’Arbutus.\nLien vers les dépot Github\nhttps://github.com/BiodiversiteQuebec/titiler-docker\nPour démarrer le Docker\ndocker compose up -d \nConfiguration NGINX\n\nserver {\n\n        root /var/www/html;\n\n        # Add index.php to the list if you are using PHP\n        index index.html index.htm index.nginx-debian.html;\n\n        server_name tiler.biodiversite-quebec.ca;\n\n        fastcgi_buffers 8 16k;\n        fastcgi_buffer_size 32k;\n\n        client_max_body_size 24M;\n        client_body_buffer_size 128k;\n\n        client_header_buffer_size 5120k;\n        large_client_header_buffers 16 5120k;\n\n        location / {\n                proxy_pass http://localhost:8000;\n        }\n\n    listen [::]:443 ssl ipv6only=on; # managed by Certbot\n    listen 443 ssl; # managed by Certbot\n    ssl_certificate /etc/letsencrypt/live/tiler.biodiversite-quebec.ca/fullchain.pem; # managed by Certbot\n    ssl_certificate_key /etc/letsencrypt/live/tiler.biodiversite-quebec.ca/privkey.pem; # managed by Certbot\n    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot\n    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot\n\n}\nserver {\n    if ($host = tiler.biodiversite-quebec.ca) {\n        return 301 https://$host$request_uri;\n    } # managed by Certbot\n\n\n        listen 80 default_server;\n        listen [::]:80 default_server;\n\n        server_name tiler.biodiversite-quebec.ca;\n    return 404; # managed by Certbot\n\n\n}",
    "crumbs": [
      "Catalogues",
      "Généralités",
      "TiTiler"
    ]
  },
  {
    "objectID": "docs/catalogues/acer/acer_intro.html",
    "href": "docs/catalogues/acer/acer_intro.html",
    "title": "Catalogue ACER",
    "section": "",
    "text": "Acer est un catalogue de couches de données issues des modélisations et analyses effectuées par Biodiversité Québec en format raster. Cette ressource est sous forme de catalogue STAC, qui est accessible via différentes méthodes, notamment sous R grâce aux packages rstac et gdalcubes.\nVoir la documentation pour IO, pour voir l’infrastructure logicielle et l’interaction avec le catalogue.\nLes couches disponibles dans le catalogue peuvent être visualisées ici.",
    "crumbs": [
      "Catalogues",
      "Acer",
      "Catalogue ACER"
    ]
  },
  {
    "objectID": "docs/catalogues/catalogues_intro.html",
    "href": "docs/catalogues/catalogues_intro.html",
    "title": "Catalogues",
    "section": "",
    "text": "Il y a actuellement trois catalogues principaux dans Biodiversité Québec. Le catalogue IO consiste en un catalogue et API STAC pour la gestion des couches environnementales en format raster Cloud Optimized GeoTIFF. Ce catalogue est partagé avec le projet Bon-in-a-Box de GEO BON. Le catalogue Acer est un catalogue et API équivalents pour stocker et rendre disponible les résultats de modélisation. Le catalogue GEOIO est une API FastAPI pour faire des requêtes géospatiales sur des fichiers vectoriels ou des requêtes impliquant des données vectorielles et les catalogues Acer et IO.\nCes catalogues sont tous hébergés sur le cloud Arbutus de l’Alliance.",
    "crumbs": [
      "Catalogues"
    ]
  },
  {
    "objectID": "docs/catalogues/io/rstac-gdalcubes.html",
    "href": "docs/catalogues/io/rstac-gdalcubes.html",
    "title": "Exemples de scripts utilisant RStac et GDALCUBES",
    "section": "",
    "text": "IO est un catalogue de couches de données environnementales en format raster pouvant servir dans le contexte de modélisations en lien avec la biodiversité. Cette ressource est sous forme de catalogue STAC, qui est accessible via différentes méthodes, notamment sous R grâce aux packages rstac et gdalcubes.\nLes couches disponibles dans le catalogue peuvent être visualisées ici.\nLes couches de données dans IO sont sous format COG (Cloud Optimized Geotiff), qui est un format permettant un accès optimal avec des requêtes à distance. Par exemple, il est possible d’extraire seulement une petite région d’un fichier global et de transformer sa résolution et son système de coordonnées de référence, sans jamais avoir à le télécharger en entier.\nVoici quelques exemples de requêtes permettant d’intéragir avec le catalogue STAC IO et les fichiers COG.\nlibrary(gdalcubes)\nlibrary(rstac)\nConnexion au catalogue STAC\ns_obj &lt;- stac(\"https://io.biodiversite-quebec.ca/stac/\")\nLister les collections\ncollections &lt;- s_obj |&gt; collections() |&gt; get_request()\nVoir les collections et leurs descriptions\nlibrary(knitr)\ndf&lt;-data.frame(id=character(),title=character(),description=character())\nfor (c in collections[['collections']]){\n  df&lt;-rbind(df,data.frame(id=c$id,title=c$title,description=c$description))\n}\nkable(df)\nChercher une collection spécifique (earthen_landcover)\nit_obj &lt;- s_obj |&gt;\n  stac_search(collections = \"earthenv_landcover\") |&gt;\n  post_request() |&gt; items_fetch()\nit_obj\nVoir les couches disponibles dans cette collection\nit_obj &lt;- s_obj |&gt;\n  collections(\"earthenv_landcover\") |&gt; items() |&gt;\n  get_request() |&gt; items_fetch()\nit_obj\nVoir les propriétés du premier item (couche)\nit_obj[['features']][[1]]$properties\nRésumé des items\ndf&lt;-data.frame(id=character(),datetime=character(), description=character())\nfor (f in it_obj[['features']]){\n  df&lt;-rbind(df,data.frame(id=f$id,datetime=f$properties$datetime,description=f$properties$description))\n}\nkable(df)\nAccéder au premier item avec le package STARS. Ce package permet d’accéder à des fichiers COG à distance de façon rapide et efficace. Ici, nous allons sélectionner la couche qui représente le pourcentage de “Evergreen/Deciduous Needleleaf Trees”.\nlibrary(stars)\nlc1&lt;-read_stars(paste0('/vsicurl/',it_obj[['features']][[12]]$assets$data$href), proxy = TRUE)\nplot(lc1)\nSélectionner seulement une partie du raster\nbbox&lt;-st_bbox(c(xmin = -76, xmax = -70, ymax = 54, ymin = 50), crs = st_crs(4326))\nlc2 &lt;- lc1 |&gt; st_crop(bbox)\nLa visualiser\npal &lt;- colorRampPalette(c(\"black\",\"darkblue\",\"red\",\"yellow\",\"white\"))\nplot(lc2,breaks=seq(0,100,10),col=pal(10))\nSauvegarder sur votre ordinateur en format Cloud Optimized GeoTiff.\nwrite_stars(lc2,'~/lc3.tif',driver='COG',options=c('COMPRESS=DEFLATE'))\nNotez que pour une variable avec des valeurs catégoriques, la sauvegarde est un peu plus complexe.\nlc1 |&gt; st_crop(bbox) |&gt; write_stars('~/lc1.tif',driver='COG',RasterIO=c('resampling'='mode'),options=c('COMPRESS=DEFLATE','OVERVIEW_RESAMPLING=MODE','LEVEL=6','OVERVIEW_COUNT=8','RESAMPLING=MODE','WARP_RESAMPLING=MODE','OVERVIEWS=IGNORE_EXISTING'))",
    "crumbs": [
      "Catalogues",
      "IO",
      "Exemples de scripts utilisant RStac et GDALCUBES"
    ]
  },
  {
    "objectID": "docs/catalogues/io/rstac-gdalcubes.html#utlisation-de-gdalcubes",
    "href": "docs/catalogues/io/rstac-gdalcubes.html#utlisation-de-gdalcubes",
    "title": "Exemples de scripts utilisant RStac et GDALCUBES",
    "section": "Utlisation de GDALCUBES",
    "text": "Utlisation de GDALCUBES\nCeci est une étape nécessaire pour que GDALCUBES fonctionne avec les données IO\n\nfor (i in 1:length(it_obj$features)){\n  it_obj$features[[i]]$assets$data$roles='data'\n}\n\nFiltrer en fonction des propriétés des items et créer une collection\n\nst &lt;- stac_image_collection(it_obj$features, asset_names=c('data'), property_filter = function(f){f[['class']] %in% c('1','2','3','4')},srs='EPSG:4326')\nst\n\nConstruire un cube pour traiter ou visualiser les données. Notez que ce cube peut être dans un SCR et une résolution différents de ceux des éléments/fichiers d’origine. Cependant, la dimension temporelle doit capturer le cadre temporel de l’élément. dt est exprimé en tant que période de temps. P1D est une période d’un jour, P1M est une période d’un mois, P1Y est une période d’un an. Les méthodes de rééchantillonnage doivent être adaptées au type de données. Pour les données catégorielles, utilisez “mode” ou “nearest”. Pour les données continues, utilisez “bilinear”. L’agrégation n’est pertinente que lorsque plusieurs rasters se chevauchent.\nIci, on va additionner les quatre catégories de forêts en utilisant aggregation=“sum”. On va aussi changer le système de référence des données pour utiliser Quebec Lambert (EPSG:32198) et mettre la résolution à 1km.\n\nbbox&lt;-st_bbox(c(xmin = -483695, xmax = -84643, ymin = 112704 , ymax = 684311), crs = st_crs(32198))\n\nv &lt;- cube_view(srs = \"EPSG:32198\", extent = list(t0 = \"2000-01-01\", t1 = \"2000-01-01\",\n                                                left = bbox$xmin, right =bbox$xmax, top = bbox$ymax, bottom = bbox$ymin), dx=1000, dy=1000, dt=\"P1D\", aggregation = \"sum\", resampling = \"mean\")\n\nJumeler la collection et le cube_view pour créer un raster cube.\n\nlc_cube &lt;- raster_cube(st, v)\n\nSauvegarder le fichier résultant sur votre ordinateur\n\nlc_cube |&gt; write_tif('~/',prefix='lc2',creation_options=list('COMPRESS'='DEFLATE'))\n\n\nlc_cube |&gt; plot(zlim=c(0,100),col=pal(10))\n\nUtiliser le jeu de données “Accessibility from cities”, en gardant le même SCR et étendue.\n\nit_obj &lt;- s_obj |&gt;\n  collections(\"accessibility_to_cities\") |&gt; items() |&gt;\n  get_request() |&gt; items_fetch()\nv &lt;- cube_view(srs = \"EPSG:32198\", extent = list(t0 = \"2015-01-01\", t1 = \"2015-01-01\",\n                                                left = bbox$xmin, right =bbox$xmax, top = bbox$ymax, bottom = bbox$ymin), dx=1000, dy=1000, dt=\"P1D\", aggregation = \"mean\", resampling = \"bilinear\")\nfor (i in 1:length(it_obj$features)){\n  it_obj$features[[i]]$assets$data$roles='data'\n}\nst &lt;- stac_image_collection(it_obj$features)\nlc_cube &lt;- raster_cube(st, v)\nlc_cube |&gt; plot(col=heat.colors)\n\nUtilisez le jeu de données CHELSA sur les climatologies et créez une carte des moyennes pour les mois de juin, juillet et août 2010 à 2019\n\nit_obj &lt;- s_obj |&gt;\n  stac_search(collections = \"chelsa-monthly\", datetime=\"2010-06-01T00:00:00Z/2019-08-01T00:00:00Z\") |&gt; get_request() |&gt; items_fetch()\n\nv &lt;- cube_view(srs = \"EPSG:32198\", extent = list(t0 = \"2010-06-01\", t1 = \"2019-08-31\",\n                                                left = bbox$xmin, right =bbox$xmax, top = bbox$ymax, bottom = bbox$ymin),\n               dx=1000, dy=1000, dt=\"P10Y\",\n               aggregation = \"mean\",\n               resampling = \"bilinear\")\n\nfor (i in 1:length(it_obj$features)){\n  names(it_obj$features[[i]]$assets)='data'\n  it_obj$features[[i]]$assets$data$roles='data'\n}\nanames=unlist(lapply(it_obj$features,function(f){f['id']}))\nst &lt;- stac_image_collection(it_obj$features, asset_names = 'data',  property_filter = function(f){f[['variable']] == 'tas' & (f[['month']] %in% c(6,7,8)) })\nc_cube &lt;- raster_cube(st, v)\nc_cube |&gt; plot(col=heat.colors)\n\nc_cube |&gt; write_tif('~/',prefix='chelsa-monthly',creation_options=list('COMPRESS'='DEFLATE'))",
    "crumbs": [
      "Catalogues",
      "IO",
      "Exemples de scripts utilisant RStac et GDALCUBES"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html",
    "title": "Liste des tableaux de bord archivés",
    "section": "",
    "text": "Responsable : Guillaume Larocque\nCatégorie : COLEO\nPrototype : lien\n\n\nVisualisation des sites d’échantillonnage du réseau de suivi de la biodiversité du Québec avec emphase sur les informatiques descriptives de l’échantillonnage\n\n\n\nLe Gouvernement du Québec réalise un programme ambitieux de suivi de la biodiversité sur un territoire qui est vaste et extrêmement hétérogène.\n\n\n\n\nCoordonnées de localisation des sites\nCatégories d’organismes inventoriés (indicateurs)\nInformations sur le protocole d’échantillonnage (e.g. nom du partenaire, propriétaire du terrain, type de milieu)\nDate de visite\nDonnées météorologiques\nDonnée de température de l’eau (aquatique)\nUtilisation du territoire (e.g. agricole, forestier, urbain)*\nProjections de changements climatiques*\nIndice d’empreinte humaine*\nDensité de routes*\nDensité de population*\nPhotos par les camera traps*\nEnregistrements sonores d’oiseaux*\nImage des organismes inventoriés\n\n\n\n\n\nAccès direct par API Coléo\nDonnées environnementales à suppléer\nAccès à des images d’espèces à partir de l’API iNaturalist\nmarque les données qui pourraient être accessibles mais qui ne le sont pas pour le moment\n\n\n\n\n\nLes données brutes sont présentées\n\n\n\n\n\nCarte de visualisation des sites avec pictogrammes décrivant les différentes campagnes d’inventaire\nListe de “fun facts” sur le dispositif dans la marge de gauche du tableau de bord\nCliquer sur un site fait apparaître une boite avec des onglets qui illustrent\n\nliste des indicateurs qui ont été inventoriés ainsi que les informations descriptives du site\nfigure qui illustre la variation au fil de l’année des précipitations et de la température\nfigure qui illustre les projections de changement climatique pour le site",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif",
    "title": "Liste des tableaux de bord archivés",
    "section": "",
    "text": "Visualisation des sites d’échantillonnage du réseau de suivi de la biodiversité du Québec avec emphase sur les informatiques descriptives de l’échantillonnage",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer",
    "title": "Liste des tableaux de bord archivés",
    "section": "",
    "text": "Le Gouvernement du Québec réalise un programme ambitieux de suivi de la biodiversité sur un territoire qui est vaste et extrêmement hétérogène.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données",
    "title": "Liste des tableaux de bord archivés",
    "section": "",
    "text": "Coordonnées de localisation des sites\nCatégories d’organismes inventoriés (indicateurs)\nInformations sur le protocole d’échantillonnage (e.g. nom du partenaire, propriétaire du terrain, type de milieu)\nDate de visite\nDonnées météorologiques\nDonnée de température de l’eau (aquatique)\nUtilisation du territoire (e.g. agricole, forestier, urbain)*\nProjections de changements climatiques*\nIndice d’empreinte humaine*\nDensité de routes*\nDensité de population*\nPhotos par les camera traps*\nEnregistrements sonores d’oiseaux*\nImage des organismes inventoriés",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données",
    "title": "Liste des tableaux de bord archivés",
    "section": "",
    "text": "Accès direct par API Coléo\nDonnées environnementales à suppléer\nAccès à des images d’espèces à partir de l’API iNaturalist\nmarque les données qui pourraient être accessibles mais qui ne le sont pas pour le moment",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données",
    "title": "Liste des tableaux de bord archivés",
    "section": "",
    "text": "Les données brutes sont présentées",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation",
    "title": "Liste des tableaux de bord archivés",
    "section": "",
    "text": "Carte de visualisation des sites avec pictogrammes décrivant les différentes campagnes d’inventaire\nListe de “fun facts” sur le dispositif dans la marge de gauche du tableau de bord\nCliquer sur un site fait apparaître une boite avec des onglets qui illustrent\n\nliste des indicateurs qui ont été inventoriés ainsi que les informations descriptives du site\nfigure qui illustre la variation au fil de l’année des précipitations et de la température\nfigure qui illustre les projections de changement climatique pour le site",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-1",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-1",
    "title": "Liste des tableaux de bord archivés",
    "section": "Objectif",
    "text": "Objectif\nCommuniquer rapidement les données accessibles par la base de données ATLAS",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-1",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-1",
    "title": "Liste des tableaux de bord archivés",
    "section": "Message à communiquer",
    "text": "Message à communiquer\nLa distribution des observations de biodiversité est très inégale à travers le Québec. Certaines régions sont plus diversifiées que d’autres, certains organismes sont mieux documentés que d’autres. La plupart des observations sont réalisées au sud du Québec là où la population est la plus dense et très peu d’observations sont faites dans l’extrême nord du Québec.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-1",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-1",
    "title": "Liste des tableaux de bord archivés",
    "section": "Données",
    "text": "Données\n\nExtraction de ATLAS qui permet d’avoir, pour une grille avec un maillage mobile, une densité d’observations et un nombre d’espèces.\nExtraction de ATLAS qui permet d’avoir la liste des données disponibles dans ATLAS",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-1",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-1",
    "title": "Liste des tableaux de bord archivés",
    "section": "Accessibilité des données",
    "text": "Accessibilité des données\n\nAccès direct par l’API ATLAS",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-1",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-1",
    "title": "Liste des tableaux de bord archivés",
    "section": "Traitement des données",
    "text": "Traitement des données\n\nLes données sont pré-traitées dans l’API pour maximiser la performance des requêtes",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-1",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-1",
    "title": "Liste des tableaux de bord archivés",
    "section": "Description sommaire de la représentation",
    "text": "Description sommaire de la représentation\n\nUne carte illustre les observations disponibles dans ATLAS.\nLe menu permet de sélectionner le type d’organisme d’intérêt\nUn compteur en haut à droite résume les données\nUn histogramme au bas permet de sélectionner la plage temporelle couverte.\nUn onglet permet d’accéder à la liste des données disponibles dans ATLAS.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-2",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-2",
    "title": "Liste des tableaux de bord archivés",
    "section": "Objectif",
    "text": "Objectif\nCommuniquer rapidement les données accessibles par la base de données IO. Cette base de données rassemble les couches géomatiques de données environnementales (e.g. climat, qualité des sols, productivité primaire), utilisation du territoire (e.g. empreinte humaine, densité de population, densité des routes) et scénarios de changements climatiques.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-2",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-2",
    "title": "Liste des tableaux de bord archivés",
    "section": "Message à communiquer",
    "text": "Message à communiquer\nComprendre les changements de biodiversité requiert une connaissance des variables susceptibles de l’affecter. Les représentations au sein des différents tableaux de bord sollicitent pour la plupart des données qui sont synthétisées dans la base de données IO et représentées au moyen de ce tableau de bord.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-2",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-2",
    "title": "Liste des tableaux de bord archivés",
    "section": "Données",
    "text": "Données\n\nExtraction de IO qui permet d’avoir de l’information cartographique pour des couches de toutes natures.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-2",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-2",
    "title": "Liste des tableaux de bord archivés",
    "section": "Accessibilité des données",
    "text": "Accessibilité des données\n\nAccès direct par l’API IO",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-2",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-2",
    "title": "Liste des tableaux de bord archivés",
    "section": "Traitement des données",
    "text": "Traitement des données\n\nLes données sont pré-traitées dans l’API pour maximiser la performance des requêtes",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-2",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-2",
    "title": "Liste des tableaux de bord archivés",
    "section": "Description sommaire de la représentation",
    "text": "Description sommaire de la représentation\n\nUne carte illustre les différentes couches stockées dans IO. Un menu déroulant permet d’illustrer la couche à illustrer.\nLa transparence permet de superposer plusieurs couches pour apprécier l’occurrence de stresseurs multiples.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-3",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-3",
    "title": "Liste des tableaux de bord archivés",
    "section": "Objectif",
    "text": "Objectif\nAccéder rapidement à une description simplifiée de la richesse en espèces au sein du réseau de suivi du Gouvernement du Québec.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-3",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-3",
    "title": "Liste des tableaux de bord archivés",
    "section": "Message à communiquer",
    "text": "Message à communiquer\nDécouvrir la diversité des espèces échantillonnées par le programme.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-3",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-3",
    "title": "Liste des tableaux de bord archivés",
    "section": "Données",
    "text": "Données\n\nLes données sont regroupées par campagne / indicateur. Pour chaque site, une liste des espèces observées\nLocalisation des sites d’échantillonnage\nImage des organismes inventoriés",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-3",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-3",
    "title": "Liste des tableaux de bord archivés",
    "section": "Accessibilité des données",
    "text": "Accessibilité des données\n\nAccès direct par API Coléo\nDonnées environnementales à suppléer\nAccès à des images d’espèces à partir de l’API iNaturalist",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-3",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-3",
    "title": "Liste des tableaux de bord archivés",
    "section": "Traitement des données",
    "text": "Traitement des données\n\nUn décompte du nombre d’espèces par campagne / indicateur est réalisé",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-3",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-3",
    "title": "Liste des tableaux de bord archivés",
    "section": "Description sommaire de la représentation",
    "text": "Description sommaire de la représentation\n\nUne carte donne accès aux différents sites échantillonnés. On peut filtrer les sites par catégorie d’échantillonnage. La sélection du type d’icone permet de visualiser la richesse en espèces.\nLa sélection d’un site fait apparaître une fenêtre où est résumé le nombre d’espèces par campagne / indicateur. Des images des espèces les plus abondantes apparaissent pour illustrer cette diversité.\nCertains partenaires du projet ont exprimé le souhait de comparer plusieurs sites.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-4",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-4",
    "title": "Liste des tableaux de bord archivés",
    "section": "Objectif",
    "text": "Objectif\nIlluster le changement dans l’indice planète vivante pour des populations qui font l’objet d’un suivi à long terme au Québec.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-4",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-4",
    "title": "Liste des tableaux de bord archivés",
    "section": "Message à communiquer",
    "text": "Message à communiquer\nL’indice planète vivante est un standard établit par le World Wildlife Fund pour rapporter les tendances dans les populations animales et végétales. Découvrez l’indice et ses variantes dans cette première étude pour le Québec.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-4",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-4",
    "title": "Liste des tableaux de bord archivés",
    "section": "Données",
    "text": "Données\n\nSéries temporelles de populations stockées dans ATLAS\nCoordonnées spatiales des populations\nImage des organismes inventoriés\nSéries temporelles du climat pour chaque site",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-4",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-4",
    "title": "Liste des tableaux de bord archivés",
    "section": "Accessibilité des données",
    "text": "Accessibilité des données\n\nAccès direct par API Coléo\nDonnées environnementales à suppléer\nAccès à des images d’espèces à partir de l’API iNaturalist",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-4",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-4",
    "title": "Liste des tableaux de bord archivés",
    "section": "Traitement des données",
    "text": "Traitement des données\n\nL’indice planète vivant est le résultat d’un calcul assez complexe qui transforme les données et aggrège plusieurs modèles",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-envisagée-ou-réalisée",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-envisagée-ou-réalisée",
    "title": "Liste des tableaux de bord archivés",
    "section": "Description sommaire de la représentation envisagée ou réalisée",
    "text": "Description sommaire de la représentation envisagée ou réalisée\n\nUne carte localise les populations étudiées\nSélectionner une population sur la carte permet d’illustrer la tendance temporelle\nUn bouton dans la marge de gauche permet d’illuster l’indice pour l’ensemble du territoire. Des options permettent d’afficher la moyenne ou les courbes individuelles. Des filtres permettent de distinguer les différents groupes d’espèces",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-5",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-5",
    "title": "Liste des tableaux de bord archivés",
    "section": "Objectif",
    "text": "Objectif\nMettre en valeur les espèces les plus rares échantillonnées dans le réseau de suivi.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-5",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-5",
    "title": "Liste des tableaux de bord archivés",
    "section": "Message à communiquer",
    "text": "Message à communiquer\nLes espèces emblématiques attirent d’abord notre attention, alors que les espèces rares contribuent davantage à la biodiversité. Découvrez dans ce module le rôle que ces espèces peuvent avoir pour comprendre les changements climatiques. Les écosystèmes sont habituellement composés d’une série d’espèces abondates, communes, mais aussi d’espèces rares que l’on ne retrouve que très occasionnellement. Ces espèces contribuent exceptionnellement à la diversité avec des caractéristiques exceptionnelles et une histoire évolutive unique.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-5",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-5",
    "title": "Liste des tableaux de bord archivés",
    "section": "Données",
    "text": "Données\n\nLes données sont regroupées par campagne / indicateur. Pour chaque site, une liste des espèces observées\nLocalisation des sites d’échantillonnage\nImage des organismes inventoriés",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-5",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-5",
    "title": "Liste des tableaux de bord archivés",
    "section": "Accessibilité des données",
    "text": "Accessibilité des données\n\nAccès direct par API Coléo\nDonnées environnementales à suppléer (climat, indice empreinte humaine)\nAccès à des images d’espèces à partir de l’API iNaturalist\nArborescence taxonomique pour chaque espèce",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-5",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-5",
    "title": "Liste des tableaux de bord archivés",
    "section": "Traitement des données",
    "text": "Traitement des données\n\nAnalyse de l’abondance régionale des espèces\nAnalyse de la position phylogénétique des espèces",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-4",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-4",
    "title": "Liste des tableaux de bord archivés",
    "section": "Description sommaire de la représentation",
    "text": "Description sommaire de la représentation\n\nUne carte donne accès aux différents sites échantillonnés. On peut filtrer les sites par catégorie d’échantillonnage. La sélection du type d’icone permet de visualiser la richesse en espèces.\nLa sélection d’un site fait apparaître une fenêtre où se trouvent plusieurs onglets qui soulignent les observations exceptionnelles :\n\nDistribution d’abondance des espèces sur l’ensemble des données avec identification des espèces pour le site en particulier\nPrésentation d’une rosette taxonomique qui décrit l’originalité phylogénétique des espèces observées sur le site\nPossibilité d’ajouter des images pour illustrer ces espèces\nUne figure représente les préférences des espèces trouvées à ce site pour le climat (de chaud à froid) et pour les perturbations anthropiques (indice empreinte humaine)",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-6",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-6",
    "title": "Liste des tableaux de bord archivés",
    "section": "Objectif",
    "text": "Objectif\nComptabiliser le nombre d’espèces qui se trouvent au Québec, par région et ainsi que pour les sites du réseau de suivi.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-6",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-6",
    "title": "Liste des tableaux de bord archivés",
    "section": "Message à communiquer",
    "text": "Message à communiquer\nLe nombre d’espèces qui se trouve dans une région peut être difficile à évaluer. Des inventaires sont réalisés et on peut aisément calculer le nombre d’espèces observées. Certaines espèces sont cependant rares, cryptiques, et elles échappent à l’inventaire. Les scientifiques font appel à une technique dite de “raréfaction” pour estimer la diversité en espèces d’une région et procurer un degré de confiance dans cette estimation.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-6",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-6",
    "title": "Liste des tableaux de bord archivés",
    "section": "Données",
    "text": "Données\n\nDistribution des espèces dans ATLAS\nDistribution des espèces dans COLEO\nLocalisation des sites du suivi\nFond de carte avec les régions écologiques du Québec",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-6",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-6",
    "title": "Liste des tableaux de bord archivés",
    "section": "Accessibilité des données",
    "text": "Accessibilité des données\n\nToutes les données sont accessibles par les APIs de ATLAS et COLEO",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-6",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-6",
    "title": "Liste des tableaux de bord archivés",
    "section": "Traitement des données",
    "text": "Traitement des données\n\nUne librairie de R est utilisée pour comptabiliser le nombre d’espèces observées à partir d’une liste d’espèces observées, le nombre d’espèces au total et l’incertiture de cette estimation.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-5",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-5",
    "title": "Liste des tableaux de bord archivés",
    "section": "Description sommaire de la représentation",
    "text": "Description sommaire de la représentation\n\nUne carte principale offre la possibilité d’illustrer le nombre d’espèces observées dans ATLAS par région écologique ou la localisation des sites d’échantillonnage dans COLÉO.\nLe menu de la marge de gauche permet de sélectionner le groupe taxonomique d’intérêt.\nLes onglets de la carte permettent de consulter la diversité totale par groupe taxonomique.\nLa sélection d’une région ou d’une site fait apparaître une fenêtre où est illustrée la richesse en espèce par groupe taxonomique pour la sélection.\nUn onglet permet d’obtenir la liste des espèces observées pour la sélection.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-7",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-7",
    "title": "Liste des tableaux de bord archivés",
    "section": "Objectif",
    "text": "Objectif\nPrésenter la succession temporelle des chauves-souris sur les sites d’échantillonnage.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-7",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-7",
    "title": "Liste des tableaux de bord archivés",
    "section": "Message à communiquer",
    "text": "Message à communiquer\nLe changement des saisons provoque une succession d’événements marquants pour la biodiversité, comme le développement de la végétation, l’arrivée des premiers passereaux ou encore le chant des grillons. Explorez comment les changements de biodiversité affecteront la phénologie des espèces et en particulier des chauves-souris.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-7",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-7",
    "title": "Liste des tableaux de bord archivés",
    "section": "Données",
    "text": "Données\n\nDonnées d’observation des chauves souris à chaque site du réseau de suivi. La donnée est constituée du nombre d’observation par espèce par jour, pour chaque site d’échantillonnage\nDonnées météorologiques à suppléer",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-7",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-7",
    "title": "Liste des tableaux de bord archivés",
    "section": "Accessibilité des données",
    "text": "Accessibilité des données\n\nAccès direct par API Coléo\nDonnées environnementales à suppléer",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-7",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-7",
    "title": "Liste des tableaux de bord archivés",
    "section": "Traitement des données",
    "text": "Traitement des données\n\nAucun traitement n’est nécessaire\nPossibilité d’ajouter un modèle qui décrit les conditions d’arrivée et de départ",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-6",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-6",
    "title": "Liste des tableaux de bord archivés",
    "section": "Description sommaire de la représentation",
    "text": "Description sommaire de la représentation\n\nLa carte permet de représenter les sites du réseau de suivi\nPossibilité d’ajouter un onglet pour présenter la variation de la séquence temporelle pour l’ensemble du territoire\nLa sélection d’un site permet d’illustrer la séquence d’arrivée et de départ des différentes espèces documentées\nIl serait possible d’ajouter une série temporelle des conditions météorologiques pour illustrer les conditions d’arrivées et de départ des différentes espèces.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-8",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-8",
    "title": "Liste des tableaux de bord archivés",
    "section": "Objectif",
    "text": "Objectif\nCommuniquer l’impact des changements climatiques sur la distribution des espèces et leur capacité à soutenir les changements climatiques.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-8",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-8",
    "title": "Liste des tableaux de bord archivés",
    "section": "Message à communiquer",
    "text": "Message à communiquer\nCertaines espèces peinent à suivre le rythme des changements climatiques et sont déjà maladaptées avec le climat actuel. D’autres le seront dans un futur proche. Explorer cet indice qui illustre la difficulté de certaines espèces à suivre les conditions climatiques qui leurs sont favorables.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-8",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-8",
    "title": "Liste des tableaux de bord archivés",
    "section": "Données",
    "text": "Données\n\nDonnées de distribution de ATLAS\nDonnées environnementales*\nDonnées de pression humaine\nScénarios de changements climatiques",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-8",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-8",
    "title": "Liste des tableaux de bord archivés",
    "section": "Accessibilité des données",
    "text": "Accessibilité des données\n\nAccès direct par API ATLAS\nDonnées environnementales à suppléer",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-8",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-8",
    "title": "Liste des tableaux de bord archivés",
    "section": "Traitement des données",
    "text": "Traitement des données\n\nLes données d’observations seront analysées par une librairie spécialisée appelée mapSpecies.\nLes modèles seront utilisés pour créer des cartes, une par espèce, de la localisation actuelle de l’espèce.\nLes modèles pourront également être interrogés pour évaluer si pour chaque endroit l’impact du changement climatique sur la performance des espèces\nLes modèles pourront également être interrogés pour projeter la répartition future de chaque espèce",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-7",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-7",
    "title": "Liste des tableaux de bord archivés",
    "section": "Description sommaire de la représentation",
    "text": "Description sommaire de la représentation\n\nLa conceptualisation de ce tableau de bord doit être refaite. Plusieurs options sont envisagées\n\nLe calcul d’un indice de “maladaptation” sur une grille qui peut être représentée sur la carte. Cet indice rapporte le nombre d’espèces susceptible de disparaître dans un horizon spécifié par l’utilisateur sous le changement climatique\nUne animation peut être réalisée pour illustrer le déplacement possible sur une carte des aires de répartition d’une espèce sélectionnée dans le menu déroulant.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-9",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#objectif-9",
    "title": "Liste des tableaux de bord archivés",
    "section": "Objectif",
    "text": "Objectif\nIlluster le changement dans l’indice de la distribution de la biodiversité pour les oiseaux et les papillons.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-9",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-9",
    "title": "Liste des tableaux de bord archivés",
    "section": "Message à communiquer",
    "text": "Message à communiquer\nL’indice de distribution de biodiversité est un nouvel indicateur de changements de biodiveristé utilisé pour rapporter les changements d’aire de répartition, les extinctions et les introductions de nouvelles espèces. Similaire au LPI, il rapporte le bilan des accroissements et diminutions d’aires de répartition au fil du temps.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-9",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-9",
    "title": "Liste des tableaux de bord archivés",
    "section": "Données",
    "text": "Données\n\nDonnées de distribution de ATLAS.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-9",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#accessibilité-des-données-9",
    "title": "Liste des tableaux de bord archivés",
    "section": "Accessibilité des données",
    "text": "Accessibilité des données\n\nAccès direct par API ATLAS",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-9",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#traitement-des-données-9",
    "title": "Liste des tableaux de bord archivés",
    "section": "Traitement des données",
    "text": "Traitement des données\n\nLes données d’observations seront analysées par une librairie spécialisée appelée mapSpecies.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-envisagée-ou-réalisée-1",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-sommaire-de-la-représentation-envisagée-ou-réalisée-1",
    "title": "Liste des tableaux de bord archivés",
    "section": "Description sommaire de la représentation envisagée ou réalisée",
    "text": "Description sommaire de la représentation envisagée ou réalisée\n\nUne carte représente le résultat sommaire des changements de diversité pour l’ensemble du Québec.\nL’indice est illustré par défaut, un filtre permet d’illustrer les extinctions locales ou les colonisations.\nUn bouton dans la marge de gauche permet d’accéder à la tendance globale de l’indice sur l’ensemble du territoire. Un filtre permet de distinguer les groupes (e.g. rapaces, passeraux, sauvagine, limicoles)",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-10",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#message-à-communiquer-10",
    "title": "Liste des tableaux de bord archivés",
    "section": "Message à communiquer",
    "text": "Message à communiquer\nLes changements de biodiversité ne sont pas seulement la diminution d’abondance et la perte d’espèce indigène et l’arrivée d’espèce exotique, c’est aussi les changements (positifs et négatifs).",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-10",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#données-10",
    "title": "Liste des tableaux de bord archivés",
    "section": "Données",
    "text": "Données\n\nSéries temporelles de population dans Living Planet Database\nSéries temporelles de population dans BioTIME\nLes modèles de distribution d’espèces (SDM) provenant des données d’Atlas",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#assemblage-du-tableau-de-bord",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#assemblage-du-tableau-de-bord",
    "title": "Liste des tableaux de bord archivés",
    "section": "Assemblage du tableau de bord",
    "text": "Assemblage du tableau de bord\nDownload PDF file.\nOnglet de sélection: chaque groupe taxonomique (plantes, mammifères, vue d’ensemble)\nPanneau 1 : Carte illustrant les extinctions et les colonisations sur l’ensemble du Québec au fil des années depuis l’année de référence du BDI. Ces extinctions et colonisations peuvent être visualisées par espèces (e.g. érable à sucre), par groupe d’espèce (e.g. oiseaux), par région ou pour l’ensemble du Québec.\nPanneau 2 : Tendance moyenne du Biodiversity Distribution Index (BDI), animée pour montrer la tendance progressivement (voir panneau 3)\nPanneau 3 : Tendance moyenne de l’indice Planète Vivante*, animée pour montrer la tendance progressivement (e.g. https://www.r-graph-gallery.com/287-smooth-animation-with-tweenr.html)\nPanneau 4 : Diagramme waffle interactif avec 1 case par espèce, où chaque case est colorée selon un gradient vert-blanc-rouge: populations en croissance sont en vert, les populations stables sont en blanc, et les populations en déclin sont en rouge. En passant sur chaque case avec le curseur, un pop-up montre le nom de l’espèce, une photo, et un graphique de la tendance de son indice Planète Vivante en fonction du temps. Au-dessus du graphique, on affichera les pourcentages de populations en croissance / stables / en déclin, et la tendance moyenne de ces sous-groupes (e.g. 5% en déclin, et sont maintenant 20% plus petits qu’en xxxx (l’année de référence)).",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#amélioration-visuelle-possible",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#amélioration-visuelle-possible",
    "title": "Liste des tableaux de bord archivés",
    "section": "Amélioration visuelle possible",
    "text": "Amélioration visuelle possible\nLes animations des panneaux 1, 2, et 3 mettraient l’emphase sur la progression des tendances avec le temps.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#histoire-de-biodiversité-associée",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#histoire-de-biodiversité-associée",
    "title": "Liste des tableaux de bord archivés",
    "section": "Histoire de biodiversité associée",
    "text": "Histoire de biodiversité associée\n\nPour le BDI, parler d’une espèce qui a colonisé un endroit quelconque et parler des facteurs qui semblent expliquer cette colonisation. Même chose pour l’extinction. Idéalement, ce serait des espèces que les gens connaissent bien ou à lesquelles les gens ont peut-être un attachement (e.g.oiseau charismatique, l’érable à sucre)\nPour le mêmes espèces qu’en 1, il pourrait y avoir des projections pour voir à quoi ressemblera l’aire de répartition de celles-ci dans le futur selon différents scénarios.\nPour le LPI, parler d’une espèce en croissance et d’une espèce en déclin, et expliquer pourquoi elles montrent cette tendance. Idéalement, les espèces devraient être charismatiques.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description",
    "title": "Liste des tableaux de bord archivés",
    "section": "Description",
    "text": "Description\nTableau de bord qui illustre la richesse spécifique par groupe taxonomique et par région au Québec",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#les-données-incluent",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#les-données-incluent",
    "title": "Liste des tableaux de bord archivés",
    "section": "Les données incluent",
    "text": "Les données incluent\n\nOccurences des espèces issues d’ATLAS\nAnalyses de raréfaction (logiciel R; package iNEXT)\nProvinces naturelles définies selon https://www.donneesquebec.ca/recherche/fr/dataset/cadre-ecologique-de-reference",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-des-scripts",
    "href": "docs/tableaux_bord/tableaux_bord_liste_archive.html#description-des-scripts",
    "title": "Liste des tableaux de bord archivés",
    "section": "Description des scripts",
    "text": "Description des scripts\nLa ségrégation des occurences par régions québécoises et les analyses de raréfaction sont exécutées par le script make_local_data.R.\nL’application Shiny est lancée avec le script app.R. Plusieurs dépendances sont chargées automatiquement.\nL’exécution du script requiert plusieurs librairies R, celles-ci peuvent être installées avec le lancement de script_installation.r.",
    "crumbs": [
      "Tableaux de bord",
      "Liste des tableaux de bord archivés"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_intro.html",
    "href": "docs/tableaux_bord/tableaux_bord_intro.html",
    "title": "Tableaux de bord",
    "section": "",
    "text": "Explorateur IO: explorateur des couches environnementales contenues dans le STAC catalogue\nExplorateur Acer: explorateur des produits de Biodiversité Québec (incluant les SDMs, EBV, etc.)\nExplorateur SDM/paramètres: comparaison des méthodes SDMs en fonction des paramètres\nExplorateur paramètres/SDM: comparaison des paramètres au sein des méthodes SDMs\nNiches climatiques: tableau de bord des niches climatiques (ce tableau est dans un docker)",
    "crumbs": [
      "Tableaux de bord"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_intro.html#tableaux-de-bord-actuels",
    "href": "docs/tableaux_bord/tableaux_bord_intro.html#tableaux-de-bord-actuels",
    "title": "Tableaux de bord",
    "section": "",
    "text": "Explorateur IO: explorateur des couches environnementales contenues dans le STAC catalogue\nExplorateur Acer: explorateur des produits de Biodiversité Québec (incluant les SDMs, EBV, etc.)\nExplorateur SDM/paramètres: comparaison des méthodes SDMs en fonction des paramètres\nExplorateur paramètres/SDM: comparaison des paramètres au sein des méthodes SDMs\nNiches climatiques: tableau de bord des niches climatiques (ce tableau est dans un docker)",
    "crumbs": [
      "Tableaux de bord"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_intro.html#liste-des-tableaux-de-bord-fonctionnels",
    "href": "docs/tableaux_bord/tableaux_bord_intro.html#liste-des-tableaux-de-bord-fonctionnels",
    "title": "Tableaux de bord",
    "section": "Liste des tableaux de bord fonctionnels",
    "text": "Liste des tableaux de bord fonctionnels\nLes tableaux suivants sont actuellement déployés sur le serveur rweb:\n\ncog-react\ncog-test\ndata\nio-layers\nQC_in_a_CUBE_Species_richness\nrayshader-test\nSDM_benchmark_species\nsdms\nshiny-src\ntableau_atlas_couverture\ntableau-atlas-react\ntableau-atlas-vue\ntableau-btsl\ntableau-coleo-react\ntableau-comparateur_sdm\ntableau-enveloppe\ntableau-forets\ntableau-gbif\ntableau-validation\ntableaubdi\ntableauchangementstemporels\ntableaucoleo\ntableaucomposition\ntableauexplosites\ntableaulpi\ntableaurarefaction",
    "crumbs": [
      "Tableaux de bord"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_intro.html#anciens-tableaux-de-bord",
    "href": "docs/tableaux_bord/tableaux_bord_intro.html#anciens-tableaux-de-bord",
    "title": "Tableaux de bord",
    "section": "Anciens tableaux de bord",
    "text": "Anciens tableaux de bord\nLes tableaux de bords archivés sont accessibles en suivant ce lien.\n\ntableau-atlas\ntableau-composition\ntableau-explo-sites\ntableau-lpi-bdi\ntableau-move\ntableau-rarefaction\ntableauatlas",
    "crumbs": [
      "Tableaux de bord"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_intro.html#tableaux-de-bord-en-cours-de-développement",
    "href": "docs/tableaux_bord/tableaux_bord_intro.html#tableaux-de-bord-en-cours-de-développement",
    "title": "Tableaux de bord",
    "section": "Tableaux de bord en cours de développement",
    "text": "Tableaux de bord en cours de développement\n\nComposition des communautés\nCombien d’espèces au Québec ?\nLes espèces rares du Québec\nIndice planète vivante\nIndice de distribution de la biodiversité\nLa biodiversité en mouvement\nPhénologie des chauve-souris",
    "crumbs": [
      "Tableaux de bord"
    ]
  },
  {
    "objectID": "docs/serveurs/rweb/rweb_shinyapp.html",
    "href": "docs/serveurs/rweb/rweb_shinyapp.html",
    "title": "Déploiement d’une app Shiny",
    "section": "",
    "text": "Repo Github",
    "crumbs": [
      "Serveurs",
      "Rweb",
      "Déploiement d'une app Shiny"
    ]
  },
  {
    "objectID": "docs/serveurs/rweb/rweb_shinyapp.html#connexion-to-rweb",
    "href": "docs/serveurs/rweb/rweb_shinyapp.html#connexion-to-rweb",
    "title": "Déploiement d’une app Shiny",
    "section": "Connexion to rweb",
    "text": "Connexion to rweb\nssh USERNAME@rweb.vhost33",
    "crumbs": [
      "Serveurs",
      "Rweb",
      "Déploiement d'une app Shiny"
    ]
  },
  {
    "objectID": "docs/serveurs/rweb/rweb_shinyapp.html#add-a-shiny-app-to-rweb",
    "href": "docs/serveurs/rweb/rweb_shinyapp.html#add-a-shiny-app-to-rweb",
    "title": "Déploiement d’une app Shiny",
    "section": "Add a shiny app to rweb",
    "text": "Add a shiny app to rweb\n\nCreate a new SSH key with\n\nssh-keygen -t rsa -f ~/.ssh/id_rsa.REPONAME\n\nAdd the id_rsa.REPONAME.pub file as a deploy key to the Github repo with a write access\nEdit the ~/.ssh/config file to add\n\nHost REPONAME.github.com\n  HostName github.com\n  User git\n  IdentityFile /home/shiny/.ssh/id_rsa.REPONAME\n\nGit clone the repository in the shiny server\n\ncd /srv/shiny-server\ngit clone git@REPONAME.github.com:BiodiversiteQuebec/REPONAME.git",
    "crumbs": [
      "Serveurs",
      "Rweb",
      "Déploiement d'une app Shiny"
    ]
  },
  {
    "objectID": "docs/serveurs/rweb/rweb_shinyapp.html#view-shiny-apps-logs",
    "href": "docs/serveurs/rweb/rweb_shinyapp.html#view-shiny-apps-logs",
    "title": "Déploiement d’une app Shiny",
    "section": "View shiny apps logs",
    "text": "View shiny apps logs\nAll logs are stored in the var log directory\nsudo tail /var/log/shiny-server/\nAssociated Github repository",
    "crumbs": [
      "Serveurs",
      "Rweb",
      "Déploiement d'une app Shiny"
    ]
  },
  {
    "objectID": "docs/serveurs/rweb/rweb_shinyapp.html#add-or-update-a-package-used-by-a-shiny-app",
    "href": "docs/serveurs/rweb/rweb_shinyapp.html#add-or-update-a-package-used-by-a-shiny-app",
    "title": "Déploiement d’une app Shiny",
    "section": "Add or update a package used by a shiny app",
    "text": "Add or update a package used by a shiny app\n1. Connect as the shiny user\ncd home/shiny\nsudo su shiny\n2. Install the package with the following command (for example shiny)\nR -e \"install.packages('shiny')\"\nor\nR -e \"devtools::install_github(\"ReseauBiodiversiteQuebec/ratlas@httr_v1\")\"\n3. Restart the shiny server\nsudo service shiny-server restart\n\nTest the package in the shiny app",
    "crumbs": [
      "Serveurs",
      "Rweb",
      "Déploiement d'une app Shiny"
    ]
  },
  {
    "objectID": "docs/serveurs/coleo-api/index.html",
    "href": "docs/serveurs/coleo-api/index.html",
    "title": "Coleo-api",
    "section": "",
    "text": "This server runs all the APIs and web apps. It also runs the github runners that are used in our deployment pipeline.\nto connect via ssh ssh &lt;your_user&gt;@coleo-api.vhost33\n\n\n\n\nIt is api create out of the box by the library postgRest. It provides a large set of functionalities that save us an important amount of time of development and maintainance.\nThe postgRest setup folder is located in the home folder of the user coleo. To access make sudo su - coleo, then cd postgrestV8.\nTo run postgrest you use the following command: ./postgrest atlas-config.conf\nIt runs as a services called postgrest_atlas.service.\nuse sudo systemctl status postgrest_atlas.service to status the services\nuse sudo systemctl start postgrest_atlas.service to start the services\nuse sudo systemctl stop postgrest_atlas.service to stop the services\nuse sudo systemctl restart postgrest_atlas.service to restart the services\n\n\n\nBy using a url prefix followed by the name of the table in the database and a filter query(see postgRest documentation) if needed example: https://atlas.biodiversite-quebec.ca/api/v2/\nYou might need to access to a custom endpoint (procedure), in that case you need to add /rpc/\nFor security reasons, in order control the access to our database we send a json web token (jwt) (sent asBearer token) on every request to the postgRest. The token will be prodived by the dev team.\nParams can be sent as query params in a get request or in the body in case of a post request.\nExamples:\nrequest headers: the name of the schema must to be pass in the headers. In case of a post request Content-profile and for a get request Accept-profile. \nrequest bearer: \nrequest filter: \nPost request: \n\n\n\nIn this case postgRest is used to expose Coleo database. The process is exactly the same than atlas (shown before) but using a different jwt and different url.\nThe postgRest setup folder is located in the home folder of the user coleo. To access make sudo su - coleo, then cd postgrestV8.\nTo run postgrest you use the following command: ./postgrest coleo-config.conf\nIt runs as a services called postgrest_coleo.service.\nuse sudo systemctl status postgrest_coleo.service to status the services\nuse sudo systemctl start postgrest_coleo.service to start the services\nuse sudo systemctl stop postgrest_coleo.service to stop the services\nuse sudo systemctl restart postgrest_coleo.service to restart the services\nurl prefix: https://coleo.biodiversite-quebec.ca/newapi/v1/\n\n\n\nIn this case postgRest is used to expose IO database. The process is exactly the same than atlas (shown before) but using a different jwt and different url.\nThe postgRest setup folder is located in the home folder of the user coleo. To access make sudo su - coleo, then cd postgrestV8.\nTo run postgrest you use the following command: ./postgrest io-config.conf\nIt runs as a services called postgrest_io.service.\nuse sudo systemctl status postgrest_io.service to status the services\nuse sudo systemctl start postgrest_io.service to start the services\nuse sudo systemctl stop postgrest_io.service to stop the services\nuse sudo systemctl restart postgrest_io.service to restart the services\nurl prefix: https://io.biodiversite-quebec.ca/…\n\n\n\nStrapi is a Headless CMS that we use to allow any authorized member of the team to add and update content to our Web apps (mainly our web portal)\n\n\n\nImage\n\n\nTo access via api use the url: https://portail.biodiversite-quebec.ca/api/&lt;type_+_filter&gt;\n\n\n\nImage\n\n\nTo get access to the web interface contact dev team so they can provide it for you.\nA cronjob is setup to make a backup of strapi on S3 server.\nsudo su - coleo && contrab -e to see the crontab.\ncheck documentation strapi api repo\nRepo: https://github.com/ReseauBiodiversiteQuebec/strapi-backend-portail\n\n\n\nIt is a nodeJs/express api that give access to the coleo database. It is used mainly by the Coleo Web App and to inject data to coleo database (this will be eventually migrated to postgRest coleo).\n\n\n\nPlumber (https://www.rplumber.io/) is used to generate figures and extract data for the inventaires dashboard. It is installed as two docker containers. The specifications can be found here:\nhttps://github.com/ReseauBiodiversiteQuebec/plumber-api\nOne docker is used for tasks that are more time consuming. It updates a series of .Rdata files that are then used by the second container to populate the dashboard.\n\n\n\nStac-fast-api is the API used to generate the STAC catalogue. It runs as a docker. Further details can be found here: https://github.com/ReseauBiodiversiteQuebec/stac-api-getway-app\n\n\n\n\n\n\nThe web portal for the quebec biodiversity project. From here you have access to all our apps that are open to the public (Atlas, Inventaires). You have also access to Articles andIndicators related with quebec’s biodiversity.\nIt have been developed using NextJs ( reactJs framework)\nRepository: https://github.com/ReseauBiodiversiteQuebec/Portail-BiodiversiteQuebec\n\n\n\nAtlas is web dashboard that shows species’s occurrency and presence in North America.\nIt have been developed using ReactJs\nRepository: https://github.com/ReseauBiodiversiteQuebec/tableau-atlas-react\n\n\n\nIt have been developed using ReactJs\nRepository: https://github.com/ReseauBiodiversiteQuebec/tableau-coleo-react\n\n\n\nIt have been developed using ReactJs\nRepository: https://github.com/ReseauBiodiversiteQuebec/io-layers-react\n\n\n\n\nIn every project we will have at least 3 branches: main/master,dev, staging. In the main branch we push everything that is ready to go to production(After decision makers give the OK). Every new feature has to be create in the dev branch or a new one (not main, staging). Once the feature is finished it will be merge to the staging branch so it could be reviewed and validated. Once it is validated the it is merge to main/master branch. The deployment of the updates are triggered using gitactions on every web app(staging and production).\n\n\n\nImage\n\n\n\n\n\nDocker is used to run our web apps and strapi. This give us the flexibility to move our apps to any server that support docker with the minimum streess since our container will have what our apps need to be able to run.\nIn every web app and strapi you will find a folder called docker that contains the docker setup for the different environment. You will found how to use it locally in the description of every web app. ( portal, Atlas, Inventaires)\n\n\nFour github runner created under the user github-runner will be listening for git action to trigger after a changed has been pushed.These runners run as a unbuntu service.\nrunners:\n\naction-runner-io,action-runner-atlas,action-runner-coleo,action-runner-portail\n\nEvery new github runner must be created under this user.\n\n\n\nImage",
    "crumbs": [
      "Serveurs",
      "Coleo-api"
    ]
  },
  {
    "objectID": "docs/serveurs/coleo-api/index.html#workflow",
    "href": "docs/serveurs/coleo-api/index.html#workflow",
    "title": "Coleo-api",
    "section": "",
    "text": "In every project we will have at least 3 branches: main/master,dev, staging. In the main branch we push everything that is ready to go to production(After decision makers give the OK). Every new feature has to be create in the dev branch or a new one (not main, staging). Once the feature is finished it will be merge to the staging branch so it could be reviewed and validated. Once it is validated the it is merge to main/master branch. The deployment of the updates are triggered using gitactions on every web app(staging and production).\n\n\n\nImage",
    "crumbs": [
      "Serveurs",
      "Coleo-api"
    ]
  },
  {
    "objectID": "docs/serveurs/coleo-api/index.html#devops",
    "href": "docs/serveurs/coleo-api/index.html#devops",
    "title": "Coleo-api",
    "section": "",
    "text": "Docker is used to run our web apps and strapi. This give us the flexibility to move our apps to any server that support docker with the minimum streess since our container will have what our apps need to be able to run.\nIn every web app and strapi you will find a folder called docker that contains the docker setup for the different environment. You will found how to use it locally in the description of every web app. ( portal, Atlas, Inventaires)\n\n\nFour github runner created under the user github-runner will be listening for git action to trigger after a changed has been pushed.These runners run as a unbuntu service.\nrunners:\n\naction-runner-io,action-runner-atlas,action-runner-coleo,action-runner-portail\n\nEvery new github runner must be created under this user.\n\n\n\nImage",
    "crumbs": [
      "Serveurs",
      "Coleo-api"
    ]
  },
  {
    "objectID": "docs/serveurs/serveurs_intro.html",
    "href": "docs/serveurs/serveurs_intro.html",
    "title": "Serveurs",
    "section": "",
    "text": "Quebec Biodiversity’s infrastructure is composed by several virtual machines (VH) in the Sherbrooke university servers and some servers on the cloud .",
    "crumbs": [
      "Serveurs"
    ]
  },
  {
    "objectID": "docs/serveurs/serveurs_intro.html#our-stack",
    "href": "docs/serveurs/serveurs_intro.html#our-stack",
    "title": "Serveurs",
    "section": "Our stack",
    "text": "Our stack\n\nReactJs\nnodeJs\nNextJs\nStyled-components\nMaterial UI\ntailwind\nfast-stac-api (python)\nR\ndocker\ngithub\npm2\nS3",
    "crumbs": [
      "Serveurs"
    ]
  },
  {
    "objectID": "docs/serveurs/serveurs_intro.html#virtual-machines",
    "href": "docs/serveurs/serveurs_intro.html#virtual-machines",
    "title": "Serveurs",
    "section": "Virtual machines:",
    "text": "Virtual machines:\nSynapse.vhost33: Serve as proxy inside of the UdeS servers networks.\npose.vhost33: It is used to run high performance computation.\nrweb.vhost33: Host Shinny apps (web framework for R).\ncoleo-media.vhost33: Host and process media files. It runs an API called coleo-media (nodeJs).\ncoleo-app.vhost33: Host the Coleo app. This apps is developed in AngularJs and NodeJs.\ncoleo-api.vhost33: Host APIs and Web apps.\nAPIs:\n\ncoleo-api: Used to have access to coleo database (will be migrated to postgRest soon).\npostgRest Atlas: built-in api to access to atlas database\npostgRest Coleo: built-in api to access to atlas database\npostgRest CNC: built-in api to access to atlas database\nplumber: R api library\nstac-fast-api: python api used to expose our STAC catalog\nstrapi: Headless CMS used as a backend for our web portal so team members can create and update content for our web portal.\nswagger doc / api testing: web allowing us to document and test our APIs. swagger ATLAS, swagger GEOIO\n\nDatabases:\n\natlas database (docker): Used to have access to coleo database (will be migrated to postgRest soon).\ncnc database (docker): built-in api to access to atlas database\n\nweb apps in production:\n\nBiodiversité Québec: (NextJs, styled-components, Material UI): web portal.\nInventaires: (ReactJs, styled-components):\nAtlas: (ReactJs, styled-components, Material UI, tailwind):\n\nweb apps in staging:\n\nBiodiversité Québec: (NextJs, styled-components, Material UI): web portal.\nInventaires: (ReactJs, styled-components)\nAtlas: (ReactJs, styled-components, Material UI, tailwind)\n\n\n\n\nImage",
    "crumbs": [
      "Serveurs"
    ]
  },
  {
    "objectID": "docs/serveurs/serveurs_intro.html#s3",
    "href": "docs/serveurs/serveurs_intro.html#s3",
    "title": "Serveurs",
    "section": "S3",
    "text": "S3\ns3cmd is used in our servers to communicate with our S3 server.\nInstallation and config: https://tecadmin.net/install-s3cmd-manage-amazon-s3-buckets/\nPlease contact dev team so they provide you the information you need to connect to the server S3.",
    "crumbs": [
      "Serveurs"
    ]
  },
  {
    "objectID": "docs/serveurs/coleo-app/index.html",
    "href": "docs/serveurs/coleo-app/index.html",
    "title": "Coleo-app",
    "section": "",
    "text": "Host the coleo web app developed in AngularJs. Coleo-app repository:\nto connect via ssh ssh &lt;your_user&gt;@coleo-app.vhost33",
    "crumbs": [
      "Serveurs",
      "Coleo-app"
    ]
  },
  {
    "objectID": "docs/serveurs/obs-web/index.html",
    "href": "docs/serveurs/obs-web/index.html",
    "title": "obs-web",
    "section": "",
    "text": "obs-web\nIt was used to host our portal web (Our portal now runs in coleo-api). Not used in this moment.\nto connect via ssh ssh &lt;your_user&gt;@obs-web.vhost33",
    "crumbs": [
      "Serveurs",
      "obs-web"
    ]
  },
  {
    "objectID": "docs/packages/gbifinsert/gbif_insert_intro.html",
    "href": "docs/packages/gbifinsert/gbif_insert_intro.html",
    "title": "gbif_insert",
    "section": "",
    "text": "Repo Github\ngbif_insert is a Python tool used to download occurrences data from www.gbif.org and inject it into atlas postgres database.",
    "crumbs": [
      "Packages",
      "gbif_insert",
      "gbif_insert"
    ]
  },
  {
    "objectID": "docs/packages/gbifinsert/gbif_insert_intro.html#installation",
    "href": "docs/packages/gbifinsert/gbif_insert_intro.html#installation",
    "title": "gbif_insert",
    "section": "Installation",
    "text": "Installation\n\nNew installation\npip3 install git+https://github.com/BiodiversiteQuebec/gbif_insert#egg=gbif_insert",
    "crumbs": [
      "Packages",
      "gbif_insert",
      "gbif_insert"
    ]
  },
  {
    "objectID": "docs/packages/gbifinsert/gbif_insert_intro.html#environment-variables",
    "href": "docs/packages/gbifinsert/gbif_insert_intro.html#environment-variables",
    "title": "gbif_insert",
    "section": "Environment variables",
    "text": "Environment variables\nConfiguration is done through environment variables. You can set them in your shell or in a .env file.\nTo use a dotenv file, create a .env file in the root directory of the project. This file will contain the environment variables required to run the script.\nGBIF_USER=... # GBIF account username\nGBIF_PASSWORD=... # GBIF account password\nGBIF_EMAIL=... # GBIF account email\nDB_HOST=localhost # Database host\nDB_PORT=5432 # Database port\nDB_NAME=atlas # Database name\nDB_USER=postgres # Database user\nDB_PASSWORD= # Database password, optional",
    "crumbs": [
      "Packages",
      "gbif_insert",
      "gbif_insert"
    ]
  },
  {
    "objectID": "docs/packages/gbifinsert/gbif_insert_intro.html#usage",
    "href": "docs/packages/gbifinsert/gbif_insert_intro.html#usage",
    "title": "gbif_insert",
    "section": "Usage",
    "text": "Usage\n\nCommand line\nScript scrupts/run.sh can be used to download and inject data from gbif into atlas database. It reads the environment variables from the .env file.\n./scripts/run.sh",
    "crumbs": [
      "Packages",
      "gbif_insert",
      "gbif_insert"
    ]
  },
  {
    "objectID": "docs/packages/gbifinsert/gbif_insert_intro.html#how-it-works",
    "href": "docs/packages/gbifinsert/gbif_insert_intro.html#how-it-works",
    "title": "gbif_insert",
    "section": "How it works",
    "text": "How it works\nThere are 3 parameters that must be sent to the Gbif object regarding the gbif account: username, password and email. These must be a valid gbif account parameters in order to trigger the download. Those variables can be set in the .env file or in the constructor of the Gbif object as described in the configuration section.\nPlease refer to the example below to try out an injection test.\nTest code example\n# Script pour lire les .csv et les envoyer dans la BD\n#from ast import Str\nimport time\nimport tempfile\nimport os\n\n# Méthode d'entrée du programme\n# In: aucun param (fonction main)\n# Out: fin du programme\ndef main():\n    # Create temp workdir for test\n    temp_dir = tempfile.mkdtemp(prefix='gbif_insert_test_')\n    cur_dir = os.getcwd()\n    os.chdir(temp_dir)\n\n    from gbif_insert.gbif import Gbif\n    import os\n    from dotenv import load_dotenv\n\n    # Load environment variables\n    load_dotenv()\n    GBIF_USER = os.getenv('GBIF_USER')\n    GBIF_PASSWORD = os.getenv('GBIF_PASSWORD')\n    GBIF_EMAIL = os.getenv('GBIF_EMAIL')\n\n    # Calculer le temps d'execution\n    start_time = time.time() \n\n    gbif_object = Gbif(GBIF_USER, GBIF_PASSWORD, GBIF_EMAIL)\n    \n    # Téléchargement des données, validation et injection\n    gbif_object.start()\n\n    print(\"Temps d'exécution: \" + str(time.time() - start_time) + \" secondes.\")\n    os.chdir(cur_dir)                                                                                                   \n\n# Point d'entrée\nmain()",
    "crumbs": [
      "Packages",
      "gbif_insert",
      "gbif_insert"
    ]
  },
  {
    "objectID": "docs/packages/ratlas/ratlas_intro.html",
    "href": "docs/packages/ratlas/ratlas_intro.html",
    "title": "Paquet ratlas",
    "section": "",
    "text": "ratlas est un paquet R permettant l’accès et l’analyse des données de biodiversité aggrégée par le réseau d’observation de la biodiversité du Québec.\nCe paquet R expose les services RESTFull de l’API de Atlas. Atlas est un système d’information sur la biodiversité du Québec développé par le laboratoire d’Écologie Intégrative de l’Université de Sherbrooke.\nLa documentation complète du package est disponible ici.\nPour débuter avec le package ratlas nous vous recommendons l’article pour débuter par le tutoriel pour le téléchargement d’observations.\n\n\ndevtools::install_github(\"BiodiversiteQuebec/ratlas\")\n\n\n\nUn jeton d’accès vous sera mis à disposition par l’équipe de développeur de Atlas. Veuillez les contacter pour en obtenir un de manière sécurisée.\nIl est fortement recommandé de mettre en cache votre jeton d’accès (jeton d’accès stocké dans un fichier rds) afin de s’assurer qu’il ne soit pas visible ou transmis avec votre code à un autre utilisateur. Ce jeton d’accès est unique et révocable.\nPour cela, il vous suffit simplement d’enregistrer le jeton d’accès directement en tant que Environment variable\n\n\nfile.edit(\"~/.Renviron\")\nCette ligne va ouvrir un ficher text dans votre Rstudio. Rajoutez dans ce ficher un linge comme la suivante:\n# utilizez votre propre token ici\n\nATLAS_API_TOKEN=7f8df438e1be96a18436e9dab5d97d68ed0e0441d9b68f59e0ce631b2919f3aa\nLe jeton d’accès est un exemple ici et n’est aucunement valide.\n\n\n\nVous pouvez également passer votre jeton d’accès en créant une variable d’environnement nommée ATLAS_API_TOKEN au niveau de l’OS ou de l’environnement de dévelopement.\n\n\n\n\n\n\nSi vous obtenez une erreur 401, c’est que votre jeton d’accès n’est pas valide. Veuillez vous assurer que vous avez bien copié le jeton d’accès et que vous l’avez bien mis en cache dans votre environnement de développement.\nPour valider que votre jeton d’accès est bien en cache, vous pouvez utiliser la fonction Sys.getenv(\"ATLAS_API_TOKEN\") qui devrait retourner votre jeton d’accès.\n\n\n\nErreur possible lors de la récupération (GET) de données via une machine Linux (ici Ubuntu). L’erreur provient du package curl. Un correctif est disponible depuis la version 7.87.0 du package qui elle est disponible depuis la version 23.02 de Ubuntu.",
    "crumbs": [
      "Packages",
      "ratlas",
      "Paquet ratlas"
    ]
  },
  {
    "objectID": "docs/packages/ratlas/ratlas_intro.html#installer-le-paquet-ratlas",
    "href": "docs/packages/ratlas/ratlas_intro.html#installer-le-paquet-ratlas",
    "title": "Paquet ratlas",
    "section": "",
    "text": "devtools::install_github(\"BiodiversiteQuebec/ratlas\")",
    "crumbs": [
      "Packages",
      "ratlas",
      "Paquet ratlas"
    ]
  },
  {
    "objectID": "docs/packages/ratlas/ratlas_intro.html#sauthentifier-auprès-de-lapi",
    "href": "docs/packages/ratlas/ratlas_intro.html#sauthentifier-auprès-de-lapi",
    "title": "Paquet ratlas",
    "section": "",
    "text": "Un jeton d’accès vous sera mis à disposition par l’équipe de développeur de Atlas. Veuillez les contacter pour en obtenir un de manière sécurisée.\nIl est fortement recommandé de mettre en cache votre jeton d’accès (jeton d’accès stocké dans un fichier rds) afin de s’assurer qu’il ne soit pas visible ou transmis avec votre code à un autre utilisateur. Ce jeton d’accès est unique et révocable.\nPour cela, il vous suffit simplement d’enregistrer le jeton d’accès directement en tant que Environment variable\n\n\nfile.edit(\"~/.Renviron\")\nCette ligne va ouvrir un ficher text dans votre Rstudio. Rajoutez dans ce ficher un linge comme la suivante:\n# utilizez votre propre token ici\n\nATLAS_API_TOKEN=7f8df438e1be96a18436e9dab5d97d68ed0e0441d9b68f59e0ce631b2919f3aa\nLe jeton d’accès est un exemple ici et n’est aucunement valide.\n\n\n\nVous pouvez également passer votre jeton d’accès en créant une variable d’environnement nommée ATLAS_API_TOKEN au niveau de l’OS ou de l’environnement de dévelopement.",
    "crumbs": [
      "Packages",
      "ratlas",
      "Paquet ratlas"
    ]
  },
  {
    "objectID": "docs/packages/ratlas/ratlas_intro.html#troubleshooting",
    "href": "docs/packages/ratlas/ratlas_intro.html#troubleshooting",
    "title": "Paquet ratlas",
    "section": "",
    "text": "Si vous obtenez une erreur 401, c’est que votre jeton d’accès n’est pas valide. Veuillez vous assurer que vous avez bien copié le jeton d’accès et que vous l’avez bien mis en cache dans votre environnement de développement.\nPour valider que votre jeton d’accès est bien en cache, vous pouvez utiliser la fonction Sys.getenv(\"ATLAS_API_TOKEN\") qui devrait retourner votre jeton d’accès.\n\n\n\nErreur possible lors de la récupération (GET) de données via une machine Linux (ici Ubuntu). L’erreur provient du package curl. Un correctif est disponible depuis la version 7.87.0 du package qui elle est disponible depuis la version 23.02 de Ubuntu.",
    "crumbs": [
      "Packages",
      "ratlas",
      "Paquet ratlas"
    ]
  },
  {
    "objectID": "docs/packages/rcoleo/rcoleo_intro.html",
    "href": "docs/packages/rcoleo/rcoleo_intro.html",
    "title": "Paquet rcoleo",
    "section": "",
    "text": "Repo Github",
    "crumbs": [
      "Packages",
      "rcoleo",
      "Paquet rcoleo"
    ]
  },
  {
    "objectID": "docs/packages/rcoleo/rcoleo_intro.html#installer-le-paquet-rcoleo",
    "href": "docs/packages/rcoleo/rcoleo_intro.html#installer-le-paquet-rcoleo",
    "title": "Paquet rcoleo",
    "section": "Installer le paquet rcoleo",
    "text": "Installer le paquet rcoleo\ndevtools::install_github(\"BiodiversiteQuebec/rcoleo\")",
    "crumbs": [
      "Packages",
      "rcoleo",
      "Paquet rcoleo"
    ]
  },
  {
    "objectID": "docs/packages/rcoleo/rcoleo_intro.html#sauthentifier-auprès-de-lapi",
    "href": "docs/packages/rcoleo/rcoleo_intro.html#sauthentifier-auprès-de-lapi",
    "title": "Paquet rcoleo",
    "section": "S’authentifier auprès de l’API",
    "text": "S’authentifier auprès de l’API\nIl est fortement recommandé d’enregistrer le jeton d’accès directement en tant que Environment variable afin de s’assurer qu’il ne soit pas visible ou transmis avec votre code à un autre utilisateur. Ce jeton d’accès est unique et révocable.\nfile.edit(\"~/.Renviron\")\ncette linge va ouvrir un ficher text dans votre Rstudio. Ajoutez à ce ficher la ligne suivante:\nRCOLEO_TOKEN=7f8df438e1be96a18436e9dab5d97d68ed0e0441d9b68f59e0ce631b2919f3aa\n(utilizez votre propre token ici)\nLe jeton d’accès est un exemple ici et n’est aucunement valide.",
    "crumbs": [
      "Packages",
      "rcoleo",
      "Paquet rcoleo"
    ]
  },
  {
    "objectID": "docs/packages/bdqctaxa/bdqc_taxa_intro.html",
    "href": "docs/packages/bdqctaxa/bdqc_taxa_intro.html",
    "title": "bdqc_taxa",
    "section": "",
    "text": "Repo Github\nbdqc_taxa is a python package that interface with Biodiversité Québec’s database to query reference taxa sources, parse their return and generate records.",
    "crumbs": [
      "Packages",
      "bdqc_taxa",
      "bdqc_taxa"
    ]
  },
  {
    "objectID": "docs/packages/bdqctaxa/bdqc_taxa_intro.html#installation",
    "href": "docs/packages/bdqctaxa/bdqc_taxa_intro.html#installation",
    "title": "bdqc_taxa",
    "section": "Installation",
    "text": "Installation\nFor installation in postgres server\nInstallation must be performed as postgres user :\nsudo su postgres\npip install ...\nFor a new installation\npip install git+https://github.com/BiodiversiteQuebec/bdqc_taxa#egg=bdqc_taxa\nFor an upgrade\npip install --upgrade git+https://github.com/BiodiversiteQuebec/bdqc_taxa#egg=bdqc_taxa",
    "crumbs": [
      "Packages",
      "bdqc_taxa",
      "bdqc_taxa"
    ]
  },
  {
    "objectID": "docs/packages/bdqctaxa/bdqc_taxa_intro.html#usage",
    "href": "docs/packages/bdqctaxa/bdqc_taxa_intro.html#usage",
    "title": "bdqc_taxa",
    "section": "Usage",
    "text": "Usage\n\nMatch a scientific name against all sources\nThe taxa_ref module is used to query the reference taxa sources and parse their return using fuzzy matching.\nFor a specific match, the module provides functions to return scientific name, taxonomic hierarchy, vernacular name and source of the match. For each source, valid scientific names are returned as well as synonyms when corresponding to the matched input name. Parent taxa are returned as well as children taxa when corresponding to the matched input name.\n\n\nExample\nThis file was generated on 2024-04-23 from Eliso’s Répertoire des noms d’invertébrés du Québec (2022) file. The file was downloaded from https://www.eliso.ca/documents on 2024-04-23. The file was parsed using the script scripts/make_eliso.py.\nColumns: The file contains a pandas dataframe with the following columns: taxa_name: Scientific name of the taxon vernacular_fr: French vernacular name of the taxon taxa_rank: Taxon rank Embranchement: Phylum Classe: Class Ordre: Order Famille: Family Genre: Genus Espèce: Species\nNotes: The entries have no recorded author. The entries may contain comments in parentheses that are kept as is but may prevent matching. Query for all sources using a scientific name can be done with the following method. This should fits most use cases.\nfrom bdqc_taxa.taxa_ref import TaxaRef\n\nresults = TaxaRef.from_all_sources('Canis lupus')\n\n\nComplex names\nWhen the taxon related to an observation is complex, such as multiple organism are identified for the same observation(Species 1 | Species 2 | Species 3), a single observed taxonomic entry is injected as such. References will be obtained for each single organism listed by the complex and all related parents. References matched from complex observed taxons are identified as such and can then be included or discarded from queries performed by the user. Common parent taxon are identified as such and can be used to query complex observed taxons.\n\n\nConflicts\nCertain scientific names corresponds to different organism within two entirely different branches of the tree of life. For example, the scientific name Salix corresponds to the genus of willows in the plant kingdom and to a genus of tunicates in the animal kingdom. To avoid matching for such case, the user can specify a parent taxa name to restrict the results to the branch containing the parent taxa. For example, the user can specify the parent taxa name Plantae to restrict the results to the plant kingdom.\n\nImportant All sources might be match at least using kingdom or phylum level parent taxa name. However, only some sources make available the whole taxa hierarchy. Filtering results using parent_taxa with other ranks might not return any results for certain sources (e.g. Bryoquel, VASCAN, CDPNQ).\nWe thus HIGHLY recommend to use parent_taxa with kingdom or phylum level parent taxa name.\n\nThe parent_taxa argument is optional. If not specified, the module will return all results for the given scientific name.\nfrom bdqc_taxa.taxa_ref import TaxaRef\n\nresults = TaxaRef.from_all_sources('Salix', parent_taxa='Plantae')",
    "crumbs": [
      "Packages",
      "bdqc_taxa",
      "bdqc_taxa"
    ]
  },
  {
    "objectID": "docs/packages/bdqctaxa/bdqc_taxa_intro.html#find-vernacular-names-for-a-scientific-name",
    "href": "docs/packages/bdqctaxa/bdqc_taxa_intro.html#find-vernacular-names-for-a-scientific-name",
    "title": "bdqc_taxa",
    "section": "Find vernacular names for a scientific name",
    "text": "Find vernacular names for a scientific name\nThe taxa_vernacular module is used to query the reference taxa sources and parse their return in english and french.\nFor a specific match, the module provides functions to return the accepted vernacular names in english and french. The rank order of the sources is used to determine the accepted vernacular name.\n\nExample\nQuery for all sources using a scientific name can be done with the following method. This should fits most use cases.\nfrom bdqc_taxa.vernacular import Vernacular\n\nresults = Vernacular.from_match('Canis lupus')\n\n\nSynonyms\nFor certain sources, such as CDPNQ, the vernacular name will be returned for accepted synonyms. If observed scientific name differs, the user should do multiple queries for each known synonyms.",
    "crumbs": [
      "Packages",
      "bdqc_taxa",
      "bdqc_taxa"
    ]
  },
  {
    "objectID": "docs/packages/bdqctaxa/bdqc_taxa_intro.html#sources",
    "href": "docs/packages/bdqctaxa/bdqc_taxa_intro.html#sources",
    "title": "bdqc_taxa",
    "section": "Sources",
    "text": "Sources\n\nGlobal Names Resolver (GNR) : Retrieve the scientific name and the taxonomic hierarchy of a given name against many sources. Selected sources are VASCAN, ITIS and COL. Query is performed against the GNR API.\nGBIF : Retrieve the scientific name and the taxonomic hierarchy of a given name against the GBIF backbone taxonomy. Query is performed against the GBIF API.\nBryoquel : Implemented by the Biodiversité Québec team, this source is used to retrieve the scientific name and the taxonomic hierarchy of a given name against the Bryophytes of Québec. Query is performed against the the custom_sources sqlite database implemented by the Biodiversité Québec team using the Bryoquel excel file. See below for more details.\nCDPNQ : Implemented by the Biodiversité Québec team, this source is used to retrieve the scientific name and the taxonomic hierarchy of a given name against the list for the Centre de données sur le patrimoine naturel du Québec. Query is performed against the custom_sources sqlite database implemented by the Biodiversité Québec team using the CDPNQ excel files. See below for more details.\nEliso : Implemented by the Biodiversité Québec team, this source is used to retrieve the scientific name and the taxonomic hierarchy of a given name against Eliso’s Répertoire des noms d’invertébrés du Québec. Query is performed against the custom_sources sqlite database implemented by the Biodiversité Québec team using the Répertoire excel file. See below for more details.\nWikidata : Retrieve the vernacular name of a given scientific name against Wikidata. Query is performed against the Wikidata API.\n\n\nModules\nWrapper functions to query the sources using either api or the sqlite database are individually implemented in modules gbif, global_names, bryoquel, cdpnq and wikidata.",
    "crumbs": [
      "Packages",
      "bdqc_taxa",
      "bdqc_taxa"
    ]
  },
  {
    "objectID": "docs/packages/bdqctaxa/bdqc_taxa_intro.html#custom-sources",
    "href": "docs/packages/bdqctaxa/bdqc_taxa_intro.html#custom-sources",
    "title": "bdqc_taxa",
    "section": "Custom sources",
    "text": "Custom sources\nThese tables containts the custom sources used by the taxa_ref module. They are implemented in the custom_sources sqlite database. The database is located in the bdqc_taxa package directory.\n\nTABLE bryoquel\n\nDescription:\nThis file was generated on 2022-09-21 from the Bryoquel taxonomy file.\nThe file was downloaded from http://societequebecoisedebryologie.org/Bryoquel.html on 2022-09-21.\nThe last version of the bryoquel xlsx file is from 2022-09-12`.\nThe file was parsed using the script `scripts/parse_bryoquel.py`.\nThe file was parsed using the script parse_bryoquel.ipynb.\n\n\nColumns:\nThe file contains a pandas dataframe with the following columns:\nid: the Bryoquel IDtaxon\nscientific_name: Noms latins acceptés du taxon, sans auteur\ntaxon_rank: Taxon rank\ngenus: Taxon genus\nfamily: Taxon family\nclade: Taxon clade\ncanonical_full: Noms latins acceptés du taxon, avec auteur\nauthorship: Auteur obtenu de Noms latins acceptés\nvernacular_name_fr: Noms français acceptés\nvernacular_name_en: Noms anglais acceptés\n\n\n\nTABLE cdpnq_odonates\n\nDescription:\nThis file was generated from the CDPNQ odonates data file.\nThe file was obtained from Anouk.Simard@mffp.gouv.qc.ca on May 24, 2022\nThe last version of the bryoquel xlsx file is from 2022-09-12`.\nThe file was parsed using the script `scripts/make_cdpnq_odonates.py`.\n\n\nColumns:\nname: scientific name\nvalid_name: valid scientific name\nrank: rank of the taxa\nsynonym: boolean indicating if the name is a synonym\nauthor: author of the scientific name\ncanonical_full: canonical full name\nvernacular_fr: vernacular name in French\nvernacular_fr2: vernacular name in French from Natureserve\n\n\n\nTABLE cdpnq_vertebrates\n\nDescription:\nThis file was generated from the Liste de la faune vert�br�e du Qu�bec (LFVQ) Data file LFVQ_05_12_2022.xlsx \nThe file was obtained from Donn�es Qu�bec on 2023-01-12.\nThe last version of thefile is from 2022-12-05`.\nThe file was parsed using the script `scripts/make_cdpnq_vertebrates.py`.\n\n\nColumns:\nname: scientific name\nvalid_name: valid scientific name\nrank: rank of the taxa\nsynonym: boolean indicating if the name is a synonym\nauthor: author of the scientific name\ncanonical_full: canonical full name\nvernacular_fr: vernacular name in French\nvernacular_en: vernacular name in English\nNotes: The entries have no recorded author.\n\n\n\nTABLE eliso_invertebrates\n\nDescription:\nThis file was generated on 2024-04-23 from Eliso's Répertoire des noms d’invertébrés du Québec (2022) file.\nThe file was downloaded from https://www.eliso.ca/documents on 2024-04-23.\nThe file was parsed using the script `scripts/make_eliso.py`.\\n\n\n\nColumns:\nThe file contains a pandas dataframe with the following columns:\ntaxa_name: Scientific name of the taxon\nvernacular_fr: French vernacular name of the taxon\ntaxa_rank: Taxon rank\nEmbranchement: Phylum\nClasse: Class\nOrdre: Order\nFamille: Family\nGenre: Genus\nEspèce: Species\\n\nNotes: The entries have no recorded author. The entries may contain comments in parentheses that are kept as is but may prevent matching.",
    "crumbs": [
      "Packages",
      "bdqc_taxa",
      "bdqc_taxa"
    ]
  },
  {
    "objectID": "docs/portails/strapi/webhooks.html",
    "href": "docs/portails/strapi/webhooks.html",
    "title": "Webhooks",
    "section": "",
    "text": "Webhooks are functions that allow strapi to notify any Content-type or collection changes. The following example explains how to set up a webhook.\nLet’s say you create a Content-type called Artcile. Strapi will create a folder for this component behind scene. The folder will be localted in &lt;StrapiProjectRootFolder&gt;/src/api/article. This folder will contain a folder called content-types/article.\n\n\n\nImage\n\n\nInside this folder create file called lifecycles.js with the following code:\nmodule.exports = {\n // trigger after  an element is created\n afterCreate(event) {\n   // do something to the result;\n },\n\n // trigger before  an element is updated\n async beforeUpdate(event) {\n   // do something to the result;\n },\n\n // trigger after  an element is updated\n afterUpdate(event) {\n   // do something to the result;\n },\n\n // trigger after  an element is deleted\n afterDelete(event) {\n   // do something to the result;\n },\n};\neach function is called upon the corresponding action (create, update or delete). Inside the functions you could make http calls to notify other apps that a change has been done in the system.\nIn our case we created module that is reused each time that want to update our web portal. The function is locatad in the folder &lt;RootPath&gt;/config/functions/lifecyclesCalls.js. The function has a param the tells the web portal what page need to be updated.\nHere is how we use the function:\n&lt;StrapiProjectRootFolder&gt;/src/api/article/lifecycles.js:\nconst lifecycleCalls = require(\"../../../../../config/functions/lifecycleCalls\");\n\nmodule.exports = lifecycleCalls(\"decouverte\", \"dynamic\");\nIn case someone wants to override a webhook, a module like the one shown in the first code snippets must be created.\nStrapi webhook documentation",
    "crumbs": [
      "Portails",
      "Strapi",
      "Webhooks"
    ]
  },
  {
    "objectID": "docs/portails/strapi/media.html",
    "href": "docs/portails/strapi/media.html",
    "title": "Media files",
    "section": "",
    "text": "Strapi allows us to upload images which are resized to diferent formats. In this section shows how to upload images so it could be served by strapi API.\n1 - Login to your strapi account\n2 - Go to Media Library (Menu at Top Left). There you will find a list of folder where you can search or storage images. You could also create new folders if you need.\n\n\n\nImage\n\n\n3 - To add new image click in Add new asset button (Top Right). Inf you want to add a new folder then click in the Add new folder.\n4 - After clicking Add new assets button a popup will appear asking for the source of the image. You must select browse files if the image is in your computer or add the url of the image in case you go to the tab FROM URL.\n\n\n\nImage\n\n\n\n\n\nImage",
    "crumbs": [
      "Portails",
      "Strapi",
      "Media files"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_API_endpoints.html",
    "href": "docs/portails/coleo/coleo_API_endpoints.html",
    "title": "Endpoints & CRON jobs",
    "section": "",
    "text": "Repo Github",
    "crumbs": [
      "Portails",
      "Coleo",
      "Endpoints & CRON jobs"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_API_endpoints.html#api-endpoints",
    "href": "docs/portails/coleo/coleo_API_endpoints.html#api-endpoints",
    "title": "Endpoints & CRON jobs",
    "section": "API endpoints",
    "text": "API endpoints\nFUNCTION api.taxa_abundance(group_by_column text, cell_id_filter integer, cell_code_filter text, site_id_filter integer, site_code_filter text, site_type_filter text, campaign_id_filter integer, campaign_type_filter text)\nReturns a table with the sum of observation values from obs_species and obs_edna for each taxon, grouped by the column name specified as parameter and filtered by column values. The column names accepted can be any of : site_id, site_type, site_code, campaign_id, campaign_type, cell_id, cell_code.\nAll parameters are optionals and can be used in any combination. If no parameter is specified, the function will return the total abundance.\nFUNCTION api.taxa_abundance_year\nReturns the same table, expect the abundance is returned for each year of the campaigns.\nFUNCTION api.taxa_richness(group_by_column text, cell_id_filter integer, cell_code_filter text, site_id_filter integer, site_code_filter text, site_type_filter text, campaign_id_filter integer, campaign_type_filter text)\nReturns a table with the number of validated and unique taxa, based on the tip of the branch method, from both obs_species and obs_edna grouped by the column name specified as parameter and filtered by column values. The column names accepted can be any of : site_id, site_type, site_code, campaign_id, campaign_type, cell_id, cell_code.\nAll parameters are optionals and can be used in any combination. If no parameter is specified, the function will return the total richness.\nFUNCTION api.taxa_richness_year\nReturns the same table, expect the richness is returned for each year of the campaigns.",
    "crumbs": [
      "Portails",
      "Coleo",
      "Endpoints & CRON jobs"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_API_endpoints.html#cron-jobs",
    "href": "docs/portails/coleo/coleo_API_endpoints.html#cron-jobs",
    "title": "Endpoints & CRON jobs",
    "section": "Cron jobs",
    "text": "Cron jobs\nCertain tasks are executed periodically by the server. These tasks are called cron jobs and scripts are defined in the ./cron/ folder. These tasks are:\nScripts must be executed under the postgres user.\n\n./cron/refresh_taxa.sql: Refreshes the taxa_ref table, the taxa_vernacular table, and the api.taxa materialized view. This script is executed every Friday at 9:00 PM.\n\n\nEditing cron jobs for postgres user\nFirst, make sure the postgres home directory contains the latest version of the Coleo_DB repository located in /home/postgres/Coleo_DB . If not, run the following command:\nsudo -u postgres git -C /home/postgres/Coleo_DB pull\nTo edit the cron jobs for the postgres user, run the following command:\nsudo crontab -u postgres -e\nThis will open the crontab file for the postgres user. Copy the lines from the ./cron/crontab file and paste them at the end of the file. Save the file and exit.",
    "crumbs": [
      "Portails",
      "Coleo",
      "Endpoints & CRON jobs"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html",
    "href": "docs/portails/coleo/coleo_structure.html",
    "title": "Structure",
    "section": "",
    "text": "Repo Github",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#champs-communs-à-la-plupart-des-tables",
    "href": "docs/portails/coleo/coleo_structure.html#champs-communs-à-la-plupart-des-tables",
    "title": "Structure",
    "section": "Champs communs à la plupart des tables",
    "text": "Champs communs à la plupart des tables\nLes champs en gras sont obligatoires\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nid\nnombre entier\nIdentifiant unique\n\n\n\ncreated_at\ndate-heure\nDate et heure de création\n\n\n\nupdated_at\ndate-heure\nDate et heure de mise à jour",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#cellules",
    "href": "docs/portails/coleo/coleo_structure.html#cellules",
    "title": "Structure",
    "section": "Cellules",
    "text": "Cellules\nNom de la table : cells\nPoint d’accès : /api/v1/cells\nInclus dans les résultats : sites\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nname\ntexte\nNom de la cellule\n\n\n\ncell_code\ntexte\nCode de la cellule\n\n\n\ngeom\ngeometry\nLocalisation de la cellule",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#sites",
    "href": "docs/portails/coleo/coleo_structure.html#sites",
    "title": "Structure",
    "section": "Sites",
    "text": "Sites\nNom de la table : sites\nPoint d’accès : /api/v1/sites\nInclus dans les résultats : campaigns, cells\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\ncell_id\nnombre entier\nIdentifiant de la cellule\n\n\n\noff_station_code_id\ntexte\n\n\n\n\nsite_code\ntexte\nIdentifiant unique du site\n\n\n\nsite_name\ntexte\nNom du site. Par exemple, le nom du lac.\n\n\n\ntype\nchoix\nType d’inventaire réalisé sur le site\n‘lac’, ‘rivière’, ‘forestier’, ‘marais’, ‘milieu humide côtier’, ‘toundrique’, ‘tourbière’\n\n\nopened_at\ndate\nDate de l’ouverture du site\n\n\n\ngeom\ngeometry\nLocalisation du site\n\n\n\nnotes\ntexte\nCommentaires",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#campagnes",
    "href": "docs/portails/coleo/coleo_structure.html#campagnes",
    "title": "Structure",
    "section": "Campagnes",
    "text": "Campagnes\nNom de la table : campaigns\nPoint d’accès : /api/v1/campaings\nInclus dans le résultat : efforts, environments, devices, lures, landmarks(+thermographs), traps\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nsite_id\ntexte\nIdentifiant unique du site attaché à la campagne d’échantillonnage\n\n\n\ntype\nchoix\nLe type campagne réalisé\n‘végétation’, ‘végétation_transect’, ‘sol’, ‘décomposition_sol’, ‘acoustique_chiroptères’,‘acoustique_oiseaux’,‘acoustique_anoures’,‘acoustique_orthoptères’, ‘phénologie’, ‘mammifères’, ‘papilionidés’, ‘odonates’, ‘insectes_sol’, ‘ADNe’,‘zooplancton’, ‘thermographe’\n\n\ntechnicians\nARRAY(texte)\nNoms des technicien(ne)s\n\n\n\nopened_at\ndate\nDate d’ouverture de la campagne d’échantillonnage\n\n\n\nclosed_at\ndate\nDate de fermeture de la campagne d’échantillonnage\n\n\n\nnotes\ntexte\nCommentaires",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#efforts",
    "href": "docs/portails/coleo/coleo_structure.html#efforts",
    "title": "Structure",
    "section": "Efforts",
    "text": "Efforts\nNom de la table : efforts\nPoint d’accès : /api/v1/efforts\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\ncampaing_id\nnombre entier\nNuméro d’identification de la campagne\n\n\n\nstratum\nchoix\nStrate de végétation concernée par l’effort d’échantillonage\n‘arbres’, ‘arbustes/herbacées’, ‘bryophytes’\n\n\ntime_start\ndate et heure\nDate et heure de début de l’inventaire\n\n\n\ntime_finish\ndate et heure\nDate et heure de fin de l’inventaire\n\n\n\nsamp_surf\nnombre décimal\nTaille de la surface d’échantillonage\n\n\n\nsamp_surf_unit\nchoix\nUnité de mesure utilisé pour la surface d’échantillonnage\n‘cm2’, ‘m2’, ‘km2’\n\n\nrecording_minutes\nnombre entier\nNombre de minutes d’enregistrement\n\n\n\nfraction_benthos\nnombre décimal\nFraction de l’échantillon de benthos analysé\n0-1\n\n\nphoto_count\nnombre entier\nNombre de photos\n\n\n\nnotes\ntexte\nCommentaires",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#environnements",
    "href": "docs/portails/coleo/coleo_structure.html#environnements",
    "title": "Structure",
    "section": "Environnements",
    "text": "Environnements\nNom de la table : environments\nPoint d’accès : /api/v1/environment\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\ncampaing_id\nnombre entier\nNuméro d’identification de la campagne\n\n\n\nwind\nchoix 1-5\nVent en km/h\n‘1’=‘calme (moins de 1 km/h)’, ‘2’=‘très légère brise (1 à 5 km/h)’, ‘3’=‘légère brise (6 à 11 km/h)’, ‘4’=‘petite brise (12 à 19 km/h)’, ‘5’=jolie brise (20 à 28 km/h)’\n\n\nsky\nchoix 1-5\nAllure du ciel\n‘1’=‘dégagé (0 à 10 %)’, ‘2’=‘partiellement nuageux (10 à 50 %)’, ‘3’=‘nuageux (50 à 90 %)’, ‘4’=‘pluvieux’, ‘5’=‘orageux’\n\n\ntemp_c\nnombre décimal\nTempérature en celcius\n\n\n\nextra\njson\nAutres valeurs pour des colonnes additionnelles en format json\n{“largeur_riviere”: { type: ‘double’, description: ‘Largeur de la rivière’, units: ‘m’, value: 7.12}}\n\n\nnotes\ntexte\nCommentaires",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#appareils",
    "href": "docs/portails/coleo/coleo_structure.html#appareils",
    "title": "Structure",
    "section": "Appareils",
    "text": "Appareils\nNom de la table : devices\nPoint d’accès : /api/v1/devices\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\ncampaing_id\nnombre entier\nNuméro d’identification de la campagne\n\n\n\nsd_card_codes\nARRAY(texte)\nNuméro d’identification des cartes SD utilisées\n\n\n\ncam_code\nARRAY(texte)\nNuméro d’identification de la caméra utilisée\n\n\n\ncam_h_cm\nnombre décimal\nHauteur de la camera en centimètres\n\n\n\nmic_logger_code\ntexte\nNuméro d’identification du enregistreur utilisé\n\n\n\nmic_acc_code\ntexte\nNuméro d’identification du microphone accoustique utilisé\n\n\n\nmic_h_cm_acc\nnombre décimal\nHauteur du microphone ultrason utilisé en centimètres\n\n\n\nmic_ultra_code\ntexte\nHauteur du microphone ultrason utilisé en centimètres\n\n\n\nmic_orientation\nchoix\nOrientation du dispositif\n‘n’, ‘s’, ‘e’, ‘o’, ‘ne’, ‘no’, ‘se’, ‘so’",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#appâts",
    "href": "docs/portails/coleo/coleo_structure.html#appâts",
    "title": "Structure",
    "section": "Appâts",
    "text": "Appâts\nNom de la table : lures\nPoint d’accès : /api/v1/lures\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nlure\nnombre entier\nType de leurre ou appât utilisé pour le dispositif\n\n\n\ncampaign_id\nnombre entier\nNuméro d’identification de la campagne\n\n\n\ninstalled_at\ndate\nDate d’installation de l’appât/leurre",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#pièges",
    "href": "docs/portails/coleo/coleo_structure.html#pièges",
    "title": "Structure",
    "section": "Pièges",
    "text": "Pièges\nNom de la table : traps\nPoint d’accès : /api/v1/traps\nInclus dans le résultat : landmarks, samples.\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\ntrap_code\ntexte\nCode du piège\n\n\n\ncampaign_id\ntexte\nCode d’identification de la campagne\n\n\n\nnotes\ntexte\nCommentaires",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#repères",
    "href": "docs/portails/coleo/coleo_structure.html#repères",
    "title": "Structure",
    "section": "Repères",
    "text": "Repères\nNom de la table : landmarks\nPoint d’accès : /api/v1/landmarks\nInclus dans le résultat : thermographs\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\ncampaing_id\nnombre entier\nNuméro d’identification de la campagne\n\n\n\ntree_code\ntexte\nIdentifiant unique de l’arbre repère\n\n\n\ntaxa_name\ntexte\nEspèce de l’arbre repère\n\n\n\ndbh\nnombre entier\nDHP de l’arbre repère\n\n\n\ndbh_unit\nchoix\nUnité pour le DHP\n‘mm’,‘cm’,‘m’\n\n\naxis\nchoix\nL’axe du transect pour la végétation\n‘n’,‘se’,‘so’\n\n\nazimut\nnombre entier\nAzimut du dispositif/appât/borne depuis le repère (arbre ou borne), entre 0 et 360\n\n\n\ndistance\nnombre décimal\nDistance du dispositif/appât/borne depuis le repère (arbre ou borne)\n\n\n\ndistance_unit\nchoix\nDistance du dispositif/appât/borne depuis le repère (arbre ou borne)\n‘mm’,‘cm’,‘m’\n\n\ngeom\ngeometry(POINT)\nPosition du repère\n\n\n\ntype\nchoix\nType de repère\n‘gps’, ‘arbre’, ‘gps+arbre’, ‘borne_axe’, ‘thermographe’\n\n\nthermograph_type\nchoix\nType de thermographe\n‘eau’, ‘eau_extérieur’, ‘sol’, ‘sol_extérieur’, ‘puit_marais’\n\n\ntrap_id\nnombre entier\nIdentifiant du piège\n\n\n\nlure_id\nnombre entier\nIdentifiant de l’appât\n\n\n\ndevice_id\nnombre entier\nIdentifiant de l’appareil\n\n\n\nnotes\ntexte\nCommentaires",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#échantillons",
    "href": "docs/portails/coleo/coleo_structure.html#échantillons",
    "title": "Structure",
    "section": "Échantillons",
    "text": "Échantillons\nNom de la table : samples\nPoint d’accès : /api/v1/samples\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nsample_code\ntexte\nNuméro de l’échantillon\n\n\n\ndate_samp\ndate\nDate de collecte de l’échantillon\n\n\n\ntrap_id\nnombre entier\nNuméro d’identification unique du piège\n\n\n\nnotes\ntexte\nCommentaires",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#thermographes",
    "href": "docs/portails/coleo/coleo_structure.html#thermographes",
    "title": "Structure",
    "section": "Thermographes",
    "text": "Thermographes\nCette table est utilisée pour faire la liste de thermographes pour la température de l’eau et du sol, ainsi que les appareils de profondeur d’eau et température utilisée dans les marais. Cette table est liée et à la table des repères et à la table des obs_thermograph, puisqu’il peut y avoir plusieurs thermographes à un même endroit, et des milliers d’observations pour un même thermographe.\nNom de la table : thermographs\nPoint d’accès : /api/v1/thermographs\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nlandmark_id\nnombre entier\nNuméro du repère\n\n\n\nthermograph_no\ntexte\nNuméro/code du thermographe\n\n\n\ndepth\nnombre décimal\nProfondeur dans l’eau ou dans le sol (cm)\n\n\n\nheight\nnombre décimal\nHauteur pour les thermographes extérieurs (cm)\n\n\n\nis_on_bag\nbooléen 1/0\nEst-ce le dernier thermographe sur le sac de la chaîne?\n\n\n\nshading\nnombre entier\nOmbrage de 1 (aucun ombrage) à 5 (complètement ombragé)\n\n\n\nnotes\ntexte\nCommentaires",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#observations",
    "href": "docs/portails/coleo/coleo_structure.html#observations",
    "title": "Structure",
    "section": "Observations",
    "text": "Observations\nNom de la table : observations\nPoint d’accès : /api/v1/observations\nInclus dans le résultat: media, obs_soil, obs_species, obs_soil_decomposition\nCette table est la table principale qui contient les informations communes à toutes les observations. Dépendamment du type de campagne, les informations complémentaires sont dans les tables obs_*\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\ndate_obs\ndate\nDate d’observation à l’intérieur de la campagne d’inventaire\n\n\n\ntime_obs\nheure HH:mm:ss\nHeure de l’observation à l’intérieur de la campagne d’inventaire\n\n\n\nstratum\nchoix\nStrate de végétation inventoriée (spécifique aux campagnes de type végétation)\n‘arborescente’, ‘arbustive’, ‘herbacées’, ‘bryophytes’\n\n\naxis\nchoix\nL'axe du transect pour la végétation\n‘n’,‘se’,‘so’\n\n\ndistance\nnombre décimal\nLa distance le long du transect pour la végétation\n\n\n\ndistance_unit\nchoix\nUnité de mesure utilisé pour la distance le long du transect\n\n\n\ndepth\nnombre décimal\nProfondeur pour les observations de zooplancton\n\n\n\nsample_id\nnombre entier\nnuméro de l’échantillon\n\n\n\nis_valid\nbooléen 1/0\nL’observation est-elle valide?\npar défaut: 1\n\n\ncampaign_id\nnombre entier\nNuméro d’identification de la campagne\n\n\n\ncampaign_info\nchamps virtuel\nInformations sur la campagne\n\n\n\nthermograph_id\nnombre entier\nNuméro du thermographe\n\n\n\nextra\njson\nAutres valeurs pour des colonnes additionnelles en format json\n{“longueur_poisson”: { type: ‘double’, description: ‘Longueur du poisson’, units: ‘cm’, value: 34.2}}\n\n\nnotes\ntexte\nCommentaires",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#correspondance-observation-effort",
    "href": "docs/portails/coleo/coleo_structure.html#correspondance-observation-effort",
    "title": "Structure",
    "section": "Correspondance observation-effort",
    "text": "Correspondance observation-effort\nNom de la table : observations_efforts_lookup\nPoint d’accès : /api/v1/observations_efforts_lookup\nInclus dans le résultat: observations, efforts\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nobservation_id\ntexte\nNuméro d’identification de l’observation\n\n\n\neffort_id\ntexte\nNuméro d’identification de l’effort",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#correspondance-observation-repère",
    "href": "docs/portails/coleo/coleo_structure.html#correspondance-observation-repère",
    "title": "Structure",
    "section": "Correspondance observation-repère",
    "text": "Correspondance observation-repère\nNom de la table : observations_landmarks_lookup\nPoint d’accès : /api/v1/observations_landmarks_lookup\nInclus dans le résultat: observations, landmarks\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nobservation_id\ntexte\nNuméro d’identification de l’observation\n\n\n\nlandmark_id\ntexte\nNuméro d’identification du repère",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#observations-despèces",
    "href": "docs/portails/coleo/coleo_structure.html#observations-despèces",
    "title": "Structure",
    "section": "Observations d’espèces",
    "text": "Observations d’espèces\nNom de la table : obs_species\nPoint d’accès : /api/v1/obs_species\nInclus dans le résultat: attributes, ref_species\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\ntaxa_name\ntexte\nNom complet de l’espèce observée\n\n\n\nvariable\ntexte\nRéférence vers la table d’attributs\n\n\n\nvalue\nnombre décimal\nValeur de l’attribut\n\n\n\nvalue_string\ntexte\nValeur de l’attribut pour les campagnes végétation\n\n\n\nobservation_id\nnombre entier\nIdentifiant unique de la table d’observations",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#attributs",
    "href": "docs/portails/coleo/coleo_structure.html#attributs",
    "title": "Structure",
    "section": "Attributs",
    "text": "Attributs\nNom de la table : attributes\nPoint d’accès : /api/v1/attributes\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nvariable\ntexte\nNom de la variable attribuée\n\n\n\ndescription\ntexte\nDescription de la variable attribuée\n\n\n\nunit\ntexte\nUnité de la variable attribuée",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#table-de-référence-des-noms-despèces",
    "href": "docs/portails/coleo/coleo_structure.html#table-de-référence-des-noms-despèces",
    "title": "Structure",
    "section": "Table de référence des noms d’espèces",
    "text": "Table de référence des noms d’espèces\nNom de la table : ref_species\nPoint d’accès : /api/v1/taxa\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nname\ntexte\nNom scientifique du taxa\n\n\n\nvernacular_fr\ntexte\nNom vernaculaire français de l’espèce\n\n\n\nvernacular_en\ntexte\nNom vernaculaire anglais de l’espèce\n\n\n\nrank\nchoix\nRang taxonomique\n‘sous-embranchement’, ‘embranchement’, ‘sous-classe’, ‘classe’, ‘sous-ordre’, ‘ordre’, ‘super-famille’, ‘famille’, ‘genre’, ‘espèce’,‘sous-espèce’,‘variété’, ‘complexe’,‘genre_hybride’, ‘espèce_hybride’,‘variété_hybride’,‘sous-espèce_hybride’\n\n\ncategory\nchoix\nCatégorie d’organisme vivant\n‘poissons’,‘plantes’,‘oiseaux’,‘amphibiens’,‘arthropodes’,‘mammifères’,‘reptiles’,‘autres’,‘mollusques’\n\n\ntsn\nnombre entier\nIdentifiant ITS (TSN)\n\n\n\nvascan_id\nnombre entier\nIdentifiant Vascand pour les plantes\n\n\n\nbryoquel_id\nnombre entier\nIdentifiant Bryoquel pour les bryphytes",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#observations-de-la-décomposition-du-sol-sacs-de-thé",
    "href": "docs/portails/coleo/coleo_structure.html#observations-de-la-décomposition-du-sol-sacs-de-thé",
    "title": "Structure",
    "section": "Observations de la décomposition du sol (sacs de thé)",
    "text": "Observations de la décomposition du sol (sacs de thé)\nNom de la table : obs_soil_decomposition\nPoint d’accès : /api/v1/obs_soil_decomposition\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nobservation_id\nnombre entier\nNuméro de l’observation dans la table observation\n\n\n\nbag_no\ntexte\nCode du sachet de thé\n\n\n\ntype\nchoix\nType de sachet de thé\n‘thé vert’, ‘rooibos’\n\n\ngeom\ngeometry(POINT)\nlocalisation du sachet\n\n\n\ndate_end\ndate\nDate de la collecte du sachet de thé. La date de l’observation est la date de la mise en place.\n\n\n\nstart_weight\nnombre décimal\nPoids du sachet au départ\n\n\n\nend_weight_with_bag\nnombre décimal\nPoids avec le sachet à la fin\n\n\n\nend_weight_tea\nnombre décimal\nPoids sans le sachet à la fin\n\n\n\nshading\nnombre entier\nOmbrage 1-5\n1=Aucun ombrage à 5=Complètement ombragé\n\n\nhuman_impact\nnombre entier\nImpacts anthropique 1-5\n1=Aucun impact à 5=Beaucoup d’impacts",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#observations-dadn-environnemental",
    "href": "docs/portails/coleo/coleo_structure.html#observations-dadn-environnemental",
    "title": "Structure",
    "section": "Observations d’ADN Environnemental",
    "text": "Observations d’ADN Environnemental\nNote: Le format des observations d’ADN environnemental est différent des autres observations puisqu’il inclut le numéro du repère, afin de permettre le traitement de multiples repères sur un même lac.\nNom de la table : obs_edna\nPoint d’accès : /api/v1/obs_edna\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nobservation_id\nnombre entier\nNuméro de l’observation dans la table observation\n\n\n\nlandmark_id\nnombre entier\nNuméro du repère\n\n\n\ntaxa_name\ntexte\nNom de l’espèce observée\n\n\n\nsequence_count\nnombre réel\nNombre de séquences\n\n\n\nsequence_count_corrected\nnombre réel\nNombre de séquences corrigé\n\n\n\ntype_edna\nchoix\nCatégorie d’observation ADNe\n“confirmé”, “probable”, “improbable”, “non-poisson”\n\n\nnotes\nTexte\nNotes",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#observations-des-condtitions-physico-chimiques-en-lac",
    "href": "docs/portails/coleo/coleo_structure.html#observations-des-condtitions-physico-chimiques-en-lac",
    "title": "Structure",
    "section": "Observations des condtitions physico-chimiques en lac",
    "text": "Observations des condtitions physico-chimiques en lac\nNom de la table : obs_lake\nPoint d’accès : /api/v1/obs_lake\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nobservation_id\nnombre entier\nNuméro de l’observation dans la table observation\n\n\n\nwater_transparency\nnombre décimal\nTransparence dans la colonne d’eau exprimée en mètres à l’aide d’un disque de Secchi\n\n\n\nwater_temp\nnombre décimal\nTempérature de l’eau en degrés Celsius\n\n\n\noxygen_concentration\nnombre décimal\nConcentration de l’oxygène dans l’eau (mg/L)\n\n\n\nph\nnombre décimal\nMesure du pH de l’eau\n\n\n\nconductivity\nnombre décimal\nConductivité de l’eau en mètres/seconde (m/s)\n\n\n\nturbidity\nnombre décimal\nTurbidité de l’eau en unités de turbidité néphalométriques (uNT)\n\n\n\ndissolved_organic_carbon\nnombre décimal\nCarbone organique dissous (filtré 0,45 µm)\n\n\n\nammonia_nitrogen\nnombre décimal\nAzote ammonical (filtré ou non)\n\n\n\nnitrates_and_nitrites\nnombre décimal\nNitrates et nitrites (filtré ou non)\n\n\n\ntotal_nitrogen\nnombre décimal\nAzote total (filtré ou non)\n\n\n\ntotal_phosphorus\nnombre décimal\nPhosphore total en trace lac 660 nm ou 660 nm verre\n\n\n\nchlorophyl_a\nnombre décimal\nChlorophyle A active\n\n\n\npheophytin_a\nnombre décimal\nPhéophytine A\n\n\n\nnotes\nTexte\nNotes",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#observations-de-profil-du-sol",
    "href": "docs/portails/coleo/coleo_structure.html#observations-de-profil-du-sol",
    "title": "Structure",
    "section": "Observations de profil du sol",
    "text": "Observations de profil du sol\nNom de la table : obs_soil\nPoint d’accès : /api/v1/obs_soil\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nobservation_id\nnombre entier\nNuméro de l’observation dans la table observation\n\n\n\ngeom\ngeometry(POINT)\nlocalisation du sachet\n\n\n\ndepth_tot\nnombre décimal\nProfondeur du pédon\n\n\n\ndepth_hummus\nnombre décimal\nProfondeur de la couche d’humus\n\n\n\ndepth_f\nnombre décimal\nProfondeur de la couche organique F\n\n\n\ndepth_m\nnombre décimal\nProfondeur de la couche organique M\n\n\n\ndepth_h\nnombre décimal\nProfondeur de la couche organique H\n\n\n\ndepth_a\nnombre décimal\nProfondeur de la couche minérale A\n\n\n\ntexture_a\nchoix\nTexture de la couche A\n“Argile”,“Argile limoneuse”,“Argile lourd”,“Argile sableuse”,“Limon”,“Loam”,“Loam argileux”,“Loam limoneux”,“Loam limono-argileux”, “Loam sableux”,“Loam sableux très grossier”,“Loam sableux grossier”,“Loam sableux moyen”,“Loam sableux fin”,“Loam sableux très fin”,“Loam sablo-argileux”,“Sable”,“Sable très grossier”,“Sable grossier”,“Sable moyen”,“Sable fin”,“Sable très fin”,“Sable très grossier loameux”,“Sable moyen loameux”,“Sable fin loameux”,“Sable très fin loameux”,“Sable loameux”,“Autres”\n\n\ndepth_b\nnombre décimal\nProfondeur de la couche minérale b\n\n\n\ntexture_b\nchoix\nTexture de la couche B\nVoir A\n\n\ndepth_c\nnombre décimal\nProfondeur de la couche minérale c\n\n\n\ntexture_c\nchoix\nTexture de la couche C\nVoir A\n\n\ndepth_d\nnombre décimal\nProfondeur de la couche minérale d\n\n\n\ntexture_d\nchoix\nTexture de la couche D\nVoir A\n\n\nmottling\nBooléen 0/1\nPrésence de moucheture\n\n\n\nmottling_depth\nnombre décimal\nProfondeur de moucheture, si présente\n\n\n\ndrainage_class\nnombre entier\nClasse de drainage\n0,1,2,3,4,5 ou 6\n\n\nwater_table_depth\nnombre décimal\nProfondeur de la nappe phréatique\n\n\n\nreached_one_meter\nBooléen 0/1\nEst-ce que la profondeur de 1 m a été atteinte?\n\n\n\ndepth_reached\nnombre décimal\nProfondeur atteinte (si moins de 1 m)\n\n\n\nnotes\nTexte\nNotes",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#observations-de-thermographes",
    "href": "docs/portails/coleo/coleo_structure.html#observations-de-thermographes",
    "title": "Structure",
    "section": "Observations de thermographes",
    "text": "Observations de thermographes\nNom de la table : obs_thermograph\nPoint d’accès : /api/v1/obs_thermograph\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\ncampaign_id\nnombre entier\nNuméro de la campagne dans la table campaigns\n\n\n\nthermograph_id\nnombre entier\nNuméro du thermographe dans la table thermographs\n\n\n\ntemperature\nnombre décimal\nTempérature (C)\n\n\n\npressure\nnombre décimal\nPression (psi)\n\n\n\ndate_obs\ndate\nDate d’observation à l’intérieur de la campagne d’inventaire\n\n\n\ntime_obs\nheure HH:mm:ss\nHeure de l’observation à l’intérieur de la campagne d’inventaire",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#media",
    "href": "docs/portails/coleo/coleo_structure.html#media",
    "title": "Structure",
    "section": "Media",
    "text": "Media\nNom de la table : media\nPoint d’accès : /api/v1/media\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\ntype\nchoix\nType de média\n‘image’, ‘audio’, ‘video’\n\n\nrecorder\nchoix\nType d’enregistreur\n‘ultrasound’, ‘audible’\n\n\nog_format\ntexte\nOriginal format (jpeg, png, etc)\n\n\n\nog_extention\ntexte\nOriginal extension (.jpg, .png, etc.)\n\n\n\nuuid\ntexte\nUUID, Identifiant unique généré par Coléo\n\n\n\nname\ntexte\nNom du fichier original\n\n\n\nsite_id\nnombre entier\nNuméro d’identification du site\n\n\n\ncampaign_id\nnombre entier\nNuméro d’identification de la campagne",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#table-de-correspondance---observation-media",
    "href": "docs/portails/coleo/coleo_structure.html#table-de-correspondance---observation-media",
    "title": "Structure",
    "section": "Table de correspondance - Observation-media",
    "text": "Table de correspondance - Observation-media\nNom de la table : obs_media\nPoint d’accès : /api/v1/obs_media\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nobs_id\nNombre entier\nIdentifiant de l’observation\n\n\n\nmedia_id\nNombre entier\nIdentifiant du média",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#table---colonnes",
    "href": "docs/portails/coleo/coleo_structure.html#table---colonnes",
    "title": "Structure",
    "section": "Table - colonnes",
    "text": "Table - colonnes\nPoint d’accès : /api/v1/table_columns\n\n\n\n\n\n\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\ntable\nTexte\nNom de la table dans la base de données\n\n\n\n\n** Retourne: la liste des colonnes que comporte cette table",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_structure.html#options-des-colonnes",
    "href": "docs/portails/coleo/coleo_structure.html#options-des-colonnes",
    "title": "Structure",
    "section": "Options des colonnes",
    "text": "Options des colonnes\nPoint d’accès : /api/v1/enum_options\n\n\n\nChamps\nType\nDescription\nOptions\n\n\n\n\nenum\nTexte\nNom de l’enum\n\n\n\n\n** Retourne: la liste des options pour cette colonne",
    "crumbs": [
      "Portails",
      "Coleo",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_geoparquet.html",
    "href": "docs/portails/atlas/atlas_geoparquet.html",
    "title": "Support technique",
    "section": "",
    "text": "La base de données Atlas est sauvegardée de façon régulière en format GeoParquet. Ce format de fichier, combiné à DuckDB permet la connexion efficace à un jeu de données à distance sans avoir à télécharger toutes les données. Il est également possible de télécharger le fichier Geoparquet contenant toutes les données publiques de l’Atlas pour effectuer des requêtes plus rapidement.\nVoici les instructions pour intéragir avec le fichier GeoParquet entreposé sur l’infonuagique depuis R avec les packages duckdb et dplyr.\nAvant de continuer, veuillez installer les packages suivant.\ninstall.packages(c('dplyr','duckdb','duckdbfs'))\nEnsuite, à chaque fois que vous voulez accéder aux données, vous devez d’abord executer la ligne suivante pour charger les fonctions nécessaires.\nsource(\"http://atlas.biodiversite-quebec.ca/bq-atlas-parquet.R\")\n\nConnexion à distance au fichier GeoParquet.\nL’objet atlas_dates contient les dates de sauvegarde des fichiers disponibles.\natlas_dates\nÉtablir la connexion à la version la plus récente des données Atlas.\natlas_rem &lt;- atlas_remote(tail(atlas_dates$dates,n=1))\nVoir le nom des différents champs dans le fichier Parquet.\ncolnames(atlas_rem)\nVoir les différentes sources de données ainsi que le nombre d’observations dans chacune\ndatasets &lt;- atlas_rem |&gt; group_by(dataset_name) |&gt; summarize(cnt=count()) |&gt; arrange(desc(cnt))\nCharger seulement les observations d’harfang des neiges dans un data frame et ne sélectionner que quelques colonnes. Cette requête peut prendre quelques minutes selon la vitesse de votre connexion internet.\nbubo &lt;- atlas_rem |&gt; filter(valid_scientific_name == 'Bubo scandiacus') |&gt; select(latitude, longitude, dataset_name, year_obs, day_obs) |&gt; collect()\nCharger seulement les observations d’iris versicolor dans un objet spatial sf.\niris &lt;- atlas_rem |&gt; filter(valid_scientific_name == 'Iris versicolor') |&gt; \n        mutate(geom = ST_Point(as.numeric(longitude), as.numeric(latitude))) |&gt; \n        to_sf() |&gt; collect()\nVisualiser les données sur une carte\nplot(iris['year_obs'])\nObtenir le nombre d’observations de chacune des espèces présentes dans Atlas et trier par nombre d’observations décroissant. Notez que cette requête ne chargera pas les données Atlas sur votre ordinateur. Le calcul se fera à distance grâce à DuckDB et au format GeoParquet.\nsp_count &lt;- atlas_rem |&gt; group_by(valid_scientific_name) |&gt; summarize(cnt=count()) |&gt; arrange(desc(cnt)) |&gt; collect()\n\n\nTélécharger le fichier GeoParquet localement.\nPour des opérations plus lourdes et une intéraction plus rapide avec le fichier, on peut le télécharger localement sur son ordinateur. Notez que le fichier fait environ 600MB, donc s’assurer d’avoir l’espace disque sur votre ordinateur.\n# Changer le répertoire ~/Downloads pour le répertoire de votre choix sur votre ordinateur\natlas &lt;- atlas_local(tail(atlas_dates$dates,n=1),'~/Downloads/')\nRefaire le décompte d’observations par espèce avec le fichier local. Vous verrez que ça se fera plus rapidement qu’avec le fichier à distance.\nsp_count &lt;- atlas |&gt; group_by(valid_scientific_name) |&gt; \n                summarize(cnt=count()) |&gt; arrange(desc(cnt)) |&gt; collect()\n\n\nRequêtes SQL\nSi vous préférez faire des requêtes SQL au lieu d’utiliser le package dplyr, l’objet atlas_con établit une connexion à DuckDB et la fonction atlas_dbi() va établir une connexion au fichier parquet et créer une VIEW nommée atlas.\natlas_dbi(tail(atlas_dates$dates,n=1))\nacci &lt;- dbGetQuery(atlas_con,\"SELECT * FROM atlas WHERE valid_scientific_name = 'Iris versicolor'\")",
    "crumbs": [
      "Portails",
      "Atlas",
      "Connexion à distance au fichier GeoParquet."
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_intro.html",
    "href": "docs/portails/atlas/atlas_intro.html",
    "title": "Base de données Atlas",
    "section": "",
    "text": "Atlas is a database for aggregation of biodiversity observations and time-series from multiple sources (codebase available here) The database is dockerized in the coleo-api server.",
    "crumbs": [
      "Portails",
      "Atlas",
      "Base de données Atlas"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_intro.html#python-package-dependencies",
    "href": "docs/portails/atlas/atlas_intro.html#python-package-dependencies",
    "title": "Base de données Atlas",
    "section": "Python package dependencies",
    "text": "Python package dependencies\nSome taxonomic features depend on the bdqc_taxa package, which is not available on PyPI. It can be installed from the git repository using the following command… TODO (in the docker?):\nsudo su coleo\n# Enter the docker\ndocker compose -f compose-atlas.yml --env-file .atlas.env exec db bash\n\npip install --upgrade git+https://github.com/ReseauBiodiversiteQuebec/bdqc_taxa#egg=bdqc_taxa\nThe database must be restarted after installing the package. TODO confirmer avec Guillaume\ndocker compose -f compose-atlas.yml --env-file .atlas.env build\ndocker compose -f compose-atlas.yml --env-file .atlas.env stop\ndocker compose -f compose-atlas.yml --env-file .atlas.env up -d",
    "crumbs": [
      "Portails",
      "Atlas",
      "Base de données Atlas"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_intro.html#database-schemas-overview",
    "href": "docs/portails/atlas/atlas_intro.html#database-schemas-overview",
    "title": "Base de données Atlas",
    "section": "Database schemas overview",
    "text": "Database schemas overview\nThe database is organized in the following schemas:\n\npublic: contains the tables and views used to store biodiversity data\napi: contains the tables, views and functions used to store and retrieve joined data from the public schema. This schema is used by the API.\ndata_transfer: contains the tables and views used to store temporary data. This schema may be used by users with read_write_all role.\natlas_api: contains the tables, views and functions used to store and retrieve joined data from the public schema. These objects are used by the API for the web atlas portal.\nobservations_partitions: contains partitionned tables used to store observations. Observations are partitionned by column within_quebec.\nDeprecated. public_api: This schema is deprecated and will be removed in a future version and should not be used anymore. It contains the tables, views and functions used to store and retrieve joined data from the public schema. These objects are used by the API for the web atlas portal.",
    "crumbs": [
      "Portails",
      "Atlas",
      "Base de données Atlas"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_intro.html#roles-permissions",
    "href": "docs/portails/atlas/atlas_intro.html#roles-permissions",
    "title": "Base de données Atlas",
    "section": "Roles & permissions",
    "text": "Roles & permissions\nThe database is organized in the following roles:\n\nread_only_public: can only read unrestricted data from the public, api and atlas_api schemas. This role is used by the API for general public access.\nread_only_all: can only read all data, restricted or not, from the public, api and atlas_api schemas. This role is used by the API for restricted access, such as for researchers and biodiversity quebec members.\nread_write_all: can read and write all data, restricted or not, from the public, api and atlas_api schemas. This role is used by biodiversity quebec members to update the database.",
    "crumbs": [
      "Portails",
      "Atlas",
      "Base de données Atlas"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_schema.html",
    "href": "docs/portails/atlas/atlas_schema.html",
    "title": "Structure",
    "section": "",
    "text": "The observations table is central in the Atlas database to recording ecological observations. It is is designed to support different observation types, such as abundances, occurrences, and presence/absence data of various units. The type of observation is specified by the id_variables column, which references the variables table. See the variables table documentation for more details.\nIt encompasses a wide range of data including geographical coordinates, observation times, taxa details, and associated variables, relying on foreign keys to other tables to store this information. See the Dependencies section below for more details and relevant tables documentation.\nPartitions are used to store observations that are within or outside of Quebec. This allows for faster queries on observations that are within Quebec, which are the most commonly used. See the partitions section below for more details.\nTriggers are used to automatically update the within_quebec, modified_at, dwc_event_date columns on insert or update. See the triggers section below for more details.\n\n\n\n\nForeign Keys:\n\nid_datasets references public.datasets (id)\nid_taxa_obs references public.taxa_obs (id)\nid_variables references public.variables (id)\n\nPartitions:\n\nobservations_partitions.outside_quebec\nobservations_partitions.within_quebec\n\nSequence: observations_partitions.observations_id_seq\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Name\nType\nDescription\nConstraints\n\n\n\n\nid\nBigint\nA unique identifier for each observation.\nPrimary key (part of), Not Null, Default: nextval(‘observations_partitions.observations_id_seq’::regclass)\n\n\norg_parent_event\nCharacter Varying\nOptional. Identifier of the parent event in the original source.\n-\n\n\norg_event\nCharacter Varying\nOptional. Identifier of the event in the original source.\n-\n\n\norg_id_obs\nCharacter Varying\nOptional. Original identifier of the observation in the original source.\n-\n\n\nid_datasets\nInteger\nThe identifier of the dataset to which the observation belongs.\nNot Null, Foreign key\n\n\nid_taxa_obs\nInteger\nIdentifier for the observed taxon.\nNot Null, Foreign key\n\n\ngeom\nGeometry(Point, 4326)\nThe geometry of the coordinates of the observation.\nNot Null\n\n\nyear_obs\nInteger\nThe year of the observation.\nNot Null\n\n\nmonth_obs\nInteger\nThe month of the observation.\n-\n\n\nday_obs\nInteger\nThe day of the observation.\n-\n\n\ntime_obs\nTime\nThe time of the observation.\n-\n\n\nid_variables\nInteger\nThe identifier of the variable observed. Whether of type abundance, occurrence, etc. is stored in table variables\nNot Null, Foreign key\n\n\nobs_value\nNumeric\nThe observed value for the variable. Might be integer or float in the case of abundance, or 0 or 1 in case of presence/absence\nNot Null\n\n\nid_taxa (deprecated)\nInteger\nDeprecated. The identifier of the taxa observed. From taxa table. Only to maintain compatibility.\nForeign key\n\n\nissue\nCharacter Varying\nOptional. Any issues or remarks related to the observation.\n-\n\n\ncreated_by\nCharacter Varying\nAuto on insert. The user who inserted the observation record within Atlas DB.\nDefault: CURRENT_USER\n\n\nmodified_at\nTimestamp with time zone\nAuto on insert/update. The timestamp when the observation was last modified.\nNot Null, Default: CURRENT_TIMESTAMP\n\n\nmodified_by\nCharacter Varying\nAuto on insert/update. The user who last modified the observation within Atlas DB.\nDefault: CURRENT_USER\n\n\nwithin_quebec\nBoolean\nAuto on insert/update. Indicates whether the observation was within Quebec.\nPart of primary key, Not Null, Default: false\n\n\ndwc_event_date\nText\nAuto on insert/update. The Darwin Core (DwC) formatted event date of the observation.\nNot Null, Default: ’’ (empty string)\n\n\n\n\n\n\n\nPrimary Key Constraint: The combination of id and within_quebec serves as a unique identifier for each observation record.\nForeign Key Constraints: Links to datasets and taxa_obs tables ensure referential integrity.\nUnique Constraint: The combination of geom, dwc_event_date, id_taxa_obs, obs_value, id_variables, within_quebec ensures that each observation is uniquely identifiable and retrievable. This is used to prevent duplicates using the index observations_unique_rows.\n\n\n\n\n\ndwc_event_date_idx on dwc_event_date\nobservations_geom_date_time_idx on geom, year_obs, month_obs, day_obs, time_obs\nobservations_geom_idx on geom\nobservations_id_datasets_id_taxa_idx on id_datasets, id_taxa\nobservations_id_datasets_idx on id_datasets\nobservations_id_idx on id\nobservations_id_taxa_obs_dwc_event_date_geom_idx on id_taxa_obs, dwc_event_date, geom\nobservations_id_taxa_obs_idx on id_taxa_obs\nobservations_unique_rows on geom, dwc_event_date, id_taxa_obs, obs_value, id_variables, within_quebec\nobservations_year_obs_idx on year_obs\nSpecific indexes on partitions (within_quebec_year_obs_idx, observations_id_taxa_obs_idx in observations_partitions.within_quebec)\n\n\n\n\n\nset_dwc_event_date_trggr: Before insert, sets the DwC event date.\nupdate_modified_at: Before update, updates the modified_at timestamp.\naction_user_logger: In partition observations_partitions.outside_quebec, logs user actions before insert or update.\n\n\n\n\n\n\nThe use of table partitions in the observations table, particularly with the within_quebec column, is a strategic decision aimed at enhancing database performance. Partitioning is a highly effective method for managing large tables by splitting them into smaller, more manageable pieces. In this case, the observations_partitions schema segregates observation data into distinct partitions based on whether the observations occurred within Quebec (within_quebec = true) or outside it (within_quebec = false). This approach significantly improves query performance by allowing the database engine to quickly locate and retrieve data from a smaller subset of the table. Additionally, it simplifies maintenance tasks such as backups and data purges, as operations can be performed on individual partitions without affecting the entire dataset.\nPartition tables observations_partitions.within_quebec and observations_partitions.outside_quebec are created within the observations_partitions schema.\n\n\n\nThe inclusion of year_obs, month_obs, day_obs, and time_obs columns in the observations table is intentionally designed to accommodate observations with varying levels of temporal resolution. This flexibility is crucial in ecological data collection, where the exact time of an observation may range from a specific moment to a broader time frame. By breaking down the observation timestamp into separate components, the database can store and process data that is only accurate to a certain level (e.g., year, month, day, or time). This structure not only caters to the diverse nature of ecological observations but also enhances the ability to perform time-based queries and analyses with varying granularities.\n\n\n\nDuplicates are possible through the process of adding data from various sources, such as GBIF and eBird. Duplicates are limited through the use of a unique constraint on the combination of geom, dwc_event_date, id_taxa_obs, obs_value, id_variables, within_quebec.\nThe dwc_event_date column values are generated automatically using a trigger function. It is a formatted string that reflects a standardized approach to recording event dates in biodiversity data. The use of this column is required to ensure each observation is distinctly identifiable and retrievable. It also facilitates data interoperability, making it easier to share and compare ecological data across different platforms and studies.",
    "crumbs": [
      "Portails",
      "Atlas",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_schema.html#table-observations",
    "href": "docs/portails/atlas/atlas_schema.html#table-observations",
    "title": "Structure",
    "section": "",
    "text": "The observations table is central in the Atlas database to recording ecological observations. It is is designed to support different observation types, such as abundances, occurrences, and presence/absence data of various units. The type of observation is specified by the id_variables column, which references the variables table. See the variables table documentation for more details.\nIt encompasses a wide range of data including geographical coordinates, observation times, taxa details, and associated variables, relying on foreign keys to other tables to store this information. See the Dependencies section below for more details and relevant tables documentation.\nPartitions are used to store observations that are within or outside of Quebec. This allows for faster queries on observations that are within Quebec, which are the most commonly used. See the partitions section below for more details.\nTriggers are used to automatically update the within_quebec, modified_at, dwc_event_date columns on insert or update. See the triggers section below for more details.\n\n\n\n\nForeign Keys:\n\nid_datasets references public.datasets (id)\nid_taxa_obs references public.taxa_obs (id)\nid_variables references public.variables (id)\n\nPartitions:\n\nobservations_partitions.outside_quebec\nobservations_partitions.within_quebec\n\nSequence: observations_partitions.observations_id_seq\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Name\nType\nDescription\nConstraints\n\n\n\n\nid\nBigint\nA unique identifier for each observation.\nPrimary key (part of), Not Null, Default: nextval(‘observations_partitions.observations_id_seq’::regclass)\n\n\norg_parent_event\nCharacter Varying\nOptional. Identifier of the parent event in the original source.\n-\n\n\norg_event\nCharacter Varying\nOptional. Identifier of the event in the original source.\n-\n\n\norg_id_obs\nCharacter Varying\nOptional. Original identifier of the observation in the original source.\n-\n\n\nid_datasets\nInteger\nThe identifier of the dataset to which the observation belongs.\nNot Null, Foreign key\n\n\nid_taxa_obs\nInteger\nIdentifier for the observed taxon.\nNot Null, Foreign key\n\n\ngeom\nGeometry(Point, 4326)\nThe geometry of the coordinates of the observation.\nNot Null\n\n\nyear_obs\nInteger\nThe year of the observation.\nNot Null\n\n\nmonth_obs\nInteger\nThe month of the observation.\n-\n\n\nday_obs\nInteger\nThe day of the observation.\n-\n\n\ntime_obs\nTime\nThe time of the observation.\n-\n\n\nid_variables\nInteger\nThe identifier of the variable observed. Whether of type abundance, occurrence, etc. is stored in table variables\nNot Null, Foreign key\n\n\nobs_value\nNumeric\nThe observed value for the variable. Might be integer or float in the case of abundance, or 0 or 1 in case of presence/absence\nNot Null\n\n\nid_taxa (deprecated)\nInteger\nDeprecated. The identifier of the taxa observed. From taxa table. Only to maintain compatibility.\nForeign key\n\n\nissue\nCharacter Varying\nOptional. Any issues or remarks related to the observation.\n-\n\n\ncreated_by\nCharacter Varying\nAuto on insert. The user who inserted the observation record within Atlas DB.\nDefault: CURRENT_USER\n\n\nmodified_at\nTimestamp with time zone\nAuto on insert/update. The timestamp when the observation was last modified.\nNot Null, Default: CURRENT_TIMESTAMP\n\n\nmodified_by\nCharacter Varying\nAuto on insert/update. The user who last modified the observation within Atlas DB.\nDefault: CURRENT_USER\n\n\nwithin_quebec\nBoolean\nAuto on insert/update. Indicates whether the observation was within Quebec.\nPart of primary key, Not Null, Default: false\n\n\ndwc_event_date\nText\nAuto on insert/update. The Darwin Core (DwC) formatted event date of the observation.\nNot Null, Default: ’’ (empty string)\n\n\n\n\n\n\n\nPrimary Key Constraint: The combination of id and within_quebec serves as a unique identifier for each observation record.\nForeign Key Constraints: Links to datasets and taxa_obs tables ensure referential integrity.\nUnique Constraint: The combination of geom, dwc_event_date, id_taxa_obs, obs_value, id_variables, within_quebec ensures that each observation is uniquely identifiable and retrievable. This is used to prevent duplicates using the index observations_unique_rows.\n\n\n\n\n\ndwc_event_date_idx on dwc_event_date\nobservations_geom_date_time_idx on geom, year_obs, month_obs, day_obs, time_obs\nobservations_geom_idx on geom\nobservations_id_datasets_id_taxa_idx on id_datasets, id_taxa\nobservations_id_datasets_idx on id_datasets\nobservations_id_idx on id\nobservations_id_taxa_obs_dwc_event_date_geom_idx on id_taxa_obs, dwc_event_date, geom\nobservations_id_taxa_obs_idx on id_taxa_obs\nobservations_unique_rows on geom, dwc_event_date, id_taxa_obs, obs_value, id_variables, within_quebec\nobservations_year_obs_idx on year_obs\nSpecific indexes on partitions (within_quebec_year_obs_idx, observations_id_taxa_obs_idx in observations_partitions.within_quebec)\n\n\n\n\n\nset_dwc_event_date_trggr: Before insert, sets the DwC event date.\nupdate_modified_at: Before update, updates the modified_at timestamp.\naction_user_logger: In partition observations_partitions.outside_quebec, logs user actions before insert or update.\n\n\n\n\n\n\nThe use of table partitions in the observations table, particularly with the within_quebec column, is a strategic decision aimed at enhancing database performance. Partitioning is a highly effective method for managing large tables by splitting them into smaller, more manageable pieces. In this case, the observations_partitions schema segregates observation data into distinct partitions based on whether the observations occurred within Quebec (within_quebec = true) or outside it (within_quebec = false). This approach significantly improves query performance by allowing the database engine to quickly locate and retrieve data from a smaller subset of the table. Additionally, it simplifies maintenance tasks such as backups and data purges, as operations can be performed on individual partitions without affecting the entire dataset.\nPartition tables observations_partitions.within_quebec and observations_partitions.outside_quebec are created within the observations_partitions schema.\n\n\n\nThe inclusion of year_obs, month_obs, day_obs, and time_obs columns in the observations table is intentionally designed to accommodate observations with varying levels of temporal resolution. This flexibility is crucial in ecological data collection, where the exact time of an observation may range from a specific moment to a broader time frame. By breaking down the observation timestamp into separate components, the database can store and process data that is only accurate to a certain level (e.g., year, month, day, or time). This structure not only caters to the diverse nature of ecological observations but also enhances the ability to perform time-based queries and analyses with varying granularities.\n\n\n\nDuplicates are possible through the process of adding data from various sources, such as GBIF and eBird. Duplicates are limited through the use of a unique constraint on the combination of geom, dwc_event_date, id_taxa_obs, obs_value, id_variables, within_quebec.\nThe dwc_event_date column values are generated automatically using a trigger function. It is a formatted string that reflects a standardized approach to recording event dates in biodiversity data. The use of this column is required to ensure each observation is distinctly identifiable and retrievable. It also facilitates data interoperability, making it easier to share and compare ecological data across different platforms and studies.",
    "crumbs": [
      "Portails",
      "Atlas",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_schema.html#table-datasets",
    "href": "docs/portails/atlas/atlas_schema.html#table-datasets",
    "title": "Structure",
    "section": "Table: Datasets",
    "text": "Table: Datasets\n\nDescription\nThe datasets table stores comprehensive metadata about ecological observation datasets. This includes information about the source, creator, type of data, and publication details. It serves as a central repository for tracking various datasets related to ecological observations.\n\n\nDependencies\nNone\n\n\nColumns\n\n\n\n\n\n\n\n\n\nColumn Name\nType\nDescription\nConstraints\n\n\n\n\nid\nInteger\nA unique identifier for each dataset.\nPrimary key, Not Null\n\n\noriginal_source\nCharacter Varying\nThe original source of the dataset. Where was it obtained from (e.g., ‘GBIF’, ‘eBird’, ‘MELCCFP’).\nNot Null\n\n\norg_dataset_id\nCharacter Varying\nOptional. An identifier used by the original_source or publisher for the dataset.\n-\n\n\ncreator\nCharacter Varying\nOptional. The creator of the dataset (e.g., individual researcher or institution).\n-\n\n\ntitle\nCharacter Varying\nThe title of the dataset.\nNot Null\n\n\npublisher\nCharacter Varying\nOptional. The publisher of the dataset (e.g., ‘Nature Publishing Group’, ‘Elsevier’, ‘GBIF’, ‘Données Québec’).\n-\n\n\nmodified\nDate\nThe date when the dataset was last modified.\nNot Null\n\n\nkeywords\nCharacter Varying[]\nOptional. An array of keywords associated with the dataset.\n-\n\n\nabstract\nText\nOptional. A brief abstract or summary of the dataset.\n-\n\n\ntype_sampling\nCharacter Varying\nOptional. The type of sampling method used in the dataset.\n-\n\n\ntype_obs\npublic.type_observation\nOptional. The type of observation (Enum values: ‘living specimen’, ‘preserved specimen’, ‘fossil specimen’, ‘human observation’, ‘machine observation’, ‘literature’, ‘material sample’, ‘others’).\n-\n\n\nintellectual_rights\nCharacter Varying\nOptional. Information about the intellectual rights of the dataset.\n-\n\n\nlicense\nCharacter Varying\nOptional. The license under which the dataset is released (e.g., ‘CC BY 4.0’, ‘GPL’, ‘Entente de partage’).\n-\n\n\nowner\nCharacter Varying\nOptional. The owner of the dataset (e.g., a university, research institution, or individual researcher).\n-\n\n\nmethods\nText\nOptional. A detailed description of the methods used in the dataset.\n-\n\n\nopen_data\nBoolean\nA boolean indicating whether the dataset is open data.\nNot Null\n\n\nexhaustive\nBoolean\nA boolean indicating whether the dataset is exhaustive, meaning obtained from a checklist survey.\nNot Null\n\n\ndirect_obs\nBoolean\nA boolean indicating whether the dataset contains direct observations.\nNot Null\n\n\ncentroid\nBoolean\nOptional. A boolean indicating whether the dataset contains centroid data.\nDefault: False\n\n\ndoi\nText\nOptional. The Digital Object Identifier for the dataset. (e.g. https://doi.org/10.15468/ykxm8x)\nDefault: Empty String\n\n\ncitation\nText\nOptional. The recommended citation for the dataset.\nDefault: Empty String",
    "crumbs": [
      "Portails",
      "Atlas",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_schema.html#table-taxa_obs",
    "href": "docs/portails/atlas/atlas_schema.html#table-taxa_obs",
    "title": "Structure",
    "section": "Table: Taxa_Obs",
    "text": "Table: Taxa_Obs\n\nDescription\nThe taxa_obs table is structured to record raw taxonomic information directly from source data. This table is essential for capturing a wide range of taxonomic details, including scientific names, authorship, and taxonomic rank. It is designed to accommodate taxa of any rank, such as species, genus, complexes, etc., without requiring corrections for grammatical accuracy or valid taxonomy. These aspects are managed by other resources within the database, like the taxa_ref table.\n\n\nDependencies\n\nSequence: taxa_obs_id_seq\n\n\n\nColumns\n\n\n\n\n\n\n\n\n\nColumn Name\nType\nDescription\nConstraints\n\n\n\n\nid\nInteger\nA unique identifier for each taxonomic observation.\nPrimary key, Not Null, Default: nextval(‘taxa_obs_id_seq’::regclass), Using Index Tablespace ssdpool\n\n\nscientific_name\nText\nOptional. The scientific name of the taxon observed. This includes taxa at any rank, such as species or genus.\nNot Null, Part of Unique Constraint\n\n\nauthorship\nText\nOptional. The authorship of the scientific name. Defaults to an empty string if not provided.\nNot Null, Default: ’’ (empty string), Part of Unique Constraint\n\n\nrank\nText\nOptional. The taxonomic rank of the observation. Defaults to an empty string if not provided.\nDefault: ’’ (empty string), Part of Unique Constraint\n\n\nparent_scientific_name\nText\nOptional. The scientific name of the parent taxon. Used to resolve conflicts where a scientific name corresponds to different organisms in different branches of the tree of life. Optional; if not specified, all results for the given scientific name are returned.\n-\n\n\ncreated_at\nTimestamp with time zone\nAuto on insert/update. The timestamp when the taxonomic observation was created.\nNot Null, Default: CURRENT_TIMESTAMP\n\n\nmodified_at\nTimestamp with time zone\nAuto on insert/update. The timestamp when the taxonomic observation was last modified.\nNot Null, Default: CURRENT_TIMESTAMP\n\n\nmodified_by\nText\nAuto on insert/update. The user who last modified the taxonomic observation record.\nNot Null, Default: CURRENT_USER\n\n\n\n\n\nAdditional Constraints\n\nUnique Constraint: taxa_obs_unique_rows ensures that each combination of scientific_name, authorship, rank, parent_scientific_name is unique within the table.\n\n\n\nIndexes\n\ntaxa_obs_scientific_name_idx on scientific_name\n\n\n\nTriggers\n\nupdate_modified_at: Before update, updates the modified_at timestamp.\n\n\n\nConflicts and Parent Taxa\nThe design of the taxa_obs table accounts for the potential conflicts in scientific naming. For instance, the same scientific name may correspond to different organisms in separate branches of the tree of life (e.g., Salix as a genus of willows in plants and a genus of tunicates in animals). To resolve such conflicts, the parent_scientific_name column allows users to specify a parent taxa name, thus restricting results to a specific branch. This feature is particularly useful for accurate data retrieval and association in cases of nomenclatural ambiguity.",
    "crumbs": [
      "Portails",
      "Atlas",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_schema.html#table-variables",
    "href": "docs/portails/atlas/atlas_schema.html#table-variables",
    "title": "Structure",
    "section": "Table: Variables",
    "text": "Table: Variables\n\nDescription\nThe variables table in the database is crafted to support a diverse array of observation types and units. This versatility is key to accommodating various ecological data formats, such as counts of individuals, weight, density, occurrences, and presence/absence data, among others. Each of these observation types can be associated with different units of measurement. The table facilitates the recording and categorization of these different types and units, making it a pivotal component for data analysis and interpretation in ecological studies. The id_variables column, found in other related tables like observations, references the variables table to specify the type of observation, providing a clear link to the detailed description and unit of the variable being observed.\n\n\nDependencies\n\nSequence: variables_id_seq\n\n\n\nColumns\n\n\n\n\n\n\n\n\n\nColumn Name\nType\nDescription\nConstraints\n\n\n\n\nid\nInteger\nA unique identifier for each variable.\nPrimary key, Not Null, Default: nextval(‘variables_id_seq’::regclass)\n\n\nname\nCharacter Varying\nThe name of the variable, indicating the type of observation (e.g., count, weight, density).\nNot Null, Part of Unique Constraint\n\n\nunit\nCharacter Varying\nOptional. The unit of measurement for the variable, which may vary based on the observation type.\nNot Null, Default: ’’, Part of Unique Constraint\n\n\ndescription\nText\nOptional. A detailed description of the variable, explaining its significance and use in observations.\n-\n\n\n\n\n\nAdditional Constraints\n\nUnique Constraint: variables_name_unit_key ensures that each combination of name and unit is unique within the table.",
    "crumbs": [
      "Portails",
      "Atlas",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_schema.html#table-efforts",
    "href": "docs/portails/atlas/atlas_schema.html#table-efforts",
    "title": "Structure",
    "section": "Table: Efforts",
    "text": "Table: Efforts\n\nDescription\nThe efforts table is specifically designed to record the efforts associated with ecological observations. This table plays a crucial role in quantifying and categorizing the effort invested in obtaining various observations. The efforts could be in various forms like time spent, area covered, traps used, etc., and are quantified as numeric values. The link to the variables table through id_variables specifies the type of effort being recorded, ensuring a standardized and coherent approach to effort recording across different observation types.\n\nRelationship with Observations\nThe efforts table is intricately linked to the observations table through the obs_efforts lookup table. This table acts as a relational bridge, establishing a connection between individual observations and the corresponding efforts involved in their acquisition. The obs_efforts table essentially serves as a many-to-many relationship facilitator, allowing each observation to be associated with one or more effort entries, and vice versa. See the obs_efforts table documentation for more details.\n\n\n\nDependencies\n\nSequence: efforts_id_seq\nForeign Key:\n\nid_variables references public.variables (id)\n\n\n\n\nColumns\n\n\n\n\n\n\n\n\n\nColumn Name\nType\nDescription\nConstraints\n\n\n\n\nid\nInteger\nA unique identifier for each effort entry.\nPrimary key, Not Null, Default: nextval(‘efforts_id_seq’::regclass)\n\n\nid_variables\nInteger\nThe identifier of the variable associated with the effort.\nNot Null, Foreign key\n\n\neffort_value\nNumeric\nThe quantitative value representing the effort.\nNot Null, Part of Unique Constraint\n\n\n\n\n\nAdditional Constraints\n\nUnique Constraint: efforts_id_variables_effort_value_key ensures that each combination of id_variables and effort_value is unique within the table, preventing redundancy in effort recording.\n\n\n\nForeign Key Constraints\n\nefforts_id_variables_fkey: Links to the variables table, ensuring that each effort is associated with a valid variable type, providing context and meaning to the effort value recorded.",
    "crumbs": [
      "Portails",
      "Atlas",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_schema.html#table-obs_efforts",
    "href": "docs/portails/atlas/atlas_schema.html#table-obs_efforts",
    "title": "Structure",
    "section": "Table: Obs_Efforts",
    "text": "Table: Obs_Efforts\n\nDescription\nThe obs_efforts table serves as a linking mechanism between ecological observations and the efforts associated with them. It is a junction table designed to create a many-to-many relationship, capturing which efforts are related to specific observations. This design is essential for providing a comprehensive understanding of the resources and efforts expended in gathering each observation, such as the time, distance covered, or equipment used. By establishing a clear connection between observations and their corresponding efforts, this table enhances the granularity and context of ecological data analysis.\n\n\nDependencies\n\nForeign Keys:\n\nid_obs references an observation table (to be specified).\nid_efforts references public.efforts (id)\n\n\n\n\nColumns\n\n\n\n\n\n\n\n\n\nColumn Name\nType\nDescription\nConstraints\n\n\n\n\nid_obs\nBigint\nThe identifier of the observation. Represents a link to a specific observation record.\nNot Null, Part of Unique Constraint\n\n\nid_efforts\nInteger\nThe identifier of the effort. Links to a specific effort record in the efforts table.\nNot Null, Foreign key, Part of Unique Constraint\n\n\n\n\n\nAdditional Constraints\n\nUnique Constraint: obs_efforts_id_obs_id_efforts_key ensures that each combination of id_obs and id_efforts is unique within the table, preventing duplication in the linking of observations and efforts.\n\n\n\nForeign Key Constraints\n\nobs_efforts_id_efforts_fkey: Ensures the integrity of the link between the obs_efforts and efforts tables, confirming that every effort associated with an observation is valid and exists in the efforts table.",
    "crumbs": [
      "Portails",
      "Atlas",
      "Structure"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_infrastructure.html",
    "href": "docs/portails/atlas/atlas_infrastructure.html",
    "title": "Infrastructure Atlas (Staging et Production)",
    "section": "",
    "text": "La figure suivante montre comment le portail de l’atlas est structuré, y compris l’API du backend et la base de données.\n\n\n\nImage\n\n\nServer: coleo-api\nPostgres est utilisé pour stocker la base de données à l’intérieur d’un conteneur docker et pour l’API nous utilisons postgRest qui génère l’API automatiquement à partir des schémas et des tables de la base de données. Pour ce service, nous utilisons également un conteneur Docker.\nChaque service est divisé en deux catégories, la mise en scène et la production. Dans la phase d’essai, les données sont écrites pour éviter les erreurs dans la production. Une fois les modifications validées, elles sont migrées vers la production à l’aide du script : atlas_staging_to_main_swap.sh.\nIMPORTANT!!!: La migration est exécutée manuellement et seulement lorsque les modifications dans staging sont validées.\nTRIGGER SWAP MANUALLY (FOR ADMINS):\n\ntapez dans le promptsudo su être sudoer (PAS COLEO, sudo su sous votre propre utilisateur).\nallez dans le dossier /var/www/bq-db-container.\nexecuter la commande suivante: sh atlas_staging_to_main_swap.sh.",
    "crumbs": [
      "Portails",
      "Atlas",
      "Infrastructure Atlas (Staging et Production)"
    ]
  },
  {
    "objectID": "docs/portails/create_users/change_password.html",
    "href": "docs/portails/create_users/change_password.html",
    "title": "Changer le mot de passe d’utilisateur",
    "section": "",
    "text": "Cette section détaille la procédure de changement de mot de passe sur le portail web qui donne accès à l’information sensible. Par exemple, aux tableaux de données des inventaires terrain.\n\nAccéder la page de connection accessible depuis le menu hamburger sous la rubrique Se connecter\nEntrer ses informations de connexion (les comptes de l’ancienne application coleo ont été maintenus)\n\n\nnom d'utilisateur Saisir son adresse courriel\nmot de passe Saisir son mot de passe\n\n\nAccéder la page de Changement password.\n\n\nEntrer l'ancien mot de passe\nEntrer le nouveau mot de passe deux fois\n\nNote: Cette page n’est pas J'ai oublie mon mot de passe, c’est-a-dire, Il faut absolument connaître le mot de passe que vouz aimerez changer. Si jamais vous oubliez votre mot de passe contactez les administrateurs pour vous aider. \n\nSi le processus se passe bien, vous allez voir un message confirmant le changement de mot de passe et vous aurez la possibilitée d’aller vous connecter pour vérifier que le changement a eu lieu (On recommende fortement de le faire).\n\n\n\n\nImage",
    "crumbs": [
      "Portails",
      "Create user accounts",
      "Changer le mot de passe d'utilisateur"
    ]
  },
  {
    "objectID": "docs/portails/indicateurs/indicateurs_intro.html",
    "href": "docs/portails/indicateurs/indicateurs_intro.html",
    "title": "Indicateurs",
    "section": "",
    "text": "Toute l’information concernant les indicateurs est contenue dans le répertoire bdqc_indicators.",
    "crumbs": [
      "Portails",
      "Indicateurs",
      "Indicateurs"
    ]
  },
  {
    "objectID": "docs/portails/indicateurs/indicateurs_intro.html#repository-structure",
    "href": "docs/portails/indicateurs/indicateurs_intro.html#repository-structure",
    "title": "Indicateurs",
    "section": "Repository Structure",
    "text": "Repository Structure\nThe repository is organized as follows:\n\nEach indicator has its own subdirectory containing the documentation and scripts specific to that indicator.\nThe instructions for connecting to data stores, formatting the data, and running analyses are centralized in the README.md files within each indicator’s subdirectory.",
    "crumbs": [
      "Portails",
      "Indicateurs",
      "Indicateurs"
    ]
  },
  {
    "objectID": "docs/portails/indicateurs/indicateurs_intro.html#data-storage",
    "href": "docs/portails/indicateurs/indicateurs_intro.html#data-storage",
    "title": "Indicateurs",
    "section": "Data Storage",
    "text": "Data Storage\nTo ensure data integrity and security, no data should be stored directly in this repository, with rare exceptions. Instead, data should be stored in Biodiversité Québec’s designated data stores and accessed automatically through requests within the scripts.\nImportant: All data should be accessed within scripts by making requests to the appropriate data stores.",
    "crumbs": [
      "Portails",
      "Indicateurs",
      "Indicateurs"
    ]
  },
  {
    "objectID": "docs/portails/indicateurs/indicateurs_intro.html#updating-indicators",
    "href": "docs/portails/indicateurs/indicateurs_intro.html#updating-indicators",
    "title": "Indicateurs",
    "section": "Updating Indicators",
    "text": "Updating Indicators\nTo update or compute an indicator:\n\nConnect to pose.vhost33 vm. Scripts may require important resources and are designed to run from pose.\nGit clone this repository to your home directory within pose VM.\nNavigate to the indicator’s subdirectory.\nRefer to the README.md file within the subdirectory for specific instructions on how to compute the indicator. All data should be accessed, formated, ploted and metrics should all be computed from the README.md file.",
    "crumbs": [
      "Portails",
      "Indicateurs",
      "Indicateurs"
    ]
  },
  {
    "objectID": "docs/portails/indicateurs/indicateurs_intro.html#setting-up-data-store-connections",
    "href": "docs/portails/indicateurs/indicateurs_intro.html#setting-up-data-store-connections",
    "title": "Indicateurs",
    "section": "Setting Up Data Store Connections",
    "text": "Setting Up Data Store Connections\nTo enable connection to the data stores:\n\nAccess pose.vhost33 VM. Scripts and connections to data stores are designed to run from pose.\nCreate a .Renviron file in the root directory of this repository.\nAdd the necessary environment variables to the .Renviron file to facilitate connection to the data stores.\n\nRefer to the README.md file of each indicator for any specific instructions or requirements for connecting to the data stores.\n\nEnsure that the .Renviron file is not pushed to the repository by including it in the .gitignore file.\n\nNote: The .Renviron file should not be tracked by version control and should be kept local to your machine.\nRequired environmental variables :\nATLAS\n\nATLAS_DATABASE=\"ATLAS\"\nATLAS_HOSTNAME=\"coleo-api\"\nATLAS_PORT=\"5435\"\nATLAS_USER username\nATLAS_PASSWORD mot de passe\n\nSTAC CATALOG IO\n\nIO_STAC_CATALOG_URL=http://coleo-api.vhost33:8082/\n\n\nPlease refer to the individual indicator directories for more detailed information on each indicator and how to compute them.",
    "crumbs": [
      "Portails",
      "Indicateurs",
      "Indicateurs"
    ]
  },
  {
    "objectID": "docs/portails/create_users/login.html",
    "href": "docs/portails/create_users/login.html",
    "title": "Connexion au portail web de Biodiversité Québec",
    "section": "",
    "text": "Cette section détaille la procédure de connexion au portail web qui donne accès à l’information sensible. Par exemple, aux tableaux de données des inventaires terrain.\n\nAccéder la page de connection accessible depuis le menu hamburger sous la rubrique Se connecter\nEntrer ses informations de connexion (les comptes de l’ancienne application coleo ont été maintenus)\n\n\nnom d'utilisateur Saisir son adresse courriel\nmot de passe Saisir son mot de passe\n\nNote: Le nom d’utilisateur et le mot de passe sont stockés dans la base de données coleo\n\n\n\nImage",
    "crumbs": [
      "Portails",
      "Create user accounts",
      "Connexion au portail web de Biodiversité Québec"
    ]
  },
  {
    "objectID": "docs/portails/create_users/sign_up.html",
    "href": "docs/portails/create_users/sign_up.html",
    "title": "Créer un nouvel utilisateur",
    "section": "",
    "text": "Note: Il faut détenir un compte aux privilèges Administrateur pour pouvoir créer de nouveaux utilisateurs\n\n\nSe connecter à la page de connection à l’aide d’un compte détenant les privilèges Administrateur\n\n\n\n\nImage\n\n\n\nAller à la page Connexion\n\n\n\n\nImage\n\n\n\nRemplir les champs et cliquer sur Créer utilisateur pour créer le nouveau compte utilisateur\n\n\nLes utilisateurs Administrateur peuvent accéder à l’ensemble des données ainsi qu’à la création de comptes. Ne pas attribuer ce privilège à la légère\n\n\nLes utilisateurs Utilisateur peuvent accéder à l’ensemble des données\n\n\nLes utilisateurs Lecteur peuvent accéder aux tables de données simplifiées (sans accès aux tables de type gabarit)\n\n\nUne fois le compte utilisateur créé, le message suivant s’affiche. \n\nNote Importante !!!!!: Si vous essayez d’ouvrir la page d’inscription sans vous connecter au préalable avec un compte administrateur, vous obtiendrez un message d’erreur.\n\n\n\nImage",
    "crumbs": [
      "Portails",
      "Create user accounts",
      "Créer un nouvel utilisateur"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_injection.html",
    "href": "docs/portails/atlas/atlas_injection.html",
    "title": "Injection Atlas",
    "section": "",
    "text": "See repo atlas-injection[https://github.com/BiodiversiteQuebec/atlas-injection] for examples",
    "crumbs": [
      "Portails",
      "Atlas",
      "Injection Atlas"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_injection.html#step-for-injection",
    "href": "docs/portails/atlas/atlas_injection.html#step-for-injection",
    "title": "Injection Atlas",
    "section": "Step for injection",
    "text": "Step for injection\n\nCreate dataset and inject in table datasets (if not already existing)\nFormat taxonomy and inject in table taxa_obs (if not already existing)\nCreate/format observations variables (occurrence/abundance) and inject in table variables (if not already existing)\nProceed to general validation of the data (see below section)\nRetrieve datasets, taxa_obs and variables id (foreign keys) and inject observations data in table observations\nCreate effort sampling variables and the effort itself and inject in table efforts and variables (if not already existing)\nRetrieve observations id and link them to the id_efforts of effort sampling for injection in table obs_efforts",
    "crumbs": [
      "Portails",
      "Atlas",
      "Injection Atlas"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_injection.html#validation-of-data",
    "href": "docs/portails/atlas/atlas_injection.html#validation-of-data",
    "title": "Injection Atlas",
    "section": "Validation of data",
    "text": "Validation of data\n\nConvert coordinates to WGS84 first and to WKT (Well-Known Text) and drop rows where coordinates information is missing\nVerify that the observations are within the Quebec bounding box (either manually or retrieve the polygon from Atlas)\nMake sure that we at least have a value for year_obs and that month_obs &lt;= 12 and day_obs &lt;= 31\nIf there is time information (time_obs), convert it to 00:00:00 format (H%M%S%)\nCan check for whitespaces within strings values",
    "crumbs": [
      "Portails",
      "Atlas",
      "Injection Atlas"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_injection.html#question-to-clarify-with-data-providers",
    "href": "docs/portails/atlas/atlas_injection.html#question-to-clarify-with-data-providers",
    "title": "Injection Atlas",
    "section": "Question to clarify with data providers",
    "text": "Question to clarify with data providers\n\nClarify the type of data if unclear (occurrence, absence, abundance etc.)\nClarify the projection and SRID of coordinates if not provided\nClarify if there is any precision data on coordinates\nClarify if there is any effort sampling information\nClarify all the required datasets fields if they are not provided",
    "crumbs": [
      "Portails",
      "Atlas",
      "Injection Atlas"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_datasets.html",
    "href": "docs/portails/atlas/atlas_datasets.html",
    "title": "Jeu de données Atlas",
    "section": "",
    "text": "Liste des jeux de données dans l’Atlas\n\nAtlas des Oiseaux Nicheurs du Québec\nBiotime\nColeo\neBird (EOD)\neButterfly\nGBIF\nLiving Planet Index (LPI)\nObservatoire Global du Saint-Laurent (OGSL)\nPatrice Bourgault - Tortues (UDeS)\nPatrice Bourgault - Micromammifères (UDeS)\nMELCCFP:\n\nESPECES (Frederic Poisson ?)\nInventaire de la Flore Nordique du Québec (Frederic Poisson ?)\nSuivi du Benthos\nEspèces exotiques envahissantes (EEE)\nAtlas des micromammifères et des chiroptères du Québec (MMACH)\nBanque de données sur les reptiles et amphibiens du Québec (BORAQ)\nDonnées de localisation des grands mammifères\nPoints d’observations écologiques\nPlacettes-échantillons permanentes\nPlacettes-échantillons temporaires (2, 3, 4 et 5e inventaire)\nSystème d’Information sur la Faune Aquatique (IFA)",
    "crumbs": [
      "Portails",
      "Atlas",
      "Jeu de données Atlas"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_description.html",
    "href": "docs/portails/atlas/atlas_description.html",
    "title": "Description schéma public",
    "section": "",
    "text": "Ecological observations are managed through a star schema, where the core table is observations and the dimension tables are datasets, taxa_obs, variables and efforts. The observations table contains the foreign keys to the dimension tables, as well as the observation value and the observation geometry. The dimension tables contain the metadata for the observations.\nIt is designed to support different observation types, such as abundances, occurrences, and presence/absence data of various units.\n  Figure 1. Entity relationship diagram for the observations related tables\n\n\nBelow is a brief description of each table. See the Schema Reference section for more details on each table.\nObservations: Central to the schema, storing detailed records of individual ecological observations.\n\nImportant for injection\nGeometry can be injected using the WKT format: SRID=4326;POINT(-73.5 45.5) and will be automatically converted to the PostGIS geometry type.\nColumns created_at, created_by, modified_at, within_quebec, dwc_event_date are automatically set by the database and should not be set manually.\n\nDatasets: Manages metadata about different datasets to which observations belong. Includes comprehensive information like source, creator, title, and publisher.\nVariables: Categorizes different observation types and units.\nTaxa_Obs: Stores raw taxonomic information as provided by the source, facilitating the recording of various taxa. Those entries are meant as an archive of the original data and are not used for data retrieval. Taxonomic information is managed by other resources within the database, like the taxa_ref table. See the managing_taxonomy article for more details.\n\nImportant for injection\nThe scientific_name column is required. The authorship and rank columns are optional and will be set to an empty string if not provided. Entries are accepted for any taxonomic rank, including species, genus, complexes, etc.\nThe parent_scientific_name column is optional. It is used to resolve conflicts where a scientific name corresponds to different organisms in different branches of the tree of life. If not specified, all results for the given scientific name are returned. It is recommended to specify the phylum or kingdom when available\nColumns rank, authorship are optional and should only be set if available by the source.\n\nEfforts: Quantifies the efforts (like time, distance, etc.) put into making an observation.\nObs_Efforts: Links observations with their corresponding efforts. Creates a many-to-many relationship, allowing multiple efforts to be linked to a single observation and vice versa.\n\n\n\nAuto-generated columns: The following columns are automatically generated by the database and should not be set manually: created_at, created_by, modified_at, within_quebec, dwc_event_date.\nUnique Constraints: Unique constraints are used to prevent duplicate observations from being inserted into the database. The combination of geom, dwc_event_date, id_taxa_obs, obs_value, id_variables, within_quebec ensures that each observation is uniquely identifiable and retrievable.\nPartitions: Partitions are used to store observations that are within or outside of Quebec. This allows for faster queries on observations that are within Quebec, which are the most commonly used. See the partitions section in the observations schema reference for more details.\nTaxonomic information: Only raw taxonomic information is stored in the taxa_obs table. The valid taxonomy, references, and other information are managed by taxa_ref, taxa_vernacular, taxa_groups tables and related ressources. This is managed automatically with periodic updates. See the managing_taxonomy article for more details.",
    "crumbs": [
      "Portails",
      "Atlas",
      "Description schéma public"
    ]
  },
  {
    "objectID": "docs/portails/atlas/atlas_description.html#managing-observations",
    "href": "docs/portails/atlas/atlas_description.html#managing-observations",
    "title": "Description schéma public",
    "section": "",
    "text": "Ecological observations are managed through a star schema, where the core table is observations and the dimension tables are datasets, taxa_obs, variables and efforts. The observations table contains the foreign keys to the dimension tables, as well as the observation value and the observation geometry. The dimension tables contain the metadata for the observations.\nIt is designed to support different observation types, such as abundances, occurrences, and presence/absence data of various units.\n  Figure 1. Entity relationship diagram for the observations related tables\n\n\nBelow is a brief description of each table. See the Schema Reference section for more details on each table.\nObservations: Central to the schema, storing detailed records of individual ecological observations.\n\nImportant for injection\nGeometry can be injected using the WKT format: SRID=4326;POINT(-73.5 45.5) and will be automatically converted to the PostGIS geometry type.\nColumns created_at, created_by, modified_at, within_quebec, dwc_event_date are automatically set by the database and should not be set manually.\n\nDatasets: Manages metadata about different datasets to which observations belong. Includes comprehensive information like source, creator, title, and publisher.\nVariables: Categorizes different observation types and units.\nTaxa_Obs: Stores raw taxonomic information as provided by the source, facilitating the recording of various taxa. Those entries are meant as an archive of the original data and are not used for data retrieval. Taxonomic information is managed by other resources within the database, like the taxa_ref table. See the managing_taxonomy article for more details.\n\nImportant for injection\nThe scientific_name column is required. The authorship and rank columns are optional and will be set to an empty string if not provided. Entries are accepted for any taxonomic rank, including species, genus, complexes, etc.\nThe parent_scientific_name column is optional. It is used to resolve conflicts where a scientific name corresponds to different organisms in different branches of the tree of life. If not specified, all results for the given scientific name are returned. It is recommended to specify the phylum or kingdom when available\nColumns rank, authorship are optional and should only be set if available by the source.\n\nEfforts: Quantifies the efforts (like time, distance, etc.) put into making an observation.\nObs_Efforts: Links observations with their corresponding efforts. Creates a many-to-many relationship, allowing multiple efforts to be linked to a single observation and vice versa.\n\n\n\nAuto-generated columns: The following columns are automatically generated by the database and should not be set manually: created_at, created_by, modified_at, within_quebec, dwc_event_date.\nUnique Constraints: Unique constraints are used to prevent duplicate observations from being inserted into the database. The combination of geom, dwc_event_date, id_taxa_obs, obs_value, id_variables, within_quebec ensures that each observation is uniquely identifiable and retrievable.\nPartitions: Partitions are used to store observations that are within or outside of Quebec. This allows for faster queries on observations that are within Quebec, which are the most commonly used. See the partitions section in the observations schema reference for more details.\nTaxonomic information: Only raw taxonomic information is stored in the taxa_obs table. The valid taxonomy, references, and other information are managed by taxa_ref, taxa_vernacular, taxa_groups tables and related ressources. This is managed automatically with periodic updates. See the managing_taxonomy article for more details.",
    "crumbs": [
      "Portails",
      "Atlas",
      "Description schéma public"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_taxonomie.html",
    "href": "docs/portails/coleo/coleo_taxonomie.html",
    "title": "Taxonomie",
    "section": "",
    "text": "Repo Github",
    "crumbs": [
      "Portails",
      "Coleo",
      "Taxonomie"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_taxonomie.html#features",
    "href": "docs/portails/coleo/coleo_taxonomie.html#features",
    "title": "Taxonomie",
    "section": "Features",
    "text": "Features\n\nRaw observed taxonomic entry stored as-is. Minimal maintenance of stored taxon database and taxon entries overhead operation (validation, correction) is allowed by storing raw taxon values as-is. All corrected and validated referenced taxonomic entries are found through fuzzy matching and stored independently.\nTaxon accepted for all ranks : Observation may be be related to an organism identified at many different levels ie. species, genus, family depending on the type of survey. All taxonomic entries may be ingested into the observed taxon table regardless of their rank and will be related to their referenced taxons.\nFuzzy matching : Raw taxon are matched to entries in reference taxonomic databases using fuzzy matching, thus correcting for orthographic or casing error.\nUnresolved entry : If a raw taxonomic entry cannot be matched, closest taxonomic parent reference will be obtained and related if possible.\nMultiple and conflicting taxonomic sources : Raw taxons are matched to their referenced counterparts from multiple taxonomic databases. These matches allows for use of specialized databases or conflicting ones. They are stored without priority, making it possible to reference a raw taxon and related observation through any names obtained through conflicting reference database.\nParent-children taxonomic relationship : Search taxons and related observations through parent taxons possible through stored reference taxons for parents and relationship to raw entry. ie. Parent taxon class Aves can be related to all children species taxon entries Cyanocitta cristata, Falco peregrinus, etc.\nRevised taxon and valid synonym : Raw taxon whose valid reference named has changed are matched to both deprecated references and valid ones, making it possible to search raw taxons and related observation and event through either one.\nUpdating and change in reference taxonomic database : Updates to the validity of a taxonomic entry is possible through periodic update of references obtained from raw taxon entries. Raw taxons are thus stored and maintained as described in original sources and surveys\nVernacular names : A list of vernacular names (fr & en) are found for each reference taxons (parents, synonyms) related to a raw taxon and for a number of reference vernacular databases.\nComplex observation : When the taxon related to an observation is complex, such as multiple organism are identified for the same observation(Species 1 | Species 2 | Species 3), a single observed taxonomic entry is injected as such. References will be obtained for each single organism listed by the complex and all related parents. References matched from complex observed taxons are identified as such and can then be included or discarded from queries performed by the user. Common parent taxon are identified as such and can be used to query complex observed taxons.",
    "crumbs": [
      "Portails",
      "Coleo",
      "Taxonomie"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_taxonomie.html#principles",
    "href": "docs/portails/coleo/coleo_taxonomie.html#principles",
    "title": "Taxonomie",
    "section": "Principles",
    "text": "Principles\n\nRaw observed taxons are stored as is as rows in table taxa_obs, no orthographic correction nor validation of values is required. It’s primary key id_taxa_obs is used to be related to tables\nA list of reference taxons (parent, valid synonym) are found for each raw taxon and for a number of taxonomic reference databases through fuzzy match based on the Global names and GBIF taxononomic backbone API. All reference taxons are stored in table taxa_ref and may be related to observed raw taxa_obs rows through taxa_obs_ref_lookup lookup table.\nA list of vernacular names (fr & en) are found for each reference taxons (parents, synonyms) related to a raw taxons and for a number of reference vernacular databases through the GBIF taxononomic backbone API. All vernacular taxons are stored in table taxa_vernacular and may be related to observe taxa_obs rows through taxa_obs_vernacular_lookup lookup table.",
    "crumbs": [
      "Portails",
      "Coleo",
      "Taxonomie"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_taxonomie.html#common-workflows-and-procedures",
    "href": "docs/portails/coleo/coleo_taxonomie.html#common-workflows-and-procedures",
    "title": "Taxonomie",
    "section": "Common workflows and procedures",
    "text": "Common workflows and procedures\n\nIngesting entries with related taxa attributes\nExample. Insert rows in obs_species related to taxa attributes\n\nProcess summary\n\nUser : Inject raw species observation rows in TABLE obs_species\nAuto : Trigger is called on inject.\n\nRow in TABLE taxa_obs is injected from taxa_name in obs_species\nField id_tax_obs in TABLE obs_species is updated before injection\nFUNCTION match_taxa_sources(taxa_name) is called to obtain related reference taxonomy entries (synonyms, parents)\nReference taxonomic entries are injected into TABLE taxa_ref\nCorrespondence between raw observed taxon and related reference taxons are injected into TABLE taxa_obs_ref_lookup\n\n\n\n\nSQL Command\nINSERT INTO obs_species (taxa_name, variable, value, observation_id)\nVALUES (...)\n\n\n\nList species related to observations\n\nProcess summary\n\nUser : Query required observations using filters (site, campaign if required)\nUser : Join api.taxa using the id_taxa_obs foreign key\n\n\n\nSQL Command examples\nObserved taxons using api.taxa attributes (scientific name, group name, vernacular names, rank, etc).\nSELECT taxa.*\nFROM api.taxa\nList observed species using api.taxa attributes (scientific name, group name, vernacular names, rank, etc)\nSELECT taxa.*\nFROM api.taxa WHERE rank = 'species'\nListing observed species by site, site_type, campaigns, cells with api.taxa attributes (scientific name, group name, vernacular names, rank, etc).\nselect * from api.taxa_surveyed\nwhere site_type = 'forestier' -- May also use : site_id, site_code, site_type, cell_code, campaign_id, campaign_type\nand rank = 'species'\nListing observed richness for by site, site_type, campaigns, cells (etc). This function returns all distinct valid taxa name that is not a parent of any observed taxa, thus where all taxa equals 1 organism, regardless of their rank if they satisfy previous criterias.\n-- api.taxa_richness(cell_id integer, cell_code text, site_id integer, site_code text, site_type text, campaign_id integer, campaign_type text)\nselect api.taxa_richness(NULL, NULL, NULL, NULL, 'forestier', NULL, NULL)",
    "crumbs": [
      "Portails",
      "Coleo",
      "Taxonomie"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_taxonomie.html#important-notes",
    "href": "docs/portails/coleo/coleo_taxonomie.html#important-notes",
    "title": "Taxonomie",
    "section": "IMPORTANT NOTES",
    "text": "IMPORTANT NOTES\n\nALTER TABLE public.obs_species DROP CONSTRAINT obs_species_taxa_name_fkey;\n  select *                                     \n  from taxa_obs\n  where id not in (\n  select id_taxa_obs from taxa_obs_ref_lookup)\n  ;\n    id  | scientific_name |          created_at          \n  ------+-----------------+------------------------------\n   2380 | sphaigne verte  | 2022-04-07 17:14:09.45303-04\n   2658 | pellie sp.      | 2022-04-07 17:14:09.45303-04\n   5934 | Maccafertium    | 2022-08-17 18:06:51.7358-04\n   5986 | Caecidota       | 2022-08-17 18:06:51.7358-04\n   6277 | Callophrus      | 2022-08-17 18:06:51.7358-04\n  (5 rows)\nNo index on columns from cells, sites, campaigns, etc.\nNo complex are listed through API endpoints. However, their closest common parents are.\nTODO\n\nWhat’s up with l’aulne rugueux\napi.taxa disctint on id_taxa_obs ? Grouper les sources conflictuelles dans le json\nBezzia|Palpomyia is observed, should not because, to be fixed",
    "crumbs": [
      "Portails",
      "Coleo",
      "Taxonomie"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_infrastructure.html",
    "href": "docs/portails/coleo/coleo_infrastructure.html",
    "title": "Infrastructure Coleo (Staging et Production)",
    "section": "",
    "text": "La figure suivante montre comment le portail de l’atlas est structuré, y compris l’API du backend et la base de données.\n\n\n\nImage\n\n\nServer: coleo-api\nPostgres est utilisé pour stocker la base de données à l’intérieur d’un conteneur docker et pour l’API nous utilisons postgRest qui génère l’API automatiquement à partir des schémas et des tables de la base de données. Pour ce service, nous utilisons également un conteneur Docker.\nChaque service est divisé en deux catégories, la mise en scène et la production. Dans la phase d’essai, les données sont écrites pour éviter les erreurs dans la production. Une fois les modifications validées, elles sont migrées vers la production à l’aide du script : atlas_staging_to_main_swap.sh.\nIMPORTANT!!!: La migration est exécutée manuellement et seulement lorsque les modifications dans staging sont validées.\nTRIGGER SWAP MANUALLY (FOR ADMINS):\n\ntapez dans le promptsudo su être sudoer (PAS COLEO, sudo su sous votre propre utilisateur).\nallez dans le dossier /var/www/bq-db-container.\nexecuter la commande suivante: sh coleo_staging_to_main_swap.sh.",
    "crumbs": [
      "Portails",
      "Coleo",
      "Infrastructure Coleo (Staging et Production)"
    ]
  },
  {
    "objectID": "docs/portails/coleo/coleo_intro.html",
    "href": "docs/portails/coleo/coleo_intro.html",
    "title": "Base de données Coleo",
    "section": "",
    "text": "COLEO c’est la base de données du Suivi de la biodiversité du Québec. On y stock les données collectées par le MELCCFP et ses partenaires.\nL’injection des données se fait via des gabarits d’injection et le package R rcoleo. Les gabarits sont sauvés sur le SharePoint de l’équipe technique alors que le processus d’injection est décrit dans la documentation.\nListe des repos Github associés: Coleo_DB",
    "crumbs": [
      "Portails",
      "Coleo",
      "Base de données Coleo"
    ]
  },
  {
    "objectID": "docs/portails/strapi/from_staging_to_production.html",
    "href": "docs/portails/strapi/from_staging_to_production.html",
    "title": "Staging vers Production",
    "section": "",
    "text": "Tous les utilisateurs effectueront les modifications dans un environnement d’essai afin d’éviter les erreurs qui pourraient faire planter l’environnement de production. Ainsi, pour effectuer une modification, il faut d’abord se rendre sur le [Portail Strapi].(https://portail.biodiversite-quebec.ca/admin). Les changements s’appliqueront immédiatement au portail de Biodiversité Québec staging. Chaque nuit, les portails de production de strapi et de biodiversite quebec seront mis à jour via un script cronjob.\nIMPORTANT!!!: Si une mise à jour de production doit être effectuée rapidement, veuillez contacter les administrateurs car ils peuvent déclencher manuellement le processus de mise à jour.\nTRIGGER SWAP MANUALLY (FOR ADMINS):\n\ntapez dans le promptsudo su être sudoer (PAS COLEO, sudo su sous votre propre utilisateur).\nallez dans le dossier /var/www/strapi-backend-portail-staging (s’assurer d’être dans le dossier staging).\nexecuter la commande suivante: sh swap_staging_to_prod.sh.",
    "crumbs": [
      "Portails",
      "Strapi",
      "Staging vers Production"
    ]
  },
  {
    "objectID": "docs/portails/strapi/article.html",
    "href": "docs/portails/strapi/article.html",
    "title": "Create Articles",
    "section": "",
    "text": "Strapi allows our team to create content and feed our web portals such as Biodiversité Québec, Inventaire, Atlas, etc.\nIn this section we show how to create an article using strapi step by step.\n1 - Login to your strapi account\n2 - Go to Content Manager (Menu at Top Left)\n3 - Go to Article\n4 - Click on Create new entry \n5 - Fill the form with fields such Title, suffixe_addresse_url (mandatory since is the link of the article), author, article type, keywords, etc. \n6 - The fields shown in the previous step are the header of the article. In order to add more content like text, images, quoted text, spotify balados , you must click the button Add a component to the body and select a component. \n\n\n\nImage\n\n\n7 - when you finish the article, click on save and then publish (at Top Right).\n8 - Once the article is created, you need to go to the ArticlesPage and add the new article. This step will make sure that the article will show in the Discovery page as another card.\n9 - In the ArticlesPage page select the section where you want to place your article. Lets say: #Indispensable, click Add an entry and search the article in the list. Afterwards, click on save so the API notifies the web portal.\n\n\n\nImage",
    "crumbs": [
      "Portails",
      "Strapi",
      "Create Articles"
    ]
  },
  {
    "objectID": "docs/packages/rcoleo/rcoleo_telechargement.html",
    "href": "docs/packages/rcoleo/rcoleo_telechargement.html",
    "title": "Téléchargement",
    "section": "",
    "text": "Repo Github\nIl est possible de télécharger les données de coleo directement depuis R. Pour cela, il faut utiliser la fonction coleo_request_general.\nPour télécharger les données, il faut utiliser la commande coleo_request_general et spécifier la table à télécharger ou la fonction à appeler.",
    "crumbs": [
      "Packages",
      "rcoleo",
      "Téléchargement"
    ]
  },
  {
    "objectID": "docs/packages/rcoleo/rcoleo_telechargement.html#télécharger-des-données-dune-table-de-coléo",
    "href": "docs/packages/rcoleo/rcoleo_telechargement.html#télécharger-des-données-dune-table-de-coléo",
    "title": "Téléchargement",
    "section": "Télécharger des données d’une table de Coléo",
    "text": "Télécharger des données d’une table de Coléo\nLa commande coleo_request_general prend trois arguments pour télécharger desdonnées d’uen table (ou d’une View) de Coléo : d’abord un endpoint (le nom de la table), response_as_df = TRUE pour que la commande retourne la requête dans une table et le nom du schéma à l’argument schéma. Il est aussi possible d’ajouter des paramètres à la requête pour préciser les critères de recherche.\nPar exemple, pour télécharger les données de la table cells, il faut utiliser l’endpoint cells. Si aucun paramètre suuplémentaire n’est passé à la commande, toutes les entrées de la table sont retournées. ATTENTION !!! Cela peut représenter beaucoup de données.\nVoici un exemple de téléchargement de l’ensemble des données de la table cells :\ncells &lt;- coleo_request_general(\"cells\", response_as_df = TRUE, schema = \"public\")\nOn peut rafiner notre recherche en spécifiant des paramètres pour préciser les données à télécharger.\nVoici un exemple de téléchargement des données de la table cells avec le paramètre cell_code qui permet de spécifier le code de la cellule à télécharger :\ncell_105_101 &lt;- coleo_request_general(\"cells\", response_as_df = TRUE, schema = \"public\", cell_code = \"eq.105_101\")\nNotez que les paramètres passés doivent respecter la nomenclature des api postgREST. Ainsi, pour une équivalence comme cell_code = \"eq.105_101\" il faut ajouter eq. avant la valeur.",
    "crumbs": [
      "Packages",
      "rcoleo",
      "Téléchargement"
    ]
  },
  {
    "objectID": "docs/packages/rcoleo/rcoleo_telechargement.html#télécharger-des-données-dune-fonction-de-coléo",
    "href": "docs/packages/rcoleo/rcoleo_telechargement.html#télécharger-des-données-dune-fonction-de-coléo",
    "title": "Téléchargement",
    "section": "Télécharger des données d’une fonction de Coléo",
    "text": "Télécharger des données d’une fonction de Coléo\nLes requêtent sur des fonctions requièrent un format différent celles sur des tables. Les arguments demeurent les mêmes, mais rpc/ doit être ajouté avant le nom de la fonction.\nVoici un exemple de téléchargement des colonnes de la table cells en utilisant la fonction table_columns :\ncells_columns &lt;- coleo_request_general('rpc/table_columns', response_as_df = TRUE, 'table_name' = 'cells')",
    "crumbs": [
      "Packages",
      "rcoleo",
      "Téléchargement"
    ]
  },
  {
    "objectID": "docs/packages/rcoleo/rcoleo_injections.html",
    "href": "docs/packages/rcoleo/rcoleo_injections.html",
    "title": "Injections",
    "section": "",
    "text": "Repo Github\nCe tutoriel présente le processus pour injecter un jeu de données complet dans la base de données Coléo. L’exemple présenté est le même pour tout type d’inventaire ainsi que les cellules et sites.\nLe processus d’injection requiert des données saisies dans un gabarit d’injection et comprends trois étapes : (1) le chargement des données, (2) la validation des données et (3) l’injection des données. Ces étapes sont présentées dans les sections suivantes.",
    "crumbs": [
      "Packages",
      "rcoleo",
      "Injections"
    ]
  },
  {
    "objectID": "docs/packages/rcoleo/rcoleo_injections.html#formater-le-jeu-de-données",
    "href": "docs/packages/rcoleo/rcoleo_injections.html#formater-le-jeu-de-données",
    "title": "Injections",
    "section": "0. Formater le jeu de données",
    "text": "0. Formater le jeu de données\nLes données doivent d’abord être saisies dans un gabarit. Les gabarits sont spécifiques à chaque inventaire.\nAccédez aux gabarits.",
    "crumbs": [
      "Packages",
      "rcoleo",
      "Injections"
    ]
  },
  {
    "objectID": "docs/packages/rcoleo/rcoleo_injections.html#charger-le-jeu-de-données",
    "href": "docs/packages/rcoleo/rcoleo_injections.html#charger-le-jeu-de-données",
    "title": "Injections",
    "section": "1. Charger le jeu de données",
    "text": "1. Charger le jeu de données\nLa commande coleo_read est utilisée pour charger les données du gabarit excel. Elle charge et formate les données du gabarit en un dataframe qui est prêt à être injecté dans la base de données.\nNotez que l’injection de cellules requiert un shapefile plutôt que le gabarit Excel standard. Pour structurer et formater le fichier, veillez vous référer au gabarit d’injection de cellules.\ndata &lt;- coleo_read(fileName)\nLa commande coleo_read prend un argument fileName qui spécifie le chemin d’accès local vers le gabarit contenant les données.\nUne fois les données chargées, il faut les valider avant de lancer la procédure d’injection.",
    "crumbs": [
      "Packages",
      "rcoleo",
      "Injections"
    ]
  },
  {
    "objectID": "docs/packages/rcoleo/rcoleo_injections.html#valider-le-jeu-de-données",
    "href": "docs/packages/rcoleo/rcoleo_injections.html#valider-le-jeu-de-données",
    "title": "Injections",
    "section": "2. Valider le jeu de données",
    "text": "2. Valider le jeu de données\nOn utilise la commande coleo_validate pour valider les données. Cette commande vérifie que les données sont dans le bon format et qu’elles sont complètes. Elle retourne un message d’erreur si les données contiennent une erreur.\nCertains inventaires sont injéectés avec des fichier médias. Dans ces cas, le chemin d’accès vers dossier contenant les médias à injecter doit être passé avec l’argument media_path.\ndata_validated &lt;- coleo_validate(data, media_path = NULL)\nIl est à noter que tous les jeux de données requierent que les cellules et les sites aient déjà été injectés. Le processus d’injection des cellules et sites suivent le même processus d’injection.",
    "crumbs": [
      "Packages",
      "rcoleo",
      "Injections"
    ]
  },
  {
    "objectID": "docs/packages/rcoleo/rcoleo_injections.html#injecter-les-données-téléversement",
    "href": "docs/packages/rcoleo/rcoleo_injections.html#injecter-les-données-téléversement",
    "title": "Injections",
    "section": "3. Injecter les données (téléversement)",
    "text": "3. Injecter les données (téléversement)\nOn utilise la commande coleo_inject pour exécuter la procédure d’injectection des données dans la base de données Coléo. Cette commande retourne un message indiquant le nombre de lignes s’ayant injecté avec succès et erreur ainsi que le jeu de données initial avec les colonnes _id et _error pour chaque table injectée.\nL’argument media_path est requis lorsqu’il y a des fichiers médias à injecter. Il doit être le chemin local vers les fichiers médias à injecter.\ndata_injected &lt;- coelo_inject(data_validated, media_path = NULL)\nDes messages sont produits pour indiquer les données qui ont été injectées avec succès et celles qui ont échouées. Les données qui ont échoué peuvent être retrouvées dans l’objet data_injected puisqu’elles ont une valeur NULL dans la colonne *_id qui leur est associée.\nOn peut consulter les message d’erreur puisqu’ils sont sauvés dans la colonne *_error du dataframe data_injected.\nEn cas d’erreur, se référer à la personne ressource.\nC’EST TOUT !",
    "crumbs": [
      "Packages",
      "rcoleo",
      "Injections"
    ]
  },
  {
    "objectID": "docs/design/design_intro.html",
    "href": "docs/design/design_intro.html",
    "title": "Manifeste de marque",
    "section": "",
    "text": "Le manifeste de marque de Biodiversité Québec a été réalisé par Émilie Farias (université Laval) en juillet 2021. Le rapport et les différents éléments associés à l’identité visuelle de Biodiversité Québec (logos français et anglais, illustrations, motifs, …) sont centralisés dans le répertoire image_de_marque.\nDownload PDF file."
  },
  {
    "objectID": "docs/serveurs/pose/index.html",
    "href": "docs/serveurs/pose/index.html",
    "title": "Pose",
    "section": "",
    "text": "Pose\nThis server has a powerfull hardware that is used to perform high comptation demanding task.\nto connect via ssh ssh &lt;your_user&gt;@pose.vhost33",
    "crumbs": [
      "Serveurs",
      "Pose"
    ]
  },
  {
    "objectID": "docs/serveurs/coleo-media/index.html",
    "href": "docs/serveurs/coleo-media/index.html",
    "title": "Coleo-media",
    "section": "",
    "text": "Coleo-media\nHost the media api. This is an api used to store and process images and audio. Coleo-app repository\nto connect via ssh ssh &lt;your_user&gt;@coleo-media.vhost33",
    "crumbs": [
      "Serveurs",
      "Coleo-media"
    ]
  },
  {
    "objectID": "docs/serveurs/postgres01/index.html",
    "href": "docs/serveurs/postgres01/index.html",
    "title": "Postgres01",
    "section": "",
    "text": "Postgres server\nHost all databases.\nto connect via ssh ssh &lt;your_user&gt;@postgres.vhost33 Runs postgres 13.\nUser postgres is used to run commands. All the config and backup scripts are located in the home directory of postgres user. sudo su - postgres.\nThe configuration file is located: /etc/postgresql/13/main/\n\nbackup\nIn order to backup up our database Atlas a librcd .ary called pgbackrest. Here you can find the documantation of the librarby.\nyou can find our pbackrest config file here: sudo cat /etc/pgbackrest.conf\nA set of cronjobs are setup to backup Atlas database (our bigger DB), to refresh the materialized view and the strapi databse.\nsudo su - postgres && contrab -e to see the crontab.\nThe backups are sent to the S3 server.\nWe perform an Incremental Backup. This allows us to optimize space since it does a full backup only the first time pgbackrest --stanza=main --type=full  --log-level-console=info backup --repo1-s3-uri-style=path, s3FullBackup.sh script. Then, it will save only what is different from on week to another (we perfom a backup once at the end of the week) pgbackrest --stanza=main --type=incr  --log-level-console=info backup --repo1-s3-uri-style=path, s3Backup.sh script.\n\n\nimportant config settup\npostgresql.conf:\n \nThe config files for postgres and pgbackrest will be saved in our backup server (S3) (postgresql_config_files/ and pgbackrest_config_files/).",
    "crumbs": [
      "Serveurs",
      "Postgres01"
    ]
  },
  {
    "objectID": "docs/serveurs/rweb/rweb_intro.html",
    "href": "docs/serveurs/rweb/rweb_intro.html",
    "title": "Rweb",
    "section": "",
    "text": "Courte (ou pas) description",
    "crumbs": [
      "Serveurs",
      "Rweb",
      "Rweb"
    ]
  },
  {
    "objectID": "docs/tableaux_bord/tableaux_bord_docker.html",
    "href": "docs/tableaux_bord/tableaux_bord_docker.html",
    "title": "Mettre un tableau dans un docker",
    "section": "",
    "text": "Ce document décrit les étapes pour encapsuler une application Shiny dans un docker. La première étape est de télécharger une image appropriée pour votre application (en assumant que Docker est installé sur votre ordinateur). L’idéal est de trouver une image dans laquelle plusieurs de vos dépendences sont déjà installées.\ndocker pull rocker/geospatial\nPar la suite, vous pouvez entrer dans l’image pour vérifier ce qui s’y trouve et tester de façon interactive l’installation des différentes dépendences qui sont nécessaires à votre application.\ndocker run -it --rm rocker/geospatial sh\nUne fois que vous connaissez les différentes dépendences nécessaires, ajoutez un fichier Dockerfile (sans extension) dans le répertoire de votre application (app.R) avec les instructions suivantes et les différents packages à installer.\n# Base R Shiny image\nFROM rocker/geospatial\n\n# Make a directory in the container\nRUN mkdir /home/shiny-app\n\n# Install R dependencies\nRUN R -e \"options(repos = c( \\\n             INLA = 'https://inla.r-inla-download.org/R/testing', \\\n             CRAN = 'https://cloud.r-project.org' \\\n          )); \\\n          install.packages(c('shinyjs', 'sn', 'INLA', 'inlabru')); \\\n          remotes::install_github('daattali/shinycssloaders')\"\n\n# Copy the Shiny app code\nCOPY app.R /home/shiny-app/app.R\n\n# Expose the application port\nEXPOSE 8180\n\n# Run the R Shiny app\nCMD Rscript /home/shiny-app/app.R\nAu début du script R de votre application, ajoutez les instructions suivantes:\n# Specify the application port\noptions(shiny.host = \"0.0.0.0\")\noptions(shiny.port = 8180)\nMaintenant, construisez l’image pour votre application (à partir du dossier où est située votre application (à noter que le nom de l’image doit être en minuscule).\ndocker build -t nomdevotreapp . \nLancez et testez votre application à partir du docker.\ndocker run -p 8180:8180 nomdevotreapp\nAlternativement, lorsqu’il y a un fichier compose.yml dans votre repo, vous pouvez simplement faire ceci pour lancer l’application.\ndocker compose up\nAu besoin, listez ou supprimez les images à partir de leur ID avec les commandes suivantes.\ndocker images\ndocker image rm -f 812a84f022b2 \nUne fois votre application fonctionnelle à partir de docker, demandez à Guillaume comment la déployer sur un serveur…",
    "crumbs": [
      "Tableaux de bord",
      "Mettre un tableau dans un docker"
    ]
  },
  {
    "objectID": "docs/catalogues/io/io_intro.html",
    "href": "docs/catalogues/io/io_intro.html",
    "title": "Example Rstac and gdalcubes scripts",
    "section": "",
    "text": "IO est un catalogue de couches de données environnementales en format raster pouvant servir dans le contexte de modélisations en lien avec la biodiversité. Cette ressource est sous forme de catalogue STAC, qui est accessible via différentes méthodes, notamment sous R grâce aux packages rstac et gdalcubes.\nLes couches disponibles dans le catalogue peuvent être visualisées ici.\nLes couches de données dans IO sont sous format COG (Cloud Optimized Geotiff), qui est un format permettant un accès optimal avec des requêtes à distance. Par exemple, il est possible d’extraire seulement une petite région d’un fichier global et de transformer sa résolution et son système de coordonnées de référence, sans jamais avoir à le télécharger en entier.",
    "crumbs": [
      "Catalogues",
      "IO",
      "Example Rstac and gdalcubes scripts"
    ]
  },
  {
    "objectID": "docs/catalogues/io/stacsfastapi.html",
    "href": "docs/catalogues/io/stacsfastapi.html",
    "title": "STAC Catalog",
    "section": "",
    "text": "Basic tools for creating STAC items and catalogs for IO repository of geospatial data in Biodiversité Québec\nThe Github repository contains:\n\nA Python library in /bqio for loading raster data into the STAC catalog and sending it to the Stac FastAPI library.\nA StacFastAPI library for serving the catalogues for IO and Acer.\nA NodeJS app for securing the StacFastAPI requests.\nAn experimental stacItem Pipeline API for sending rasters directly to an API endpoint for automated ingestion into the STAC catalog.\n\n\n\nThe Python library located in /bqio is used to\n\nTake local or remote files and convert them into Cloud optimized Geotiffs (COG) using gdalwarp.\nSend the COGs to the Digital Research Alliance object storage using S3 tools.\nExtract information from the COGs to populate the information for the STAC items.\nCreate collections and items using the pystac Python library and send them through POST requests to STAC FASTAPI.\n\nThere are a number of example ingestion scripts located in /datasets/. Note that older scripts were using an older version of the library. These scripts were run in a Docker container on a virtual machine with sufficient resources for the COG conversion and data download/upload to proceed. Some of theses scripts take several days to run.",
    "crumbs": [
      "Catalogues",
      "IO",
      "STAC Catalog"
    ]
  },
  {
    "objectID": "docs/catalogues/io/stacsfastapi.html#python-stac-ingestion-library",
    "href": "docs/catalogues/io/stacsfastapi.html#python-stac-ingestion-library",
    "title": "STAC Catalog",
    "section": "",
    "text": "The Python library located in /bqio is used to\n\nTake local or remote files and convert them into Cloud optimized Geotiffs (COG) using gdalwarp.\nSend the COGs to the Digital Research Alliance object storage using S3 tools.\nExtract information from the COGs to populate the information for the STAC items.\nCreate collections and items using the pystac Python library and send them through POST requests to STAC FASTAPI.\n\nThere are a number of example ingestion scripts located in /datasets/. Note that older scripts were using an older version of the library. These scripts were run in a Docker container on a virtual machine with sufficient resources for the COG conversion and data download/upload to proceed. Some of theses scripts take several days to run.",
    "crumbs": [
      "Catalogues",
      "IO",
      "STAC Catalog"
    ]
  },
  {
    "objectID": "docs/catalogues/io/stacsfastapi.html#stac-fastapi",
    "href": "docs/catalogues/io/stacsfastapi.html#stac-fastapi",
    "title": "STAC Catalog",
    "section": "STAC-FASTAPI",
    "text": "STAC-FASTAPI\nAt the moment, the STAC API is generated from the generic STAC-FASTAPI/PGSTAC Docker image. A functioning nginx configuration section is in nginx-stac-endpoint.txt\nThe db-backup.sh file contains the script to backup the database through regular CRON jobs.",
    "crumbs": [
      "Catalogues",
      "IO",
      "STAC Catalog"
    ]
  },
  {
    "objectID": "docs/catalogues/io/stacsfastapi.html#experimental-stacitem-pipeline-api",
    "href": "docs/catalogues/io/stacsfastapi.html#experimental-stacitem-pipeline-api",
    "title": "STAC Catalog",
    "section": "EXPERIMENTAL stacItem Pipeline API",
    "text": "EXPERIMENTAL stacItem Pipeline API\nStacItemPipeline API allows user to send new stac items to an existing stac collection. A simple flask api has been used to expose some endpoints that make the injection possible.\n\nEndpoints\n{api_url}/newitem : POST request that allows to inject the new item.\nbody: a json object with the following format\n{ \"collection_id\":\"id of the collection where item will be inserted\",\n\"date\":\"2018-01-01\",\n\"name\":\"name of the item\",\n\"filename\":\"filename.tif\",\n\"stac_api_server\":\"&lt;url_stac_api&gt;\",\n\"file_source_host\":\"&lt;url_to_save_cog_file&gt;\",\n\"properties\" : { \"full_filename\": \"full_filename.tif\",\n        \"description\": \"Description of the item\",\n        \"otherparams....\"\n        }\n}\nreturn : json object with id of the process of your item and a message (received means that the server already have your item in the queue waiting to be processed).\n{\n  \"id\": \"c1705f50-029c-4502-a2b6-9ffd1e8876d4\",\n  \"msg\": \"received\"\n}\n{url}/status?id=c1705f50-029c-4502-a2b6-9ffd1e8876d4: GET request to verify the status of the process with the given id.\nreturn: json\n\nCase where process finished properly\n {\n    \"_description\": \"description of diferent steps of the process\",\n    \"_id\": \"c1705f50-029c-4502-a2b6-9ffd1e8876d4\",\n    \"_operation\": \"PROCESS FINISHED\",\n    \"_param\": {\n      \"collection_id\": \"collection_id\",\n      \"date\": \"2018-01-01\",\n      \"file_source_host\": \"url(location of the source .tiff file)\",\n      \"filename\": \"filename.tif\",\n      \"name\": \"item name\",\n      \"properties\": {\n        \"datetime\": \"2018-01-01T00:00:00Z\",\n        \"description\": \"TTT Other Hansen 2020\",\n        \"full_filename\": \"full_filename.tif\",\n       \"other propertis....\"\n      },\n\n      \"other propertis....\"\n    },\n    \"_status\": \"ok\"\n  }\n\n\nCase of failure\n{\n    \"_description\": \"Unable to read collection: \\\"chelsa-clim\\\" from server, error \\n: &lt;urlopen error [Errno 111] Connection refused&gt;. \\n Please check your conexion or congif.\",\n   \"_id\": \"c1705f50-029c-4502-a2b6-9ffd1e8876d4\",\n    \"_operation\": \"GETCOLLECTION\",\n    \"_param\": {\n      \"collection_id\": \"collection_id\",\n      \"date\": \"2018-01-01\",\n      \"file_source_host\": \"url(location of the source .tiff file)\",\n      \"filename\": \"filename.tif\",\n      \"name\": \"item name\",\n      \"properties\": {\n        \"datetime\": \"2018-01-01T00:00:00Z\",\n        \"description\": \"TTT Other Hansen 2020\",\n        \"full_filename\": \"full_filename.tif\",\n       \"other propertis....\"\n      },\n\n      \"other propertis....\"\n    },\n    \"_status\": \"error\"\n  }\n{api_url}/status/all: GET request that returns a list of all item sent to the API.\nreturn: list of json object with the same format of the preview ones.\n\n\n\nDocker config for stacItem Pipeline\nCreate first an environment variable file (.env) with the following variables:\n\nARBUTUS_OBJECT_ACCESS_ID=...\nARBUTUS_OBJECT_ACCESS_KEY=....\nAPI_PORT=...\nAPI_HOST=0.0.0.0\nSTAC_API_HOST=..url..\n\nnote: if the container is running in the same network as the STAC API server container, you might need to use the api address in the STAC_API_HOST variable (ex: STAC_API_HOST=STAC_API_HOST=http://172.21.0.3:8082). To get the IP address of your container, run this command:\ndocker inspect -f '{{.NetworkSettings.Networks.[network].IPAddress}}' [container name]\nRun in the terminal:\ndocker-compose -f docker-compose-api.yml up --build # only first time to build the image.\ndocker-compose -f docker-compose-api.yml up gdal-api-python\nnote: make sure the stac api server is running and accesible.",
    "crumbs": [
      "Catalogues",
      "IO",
      "STAC Catalog"
    ]
  },
  {
    "objectID": "docs/catalogues/io/stacsfastapi.html#database-backup",
    "href": "docs/catalogues/io/stacsfastapi.html#database-backup",
    "title": "STAC Catalog",
    "section": "Database Backup",
    "text": "Database Backup\nEvery day for the last 30 days a catalogdb database’s backup is created and stored in cloud server. This process is automatically triggered once a day.",
    "crumbs": [
      "Catalogues",
      "IO",
      "STAC Catalog"
    ]
  },
  {
    "objectID": "docs/catalogues/io/stacsfastapi.html#restore-a-backup",
    "href": "docs/catalogues/io/stacsfastapi.html#restore-a-backup",
    "title": "STAC Catalog",
    "section": "Restore a Backup",
    "text": "Restore a Backup\nOur backups are store in the cloud server (s3). In order to restore a database you need to do the following steps:\n\nDownload the backup file from the cloud server ( since it is S3 in our case make sure your computer is configured so it can connect to the S3)\nunzip the backup file.\ncopy the file docker-compose-io.yml inside the stac-fastapi repo folder in the server.\nRun the command docker-compose -f docker-compose-io.yml up inside the stac-fastapi repo folder in the server.\nmove backup file to the backup folder inside the docker container.\nrun docker exec -it stac-db psql in the server to run the postgresql in the container.\nCreate a new database create database newDB;\nRestore the backup file in the newDB by : psql -d newBD &lt; backupfile.sql.\nDrop the catalogdb databse inside the container (there are conflicts if we restore directly in catalogdb) by: drop database catalogdb.\nRename the database newDB to catalogdb;\nRestart containers one more time by: docker-compose -f docker-compose-io.yml up.",
    "crumbs": [
      "Catalogues",
      "IO",
      "STAC Catalog"
    ]
  },
  {
    "objectID": "docs/catalogues/io/stacsfastapi.html#stac-api-gateway",
    "href": "docs/catalogues/io/stacsfastapi.html#stac-api-gateway",
    "title": "STAC Catalog",
    "section": "STAC-API-GATEWAY",
    "text": "STAC-API-GATEWAY",
    "crumbs": [
      "Catalogues",
      "IO",
      "STAC Catalog"
    ]
  },
  {
    "objectID": "docs/catalogues/catalogues_s5cmd_use.html",
    "href": "docs/catalogues/catalogues_s5cmd_use.html",
    "title": "Utilisation de l’entreposage objet sur Arbutus",
    "section": "",
    "text": "L’outil s5cmd permet la gestion des contenus des différents catalogues utilisés par Biodiversité Québec. La première étape consiste à installer cet outil après avoir téléchargé le fichier nécessaire.\n# Pour les utilisateurs de Linux\n# Naviguer jusque dans le dossier où se situe le fichier téléchargé\nsudo dpkg -i s5cmd_2.2.2_linux_amd64.deb\n\ns5cmd # pour accéder aux différentes commandes dispo et à l'aide\nIl est nécessaire de paramétrer 3 variables environnement indispensables au bon fonctionnement de s5cmd. Il y a deux manières de procéder: spécifier ces variables à chaque lancement de s5cmd\n\nAWS_ACCESS_KEY_ID=xxxxxx\nAWS_SECRET_ACCESS_KEY=xxxxxx\nS3_ENDPOINT_URL=xxxxxx\n\ns5cmd ls s3://bq-io/\nou les enregistrer de façon permanente en local dans le fichier de paramétrage du terminal, par ex. .zshrc.\nexport AWS_ACCESS_KEY_ID=xxxxxx\nexport AWS_SECRET_ACCESS_KEY=xxxxxx\nexport S3_ENDPOINT_URL=xxxxxx\nPour plus d’informations, contacter Guillaume Larocque (guillaume.larocque@mcgill.ca) ou Claire-Cécile Juhasz (juhc3201@usherbrooke.ca).\n\nParcourir les répertoires ou buckets\nPour entrer et naviguer dans les catalogues, utiliser la commande:\ns5cmd ls s3://bq-io/\nAttention de ne pas oublier le slash final pour pouvoir voir le contenu du bucket ciblé. Dans le cas contraire, votre terminal vous dira si oui ou non le catalogue existe mais ne présentera pas son contenu.\n\n\nCréer un bucket et uploader du contenu\nLa création d’un nouveau bucket se fera automatiquement en envoyant un nouveau dossier ou fichier dans le catalogue avec la commande cp pour copy object. Par exemple, si vous souhaitez créer un nouveau bucket appelé super_data dans le catalogue ACER qui contiendra le fichier super_nice_data.tif, il vous faudra procéder de la façon suivante:\ns5cmd cp -acl public-read absolute_path/super_nice_data.tif s3://bq-io/acer/super_data/\nL’argument -acl public-read est indispensable pour permettre de rendre les données publiques et accessibles.\nAttention, si vous souhaitez stocker un fichier au format geoPackage sur lequel vous souhaitez faire des requêtes SQL par la suite, il est nécessaire de créer des index sur toutes les colonnes qui seront susceptibles d’être utilisées.\n# Indexation\nogrinfo -sql \"CREATE INDEX index_name ON table_name (column1, column2,columnN...)\" my_geopackage.gpkg\nAttention de ne pas créer des index multi-colonnes sauf si nécessaire.\n\n\nRetirer du contenu\nSi vous souhaitez retirer le bucket super_data, il vous suffit d’effacer tout son contenu à l’aide de la commande rm. Une fois le bucket vidé de son contenu, il disparaitra automatiquement de l’arborescence du catalogue.\ns5cmd rm \"s3://bq-io/acer/super_data/*\"\n\n\nDéplacer du contenu\nVous pouvez également déplacer des fichiers d’un bucket à un autre en utilisant la commande cp.\ns5cmd cp \"s3://bq-io/acer/super_data/*\" s3://bq-io/acer/autre_super_data/\n\n\nRequête sur le fichier geoPackage\n\nurl de base https://object-arbutus.cloud.computecanada.ca/bq-io/\n\n\nutilisation de viscurl\n\n\n\nRessources\nCatalogue STAC IO\nExemples IO\nExemple utilisation Rstac & gdalcubes",
    "crumbs": [
      "Catalogues",
      "Généralités",
      "Utilisation de l'entreposage objet sur Arbutus"
    ]
  },
  {
    "objectID": "docs/catalogues/geoio/geoio_intro.html",
    "href": "docs/catalogues/geoio/geoio_intro.html",
    "title": "Catalogue GEOIO",
    "section": "",
    "text": "Lien vers le dépot Github\nCette API FastAPI est utilisée pour faire des requêtes sur des données géospatiales en format vectoriel ou en format de colonnes. Des fichiers en format GeoParquet, GeoPackage, FlatGeoBuf ou autre sont entreposés dans le répertoire /data du Docker. Ensuite, les requêtes sont créées avec GDAL ou DuckDB pour accepter les paramètres GET ou POST et retourner des réponses en format GEOJSON.\nCertaines requêtes font également appel au catalogue IO. Par exemple, on peut voir la liste des pays selon Natural Earth ici:\nhttps://geoio.biodiversite-quebec.ca/country_list\nEt ensuite, avec le endpoint /country_stats, on peut spécifier le nom du pays et le lien vers un fichier COG sur IO (ou ailleurs), et obtenir les statistiques du fichier COG pour ce pays, en utilisant le serveur TiTiler.",
    "crumbs": [
      "Catalogues",
      "GEOIO",
      "Catalogue GEOIO"
    ]
  },
  {
    "objectID": "docs/catalogues/geoio/geoio_intro.html#démarrage-du-serveur",
    "href": "docs/catalogues/geoio/geoio_intro.html#démarrage-du-serveur",
    "title": "Catalogue GEOIO",
    "section": "Démarrage du serveur",
    "text": "Démarrage du serveur\ndocker compose up -d",
    "crumbs": [
      "Catalogues",
      "GEOIO",
      "Catalogue GEOIO"
    ]
  },
  {
    "objectID": "docs/catalogues/geoio/geoio_intro.html#accès-à-la-documentation-des-endpoints",
    "href": "docs/catalogues/geoio/geoio_intro.html#accès-à-la-documentation-des-endpoints",
    "title": "Catalogue GEOIO",
    "section": "Accès à la documentation des endpoints",
    "text": "Accès à la documentation des endpoints\nhttps://geoio.biodiversite-quebec.ca/docs",
    "crumbs": [
      "Catalogues",
      "GEOIO",
      "Catalogue GEOIO"
    ]
  },
  {
    "objectID": "docs/catalogues/geoio/geoio_intro.html#jeux-de-données-disponibles",
    "href": "docs/catalogues/geoio/geoio_intro.html#jeux-de-données-disponibles",
    "title": "Catalogue GEOIO",
    "section": "Jeux de données disponibles",
    "text": "Jeux de données disponibles\n\nPays et états selon Natural Earth - Format FlatGeoBuf\nWDPA protected areas - Format GPKG",
    "crumbs": [
      "Catalogues",
      "GEOIO",
      "Catalogue GEOIO"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "À propos",
    "section": "",
    "text": "(30/04/2024)",
    "crumbs": [
      "À propos"
    ]
  },
  {
    "objectID": "about.html#structure-du-site",
    "href": "about.html#structure-du-site",
    "title": "À propos",
    "section": "",
    "text": "(30/04/2024)",
    "crumbs": [
      "À propos"
    ]
  }
]